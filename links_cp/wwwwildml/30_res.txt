<!DOCTYPE html>
<html lang="en-US" class="no-js no-svg">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="profile" href="http://gmpg.org/xfn/11">
<script>(function(html){html.className = html.className.replace(/\bno-js\b/,'js')})(document.documentElement);</script>
<title>Reinforcement Learning &#8211; WildML</title>
<link rel='dns-prefetch' href='//s0.wp.com' />
<link rel='dns-prefetch' href='//secure.gravatar.com' />
<link rel='dns-prefetch' href='//fonts.googleapis.com' />
<link rel='dns-prefetch' href='//s.w.org' />
<link href='https://fonts.gstatic.com' crossorigin rel='preconnect' />
<link rel="alternate" type="application/rss+xml" title="WildML &raquo; Feed" href="http://www.wildml.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="WildML &raquo; Comments Feed" href="http://www.wildml.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="WildML &raquo; Reinforcement Learning Category Feed" href="http://www.wildml.com/category/reinforcement-learning/feed/" />
<script type="text/javascript">
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11.2.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11.2.0\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.wildml.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.1.1"}};
!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
</script>
<style type="text/css">
img.wp-smiley,
img.emoji {
display: inline !important;
border: none !important;
box-shadow: none !important;
height: 1em !important;
width: 1em !important;
margin: 0 .07em !important;
vertical-align: -0.1em !important;
background: none !important;
padding: 0 !important;
}
</style>
<link rel='stylesheet' id='crayon-css'  href='http://www.wildml.com/wp-content/plugins/crayon-syntax-highlighter/css/min/crayon.min.css?ver=_2.7.2_beta' type='text/css' media='all' />
<link rel='stylesheet' id='crayon-theme-github-css'  href='http://www.wildml.com/wp-content/plugins/crayon-syntax-highlighter/themes/github/github.css?ver=_2.7.2_beta' type='text/css' media='all' />
<link rel='stylesheet' id='crayon-font-monaco-css'  href='http://www.wildml.com/wp-content/plugins/crayon-syntax-highlighter/fonts/monaco.css?ver=_2.7.2_beta' type='text/css' media='all' />
<link rel='stylesheet' id='wp-block-library-css'  href='http://www.wildml.com/wp-includes/css/dist/block-library/style.min.css?ver=5.1.1' type='text/css' media='all' />
<link rel='stylesheet' id='wp-block-library-theme-css'  href='http://www.wildml.com/wp-includes/css/dist/block-library/theme.min.css?ver=5.1.1' type='text/css' media='all' />
<link rel='stylesheet' id='twentyseventeen-fonts-css'  href='https://fonts.googleapis.com/css?family=Libre+Franklin%3A300%2C300i%2C400%2C400i%2C600%2C600i%2C800%2C800i&#038;subset=latin%2Clatin-ext' type='text/css' media='all' />
<link rel='stylesheet' id='twentyseventeen-style-css'  href='http://www.wildml.com/wp-content/themes/twentyseventeen/style.css?ver=5.1.1' type='text/css' media='all' />
<link rel='stylesheet' id='twentyseventeen-block-style-css'  href='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/css/blocks.css?ver=1.1' type='text/css' media='all' />
<!--[if lt IE 9]>
<link rel='stylesheet' id='twentyseventeen-ie8-css'  href='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/css/ie8.css?ver=1.0' type='text/css' media='all' />
<![endif]-->
<link rel='stylesheet' id='jetpack_css-css'  href='http://www.wildml.com/wp-content/plugins/jetpack/css/jetpack.css?ver=7.1.1' type='text/css' media='all' />
<script type='text/javascript' src='http://www.wildml.com/wp-includes/js/jquery/jquery.js?ver=1.12.4'></script>
<script type='text/javascript' src='http://www.wildml.com/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var CrayonSyntaxSettings = {"version":"_2.7.2_beta","is_admin":"0","ajaxurl":"http:\/\/www.wildml.com\/wp-admin\/admin-ajax.php","prefix":"crayon-","setting":"crayon-setting","selected":"crayon-setting-selected","changed":"crayon-setting-changed","special":"crayon-setting-special","orig_value":"data-orig-value","debug":""};
var CrayonSyntaxStrings = {"copy":"Press %s to Copy, %s to Paste","minimize":"Click To Expand Code"};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/plugins/crayon-syntax-highlighter/js/min/crayon.min.js?ver=_2.7.2_beta'></script>
<!--[if lt IE 9]>
<script type='text/javascript' src='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/js/html5.js?ver=3.7.3'></script>
<![endif]-->
<link rel='https://api.w.org/' href='http://www.wildml.com/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.wildml.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.wildml.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 5.1.1" />
<link rel='dns-prefetch' href='//v0.wordpress.com'/>
<style type='text/css'>img#wpstats{display:none}</style>			<style type="text/css" id="wp-custom-css">
/*
You can add your own CSS here.
Click the help icon above to learn more.
*/
@import url('https://fonts.googleapis.com/css?family=Lora');
@media screen and (min-width: 48em) {
#content > .wrap {
max-width: 100%;
font-family: Lora;
}
#content #primary { 
width: 65%;
font-size: 1.1em;
line-height: 1.7em
}
#content #secondary { 
width: 25%;
font-size: 1.1em;
}
div[id^=highlighter] {
font-size: 0.8em !important
}
ul, ol {
padding: 1em;
}
}
.entry-content ul {
padding: 0 0 0 2em
}
/*
CSS Migrated from Jetpack:
*/
/*
CSS Migrated from Jetpack:
*/
</style>
</head>
<body class="archive category category-reinforcement-learning category-17 wp-embed-responsive hfeed has-sidebar page-two-column colors-light">
<div id="page" class="site">
<a class="skip-link screen-reader-text" href="#content">Skip to content</a>
<header id="masthead" class="site-header" role="banner">
<div class="custom-header">
<div class="custom-header-media">
</div>
<div class="site-branding">
<div class="wrap">
<div class="site-branding-text">
<p class="site-title"><a href="http://www.wildml.com/" rel="home">WildML</a></p>
<p class="site-description">Artificial Intelligence, Deep Learning, and NLP</p>
</div><!-- .site-branding-text -->
</div><!-- .wrap -->
</div><!-- .site-branding -->
</div><!-- .custom-header -->
<div class="navigation-top">
<div class="wrap">
<nav id="site-navigation" class="main-navigation" role="navigation" aria-label="Top Menu">
<button class="menu-toggle" aria-controls="top-menu" aria-expanded="false">
<svg class="icon icon-bars" aria-hidden="true" role="img"> <use href="#icon-bars" xlink:href="#icon-bars"></use> </svg><svg class="icon icon-close" aria-hidden="true" role="img"> <use href="#icon-close" xlink:href="#icon-close"></use> </svg>Menu	</button>
<div class="menu-header-menu-container"><ul id="top-menu" class="menu"><li id="menu-item-655" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-655"><a href="http://www.wildml.com">Home</a></li>
<li id="menu-item-821" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-821"><a href="https://www.getrevue.co/profile/wildml">AI Newsletter</a></li>
<li id="menu-item-694" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-694"><a href="http://www.wildml.com/deep-learning-glossary/">Deep Learning Glossary</a></li>
<li id="menu-item-732" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-732"><a href="mailto:dennybritz@gmail.com">Contact</a></li>
<li id="menu-item-654" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-654"><a href="http://www.wildml.com/about/">About</a></li>
</ul></div>
</nav><!-- #site-navigation -->
</div><!-- .wrap -->
</div><!-- .navigation-top -->
</header><!-- #masthead -->
<div class="site-content-contain">
<div id="content" class="site-content">
<div class="wrap">
<header class="page-header">
<h1 class="page-title">Category: Reinforcement Learning</h1>		</header><!-- .page-header -->
<div id="primary" class="content-area">
<main id="main" class="site-main" role="main">
<article id="post-1097" class="post-1097 post type-post status-publish format-standard hentry category-deep-learning category-neural-networks category-reinforcement-learning category-trading">
<header class="entry-header">
<div class="entry-meta"><span class="screen-reader-text">Posted on</span> <a href="http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/" rel="bookmark"><time class="entry-date published" datetime="2018-02-10T22:23:37-07:00">February 10, 2018</time><time class="updated" datetime="2018-02-10T23:01:41-07:00">February 10, 2018</time></a></div><!-- .entry-meta --><h2 class="entry-title"><a href="http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/" rel="bookmark">Introduction to Learning to Trade with Reinforcement Learning</a></h2>	</header><!-- .entry-header -->
<div class="entry-content">
<p><em>Thanks a lot to <a href="https://twitter.com/aerinykim">@aerinykim</a>, <a href="https://twitter.com/suzatweet">@suzatweet</a> and <a href="https://twitter.com/hardmaru">@hardmaru</a> for the useful feedback!</em></p>
<p>The academic Deep Learning research community has largely stayed away from the financial markets. Maybe that&#8217;s because the finance industry has a bad reputation, the problem doesn&#8217;t seem interesting from a research perspective, or because data is difficult and expensive to obtain.</p>
<p>In this post, I&#8217;m going to argue that training Reinforcement Learning agents to trade in the financial (and cryptocurrency) markets can be an extremely interesting research problem. I believe that it has not received enough attention from the research community but has the potential to push the state-of-the art of many related fields. It is quite similar to training agents for multiplayer games such as DotA, and many of the same research problems carry over. Knowing virtually nothing about trading, I have spent the past few months working on a project in this field.</p>
<p>This is not a &#8220;price prediction using Deep Learning&#8221; post. So, if you&#8217;re looking for example code and models you may be disappointed. Instead, I want to talk on a more high level about why learning to trade using Machine Learning is difficult, what some of the challenges are, and where I think Reinforcement Learning fits in. If there&#8217;s enough interest in this area I may follow up with another post that includes concrete examples.</p>
<p>I expect most readers to have no background in trading, just like I didn&#8217;t, so I will start out with covering some of the basics. I&#8217;m by no means an expert, so please let me know in the comments so if you find mistakes. I will use cryptocurrencies as a running example in this post, but the same concepts apply to most of the financial markets. The reason to use cryptocurrencies is that data is free, public, and easily accessible. Anyone can sign up to trade. The barriers to trading in the financial markets are a little higher, and data can be expensive. And well, there&#8217;s more hype so it&#8217;s more fun :)</p>
<h2>Basics of Market Microstructure</h2>
<p>Trading in the cryptocurrency (and most financial) markets happens in what&#8217;s called a continuous double auction with an open order book on an exchange. That&#8217;s just a fancy way of saying that there are buyers and sellers that get matched so that they can trade with each other. The exchange is responsible for the matching. There are <a href="https://coinmarketcap.com/exchanges/volume/24-hour/">dozens of exchanges </a>and each may carry slightly different products (such as Bitcoin or Ethereum versus U.S. Dollar). Interface-wise, and in terms of the data they provide, they all look pretty much the same.</p>
<p>Let&#8217;s take a look at GDAX, one of the more popular U.S.-based exchanges. Let&#8217;s assume you want to trade BTC-USD (Bitcoin for U.S. Dollar). You would go to <a href="https://www.gdax.com/trade/BTC-USD">this page</a> and see something like this:</p>
<p><a href="http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-20-at-2.46.01-PM.png"><img class="aligncenter size-full wp-image-1104" src="http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-20-at-2.46.01-PM.png" alt="" width="3584" height="2140" srcset="http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-20-at-2.46.01-PM.png 3584w, http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-20-at-2.46.01-PM-300x179.png 300w, http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-20-at-2.46.01-PM-768x459.png 768w, http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-20-at-2.46.01-PM-1024x611.png 1024w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px" /></a></p>
<p>There&#8217;s a lot of information here, so let&#8217;s go over the basics:</p>
<p><strong>Price chart (Middle)</strong></p>
<p>The current price is the price of the most recent trade. It varies depending on whether that trade was a buy or a sell (more on that below). The price chart is typically displayed as a <a href="https://www.investopedia.com/articles/technical/02/121702.asp">candlestick chart</a> that shows the Open/Start (O), High (H), Low (L) and Close/End (C) prices for a given time window. In the picture above, that period is 5 minutes, but you can change it using the dropdown. The bars below the price chart show the Volume (V), which is the total volume of all trades that happened in that period. The volume is important because it gives you a sense of the <em>liquidity</em> of the market.  If you want to buy $100,000 worth if Bitcoin, but there is nobody willing to sell, the market is <em>illiquid</em>. You simply can&#8217;t buy. A high trade volume indicates that many people are willing to transact, which means that you are likely to able to buy or sell when you want to do so. Generally speaking, the more money you want to invest, the more trade volume you want. Volume also indicates the &#8220;quality&#8221; of a price trend. High volume means you can rely on the price movement more than if there was low volume. High volume is often (but not always, as in the case of market manipulation) the consensus of a large number of market participants.</p>
<p><strong>Trade History (Right)</strong></p>
<p>The right side shows a history of all recent trades. Each trade has a size, price, timestamp, and direction (buy or sell). A trade is a match between two parties, a <em>taker</em> and a <em>maker</em>. More on that below.</p>
<p><strong>Order Book (Left)</strong></p>
<p>The left side shows the order book, which contains information about who is willing to buy and sell at what price. The order book is made up of two sides: <em>Asks</em> (also called offers), and <em>Bids</em>. Asks are people willing to sell, and bids are people willing to buy. By definition, the <em>best ask</em>, the lowest price that someone is willing to sell at, is larger than the <em>best bid</em>, the highest price that someone is willing to buy at. If this was not the case, a trade between these two parties would&#8217;ve already happened. The difference between the best ask and best bid is called the <em>spread</em>.</p>
<p>Each <em>level</em> of the order book has a price and a volume. For example, a volume of 2.0 at a price level of $10,000 means that you can buy 2 BTC for $10,000. If you want to buy more, you would need to pay a higher price for the amount that exceeds 2 BTC. The volume at each level is cumulative, which means that you don&#8217;t know how many people, or orders, that 2 BTC consists of. There could one person selling 2 BTC, or there could be 100 people selling 0.02 BTC each (some exchanges provide this level of information, but most don&#8217;t). Let&#8217;s look at an example:</p>
<p><a href="http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-21-at-7.56.13-PM.png"><img class="aligncenter size-large wp-image-1105" src="http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-21-at-7.56.13-PM-1024x720.png" alt="" width="525" height="369" srcset="http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-21-at-7.56.13-PM-1024x720.png 1024w, http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-21-at-7.56.13-PM-300x211.png 300w, http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-21-at-7.56.13-PM-768x540.png 768w, http://www.wildml.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-21-at-7.56.13-PM.png 1354w" sizes="(max-width: 525px) 100vw, 525px" /></a></p>
<p>So what happens when you send an order to buy 3 BTC? You would be buying (rounding up) 0.08 BTC at $12,551.00, 0.01BTC at $12,551.6 and 2.91 BTC at $12,552.00. On GDAX, you would also be paying a 0.3% taker fee, for a total of about <code>1.003 * (0.08 * 12551 + 0.01 * 12551.6 + 2.91 * 12552) = $37,768.88</code> and an average price per BTC of <code>37768.88 / 3 = $12,589.62</code>. It&#8217;s important to note that what you are actually paying is much higher than $12,551.00, which was the current price! The 0.3% fee on GDAX is extremely high compared to fees in the financial markets, and also much higher than the fees of many other cryptocurrency exchanges, which are often between 0% and 0.1%.</p>
<p>Also note that your buy order has <em>consumed</em> all the volume that was available at the $12,551.00 and $12,551.60 levels. Thus, the order book will &#8220;move up&#8221;, and the best ask will become $12,552.00. The current price will also become $12,552.00, because that is where the last trade happened. Selling works analogously, just that you are now operating on the bid side of the order book, and potentially moving the order book (and price) down. In other words, by placing buy and sell orders, you are removing volume from the order book. If your orders are large enough, you may shift the order book by several levels. In fact, if you placed a very large order for a few million dollars, you would shift the order book and price significantly.</p>
<p>How do orders get into the order book? That&#8217;s the difference between <em>market</em> and <em>limit</em> orders. In the above example, you&#8217;ve issued a <em>market order</em>, which basically means &#8220;Buy/Sell X amount of BTC at the best price possible, <em>right now</em>&#8220;. If you are not careful about what&#8217;s in the order book you could end up paying significantly more than the current price shows. For example, imagine that most of the lower levels in the order book only had a volume at 0.001 BTC available. Most of your buy volume would then get matched at a much higher, more expensive, price level. If you submit a <em>limit order</em>, also called a passive order, you specify the price and quantity you&#8217;re willing to buy or sell at. The order will be placed into the book, and you can cancel it as long as it has not been matched.  For example, let&#8217;s assume the Bitcoin price is at $10,000, but you want to sell at $10,010. You place a limit order. First, nothing happens. If the price keeps moving down your order will just sit there, do nothing, and will never be matched. You can cancel it anytime. However, if the price moves up, your order will at some point become the best price in the book, and the next person submitting a market order for a sufficient quantity will match it.</p>
<p><img class="alignnone  wp-image-2018" src="http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-05-at-9.58.15-PM.png" alt="limit_order" width="156" height="347" srcset="http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-05-at-9.58.15-PM.png 442w, http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-05-at-9.58.15-PM-135x300.png 135w" sizes="(max-width: 156px) 100vw, 156px" /></p>
<p>Market orders <em>take liquidity</em> from the market. By matching with orders from the order book, you are taking away the option to trade to from other people &#8211; there&#8217;s less volume left! That&#8217;s also why market orders, or <em>market takers</em>, often need to pay higher fees than<em> market makers</em>, who put orders into the book. Limit orders <em>providing liquidity</em> because they are giving others the option to trade. At the same time, limit orders guarantee that you will not pay more than the price specified in the limit order. However, you don&#8217;t know when, or if, someone will match your order. You are also giving the market information about what you believe the price should be. This can also be used to manipulate the other participants in the market, who may act a certain way based on the orders you are executing or putting into the book. Because they provide the option to trade and give away information, market makers typically pay lower fees than market takers. Some exchanges also provide stop orders, which allow you to set a maximum price for your market orders.</p>
<p>This was a very short introduction of how order books works and matching works. There are many more subtleties as well other, much more complex, order types. If the above was not clear, you can find a wealth of information about order book mechanics, and research in that area, through Google.</p>
<h2>Data</h2>
<p>The main reasons I am using cryptocurrencies in this post is because data is public, free, and easy to obtain. Most exchanges have streaming APIs that allow you to receive market updates in real-time. We&#8217;ll use GDAX (<a href="https://docs.gdax.com/">API Documentation</a>) as an example again, but the data for other exchanges looks very similar. Let&#8217;s go over the basic types of events you would use to build a Machine Learning model.</p>
<p><strong>Trade</strong></p>
<p>A new Trade has happened. Each trade has a timestamp, a unique ID assigned by the exchange, a price, size, and side, as discussed above. If you wanted to plot the price graph of an asset, you would simply plot the price of all trades. If you wanted to plot the candlestick chart, you would window the trade events for a certain period, such as five minutes, and then plot the windows.</p>
<p></p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->
<div id="crayon-5cbacbcfeca51788198441" class="crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover wrap" style=" margin-top: 12px; margin-bottom: 12px; margin-left: 12px; font-size: 12px !important; line-height: 15px !important;">
<div class="crayon-plain-wrap"></div>
<div class="crayon-main" style="">
<table class="crayon-table">
<tr class="crayon-row">
<td class="crayon-nums " data-settings="show">
<div class="crayon-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="crayon-5cbacbcfeca51788198441-1">1</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca51788198441-2">2</div><div class="crayon-num" data-line="crayon-5cbacbcfeca51788198441-3">3</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca51788198441-4">4</div><div class="crayon-num" data-line="crayon-5cbacbcfeca51788198441-5">5</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca51788198441-6">6</div><div class="crayon-num" data-line="crayon-5cbacbcfeca51788198441-7">7</div></div>
</td>
<td class="crayon-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="crayon-5cbacbcfeca51788198441-1"><span class="crayon-sy">{</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca51788198441-2"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"time"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"2014-11-07T22:19:28.578544Z"</span><span class="crayon-sy">,</span></div><div class="crayon-line" id="crayon-5cbacbcfeca51788198441-3"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"trade_id"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">74</span><span class="crayon-sy">,</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca51788198441-4"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"price"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"10.00000000"</span><span class="crayon-sy">,</span></div><div class="crayon-line" id="crayon-5cbacbcfeca51788198441-5"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"size"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"0.01000000"</span><span class="crayon-sy">,</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca51788198441-6"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"side"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"buy"</span></div><div class="crayon-line" id="crayon-5cbacbcfeca51788198441-7"><span class="crayon-sy">}</span></div></div></td>
</tr>
</table>
</div>
</div>
<!-- [Format Time: 0.0056 seconds] -->
<p></p>
<p><strong>BookUpdate</strong></p>
<p>One or more levels in the order book were updated. Each level is made up of the side (Buy=Bid, Sell=Ask), the price/level, and the new quantity at that level. Note that these are changes, or deltas, and you must construct the full order book yourself by merging them.</p>
<p></p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->
<div id="crayon-5cbacbcfeca62337582208" class="crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover wrap" style=" margin-top: 12px; margin-bottom: 12px; margin-left: 12px; font-size: 12px !important; line-height: 15px !important;">
<div class="crayon-plain-wrap"></div>
<div class="crayon-main" style="">
<table class="crayon-table">
<tr class="crayon-row">
<td class="crayon-nums " data-settings="show">
<div class="crayon-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="crayon-5cbacbcfeca62337582208-1">1</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca62337582208-2">2</div><div class="crayon-num" data-line="crayon-5cbacbcfeca62337582208-3">3</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca62337582208-4">4</div><div class="crayon-num" data-line="crayon-5cbacbcfeca62337582208-5">5</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca62337582208-6">6</div><div class="crayon-num" data-line="crayon-5cbacbcfeca62337582208-7">7</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca62337582208-8">8</div><div class="crayon-num" data-line="crayon-5cbacbcfeca62337582208-9">9</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca62337582208-10">10</div></div>
</td>
<td class="crayon-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="crayon-5cbacbcfeca62337582208-1"><span class="crayon-sy">{</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca62337582208-2"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"type"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"l2update"</span><span class="crayon-sy">,</span></div><div class="crayon-line" id="crayon-5cbacbcfeca62337582208-3"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"product_id"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"BTC-USD"</span><span class="crayon-sy">,</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca62337582208-4"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"changes"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-sy">[</span></div><div class="crayon-line" id="crayon-5cbacbcfeca62337582208-5"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-sy">[</span><span class="crayon-s">"buy"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"10000.00"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"3"</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca62337582208-6"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-sy">[</span><span class="crayon-s">"sell"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"10000.03"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"1"</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span></div><div class="crayon-line" id="crayon-5cbacbcfeca62337582208-7"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-sy">[</span><span class="crayon-s">"sell"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"10000.04"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"2"</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca62337582208-8"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-sy">[</span><span class="crayon-s">"sell"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"10000.07"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"0"</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="crayon-5cbacbcfeca62337582208-9"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca62337582208-10"><span class="crayon-sy">}</span></div></div></td>
</tr>
</table>
</div>
</div>
<!-- [Format Time: 0.0002 seconds] -->
<p></p>
<p><strong>BookSnapshot</strong></p>
<p>Similar to a BookUpdate, but a snapshot of the complete order book. Because the full order book can be very large, it is faster and more efficient to use the BookUpdate events instead. However, having an occasional snapshot can be useful.</p>
<p></p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->
<div id="crayon-5cbacbcfeca64643513161" class="crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover wrap" style=" margin-top: 12px; margin-bottom: 12px; margin-left: 12px; font-size: 12px !important; line-height: 15px !important;">
<div class="crayon-plain-wrap"></div>
<div class="crayon-main" style="">
<table class="crayon-table">
<tr class="crayon-row">
<td class="crayon-nums " data-settings="show">
<div class="crayon-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="crayon-5cbacbcfeca64643513161-1">1</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca64643513161-2">2</div><div class="crayon-num" data-line="crayon-5cbacbcfeca64643513161-3">3</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca64643513161-4">4</div><div class="crayon-num" data-line="crayon-5cbacbcfeca64643513161-5">5</div><div class="crayon-num crayon-striped-num" data-line="crayon-5cbacbcfeca64643513161-6">6</div></div>
</td>
<td class="crayon-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="crayon-5cbacbcfeca64643513161-1"><span class="crayon-sy">{</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca64643513161-2"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"type"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"snapshot"</span><span class="crayon-sy">,</span></div><div class="crayon-line" id="crayon-5cbacbcfeca64643513161-3"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"product_id"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-s">"BTC-EUR"</span><span class="crayon-sy">,</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca64643513161-4"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"bids"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-s">"10000.00"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"2"</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span></div><div class="crayon-line" id="crayon-5cbacbcfeca64643513161-5"><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-s">"asks"</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-s">"10000.02"</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">"3"</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="crayon-5cbacbcfeca64643513161-6"><span class="crayon-sy">}</span></div></div></td>
</tr>
</table>
</div>
</div>
<!-- [Format Time: 0.0002 seconds] -->
<p></p>
<p>That&#8217;s pretty much all you need in terms of market data. A stream of the above events contains all the information you saw in the GUI interface. You can imagine how you could make prediction based on a stream of the above events.</p>
<h2>A few Trading Strategy Metrics</h2>
<p>When developing trading algorithms, what do you optimize for? The obvious answer is profit, but that&#8217;s not the whole story. You also need to compare your trading strategy to baselines, and compare its risk and volatility to other investments. Here are a few of the most basic metrics that traders are using. I won&#8217;t go into detail here, so feel free to follow the links for more information.</p>
<p><strong>Net PnL (Net Profit and Loss)</strong></p>
<p>Simply how much money an algorithm makes (positive) or loses (negative) over some period of time, minus the trading fees.</p>
<p><strong>Alpha and Beta</strong></p>
<p><a href="https://en.wikipedia.org/wiki/Alpha_(finance)">Alpha</a> defines how much better, in terms of profit, your strategy is when compared to an alternative, relatively risk-free, investment, like a government bond. Even if your strategy is profitable, you could be better off investing in a risk-free alternative. <a href="https://en.wikipedia.org/wiki/Beta_(finance)">Beta</a> is closely related, and tells you how volatile your strategy is compared to the market. For example, a beta of 0.5 means that your investment moves $1 when the market moves $2.</p>
<p><strong>Sharpe Ratio</strong></p>
<p>The <a href="https://en.wikipedia.org/wiki/Sharpe_ratio#Use_in_finance">Sharpe Ratio</a> measures the excess return per unit of risk you are taking. It&#8217;s basically your return on capital over the standard deviation, adjusted for risk. Thus, the higher the better. It takes into account both the volatility of your strategy, as well as an alternative risk-free investment.</p>
<p><b>Maximum Drawdown</b></p>
<p>The <a href="https://en.wikipedia.org/wiki/Drawdown_(economics)">Maximum Drawdown</a> is the maximum difference between a local maximum and the subsequent local minimum, another measure of risk. For example, a maximum drawdown of 50% means that you lose 50% of your capital at some point. You then need to make a 100% return to get back to your original amount of capital. Clearly, a lower maximum drawdown is better.</p>
<p><b>Value at Risk (VaR)</b></p>
<p><a href="https://en.wikipedia.org/wiki/Value_at_risk">Value at Risk</a> is a risk metric that quantifies how much capital you may lose over a given time frame with some probability, assuming normal market conditions. For example, a 1-day 5% VaR of 10% means that there is a 5% chance that you may lose more than 10% of an investment within a day.</p>
<h2>Supervised Learning</h2>
<p>Before looking at the problem from a Reinforcement Learning perspective, let&#8217;s understand how we would go about creating a profitable trading strategy using a supervised learning approach. Then we will see what&#8217;s problematic about this, and why we may want to use Reinforcement Learning techniques.</p>
<p>The most obvious approach we can take is price prediction. If we can predict that the market will move up we can buy now, and sell once the market has moved. Or, equivalently, if we predict the market goes down, we can go short (borrowing an asset we don&#8217;t own) and then buy once the market has moved. However, there are a few problems with this.</p>
<p>First of all, what price do we actually predict? As we&#8217;ve seen above, there is not a &#8220;single&#8221; price we are buying at. The final price we pay depends on the volume available at different levels of the order book, and the fees we need to pay. A naive thing to do is to predict the <em>mid price</em>, which is the mid-point between the <em>best bid</em> and <em>best ask</em>. That&#8217;s what most researchers do. However, this is just a theoretical price, not something we can actually execute orders at, and could differ significantly from the real price we&#8217;re paying.</p>
<p>The next question is time scale. Do we predict the price of the next trade? The price at the next second? Minute? Hour? Day? Intuitively, the further in the future we want to predict, the more uncertainty there is, and the more difficult the prediction problem becomes.</p>
<p>Let&#8217;s look at an example. Let&#8217;s assume the BTC price is $10,000 and we can accurately predict that the &#8220;price&#8221; moves up from $10,000 to $10,050 in the next minute. So, does that mean you can make $50 of profit by buying and selling? Let&#8217;s understand why it doesn&#8217;t.</p>
<ul>
<li>We buy when the best ask is $10,000. Most likely we will not be able to get all our 1.0 BTC filled at that price because the order book does not have the required volume. We may be forced to buy 0.5 BTC at $10,000 and 0.5 BTC at $10,010, for an average price of $10,005. On GDAX, we also pay a 0.3% taker fee, which corresponds to roughly $30.</li>
<li>The price is now at $10,050, as predicted. We place the sell order. Because the market moves very fast, by the time the order is delivered over the network the price has slipped already. Let&#8217;s say it&#8217;s now at $10,045. Similar to above, we most likely cannot sell all of your 1 BTC at that price. Perhaps we are forced to sell 0.5 BTC are $10,045 and 0.5 BTC at $10,040, for an average price of $10,042.5. Then we pay another 0.3% taker fee, which corresponds to roughly $30.</li>
</ul>
<p>So, how much money have we made? <code>-10005 - 30 - 30 + 10,042.5 = -$22.5</code>. Instead of making $50, we have lost $22.5, even though we accurately predicted a large price movement over the next minute! In the above example there were three reasons for this: No liquidity in the best order book levels, network latencies, and fees, none of which the supervised model could take into account.</p>
<p>What is the lesson here? In order to make money from a simple price prediction strategy, we must predict relatively large price movements over longer periods of time, or be very smart about our fees and order management. And that&#8217;s a very difficult prediction problem. We could have saved on the fees by using limit instead of market orders, but then we would have no guarantees about our orders being matched, and we would need to build a complex system for order management and cancellation.</p>
<p>But there&#8217;s another problem with supervised learning: It does not imply a <em>policy</em>. In the above example we bought because we predicted that the price moves up, and it actually moved up. Everything went according to plan. But what if the price had moved down? Would you have sold? Kept the position and waited? What if the price had moved up just a little bit and then moved down again? What if we had been uncertain about the prediction, for example 65% up and 35% down? Would you still have bought? How do you choose the threshold to place an order?</p>
<p>Thus, you need more than just a price prediction model (unless your model is extremely accurate and robust). We also need a <em>rule-based policy</em> that takes as input your price predictions and decides what to actually do: Place an order, do nothing, cancel an order, and so on. How do we come up with such a policy? How do we optimize the policy parameters and decision thresholds? The answer to this is not obvious, and many people use simple heuristics or human intuition.</p>
<h2>A Typical Strategy Development Workflow</h2>
<p>Luckily, there are solutions to many of the above problems. The bad news is, the solutions are not very effective. Let&#8217;s look a typical workflow for trading strategy development. It looks something like this:</p>
<p><img class="alignnone size-full wp-image-1764" src="http://www.wildml.com/wp-content/uploads/2018/02/supervised_strat_development.png" alt="Supervised Trading Strategy Development" width="821" height="136" srcset="http://www.wildml.com/wp-content/uploads/2018/02/supervised_strat_development.png 821w, http://www.wildml.com/wp-content/uploads/2018/02/supervised_strat_development-300x50.png 300w, http://www.wildml.com/wp-content/uploads/2018/02/supervised_strat_development-768x127.png 768w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px" /></p>
<ol>
<li><strong>Data Analysis</strong>: You perform exploratory data analysis to find trading opportunities. You may look at various charts, calculate data statistics, and so on. The output of this step is an &#8220;idea&#8221; for a trading strategy that should be validated.</li>
<li><strong>Supervised Model Training:</strong> If necessary, you may train one or more supervised learning models to predict quantities of interest that are necessary for the strategy to work. For example, price prediction, quantity prediction, etc.</li>
<li><strong>Policy Development</strong>: You then come up with a rule-based policy that determines what actions to take based on the current state of the market and the outputs of supervised models. Note that this policy may also have parameters, such as decision thresholds, that need to be optimized. This optimization is done later.</li>
<li><strong>Strategy Backtesting:</strong> You use a simulator to test an initial version of the strategy against a set of historical data. The simulator can take things such as order book liquidity, network latencies, fees, etc into account.  If the strategy performs reasonably well in backtesting, we can move on and do parameter optimization.</li>
<li><strong>Parameter Optimization:</strong> You can now perform a search, for example a grid search, over possible values of strategy parameters like thresholds or coefficient, again using the simulator and a set of historical data. Here, overfitting to historical data is a big risk, and you must be careful about using proper validation and test sets.</li>
<li><strong>Simulation &#038; Paper Trading:</strong> Before the strategy goes live, simulation is done on new market data, in real-time. That&#8217;s called paper trading and helps prevent overfitting. Only if the strategy is successful in paper trading, it is deployed in a live environment.</li>
<li><strong>Live Trading:</strong> The strategy is now running live on an exchange.</li>
</ol>
<p>That&#8217;s a complex process. It may vary slightly depending on the firm or researcher, but something along those lines typically happens when new trading strategies are developed. Now, why do I think this process is not effective? There are a couple of reasons.</p>
<ol>
<li>Iteration cycles are slow. Step 1-3 are largely based on intuition, and you don&#8217;t know if your strategy works until the optimization in step 4-5 is done, possibly forcing you to start from scratch. In fact, every step comes with the risk of failing and forcing you to start from scratch.</li>
<li>Simulation comes too late. You do not explicitly take into account environmental factors such as latencies, fees, and liquidity until step 4. Shouldn&#8217;t these things directly inform your strategy development or the parameters of your model?</li>
<li> Policies are developed independently from supervised models even though they interact closely. Supervised predictions are an input to the policy. Wouldn&#8217;t it make sense to jointly optimize them?</li>
<li>Policies are simple. They are limited to what humans can come up with.</li>
<li>Parameter optimization is inefficient. For example, let&#8217;s assume you are optimizing for a combination of profit and risk, and you want to find parameters that give you a high <a href="https://www.investopedia.com/articles/07/sharpe_ratio.asp">Sharpe Ratio</a>. Instead of using an efficient gradient-based approach you are doing an inefficient grid search and hope that you&#8217;ll find something good (while not overfitting).</li>
</ol>
<p>Let&#8217;s take a look at how a Reinforcement Learning approach can solve most of these problems.</p>
<h2>Deep Reinforcement Learning for Trading</h2>
<p>Remember that the traditional Reinforcement Learning problem can be formulated as a Markov Decision Process (MDP). We have an agent acting in an environment. Each time step <img src="//s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="t" title="t" class="latex" /> the agent receives as the input the current state <img src="//s0.wp.com/latex.php?latex=S_t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="S_t" title="S_t" class="latex" />, takes an action <img src="//s0.wp.com/latex.php?latex=A_t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="A_t" title="A_t" class="latex" />, and receives a reward <img src="//s0.wp.com/latex.php?latex=R_%7Bt%2B1%7D&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="R_{t+1}" title="R_{t+1}" class="latex" /> and the next state <img src="//s0.wp.com/latex.php?latex=S_%7Bt%2B1%7D&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="S_{t+1}" title="S_{t+1}" class="latex" />. The agent chooses the action based on some policy <img src="//s0.wp.com/latex.php?latex=%5Cpi&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="&#92;pi" title="&#92;pi" class="latex" />: <img src="//s0.wp.com/latex.php?latex=A_t+%3D+%5Cpi%28S_t%29&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="A_t = &#92;pi(S_t)" title="A_t = &#92;pi(S_t)" class="latex" />. It is our goal to find a policy that maximizes the cumulative reward <img src="//s0.wp.com/latex.php?latex=%5Csum+R_t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="&#92;sum R_t" title="&#92;sum R_t" class="latex" /> over some finite or infinite time horizon.</p>
<p><img class="alignnone size-full wp-image-1403" src="http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-03-at-11.07.07-PM.png" alt="Reinforcement Learning" width="1688" height="566" srcset="http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-03-at-11.07.07-PM.png 1688w, http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-03-at-11.07.07-PM-300x101.png 300w, http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-03-at-11.07.07-PM-768x258.png 768w, http://www.wildml.com/wp-content/uploads/2018/02/Screen-Shot-2018-02-03-at-11.07.07-PM-1024x343.png 1024w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px" /></p>
<p>Let&#8217;s try to understand what these symbols correspond to in the trading setting.</p>
<p><strong>Agent</strong></p>
<p>Let&#8217;s start with the easy part. The agent is our trading agent. You can think of the agent as a human trader who opens the GUI of an exchange and makes trading decision based on the current state of the exchange and his or her account.</p>
<p><strong>Environment</strong></p>
<p>Here it gets a little hairy. The obvious answer would be that the exchange is our environment. But the important thing to note is that there are many other agents, both human and algorithmic market players, trading on the same exchange. Let&#8217;s assume for a moment that we are taking actions on a minutely scale (more on that below). We take some action, wait a minute, get a new state, take another action, and so on. When we observe a new state it will be the response of the market environment, which includes the response of the other agents. Thus, from the perspective of our agent, these agents are also part of the environment. They&#8217;re not something we can control.</p>
<p>However, by putting other agents together into some big complex environment we lose the ability to explicitly model them. For example, one can imagine that we could learn to reverse-engineer the algorithms and strategies that other traders are running and then learn to exploit them. Doing so would put us into a Multi-Agent Reinforcement Learning (MARL) problem setting, which is an active research area. I&#8217;ll talk more about that below. For simplicity, let&#8217;s just assume we don&#8217;t do this, and assume we&#8217;re interacting with a single complex environment that includes the behavior of all other agents.</p>
<p><strong>State</strong></p>
<p>In the case of trading on an exchange, we do not observe the complete state of the environment. For example, we don&#8217;t know about the other agents are in the environment, how many there are, what their account balances are, or what their open limit orders are. This means, we are dealing with a Partially Observable Markov Decision Process (POMDP). What the agent observes is not the actual state <img src="//s0.wp.com/latex.php?latex=S_t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="S_t" title="S_t" class="latex" /> of the environment, but some derivation of that. Let&#8217;s call that the observation <img src="//s0.wp.com/latex.php?latex=X_t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="X_t" title="X_t" class="latex" />, which is calculated using some function of the full state <img src="//s0.wp.com/latex.php?latex=X_t+%5Csim+O%28S_t%29&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="X_t &#92;sim O(S_t)" title="X_t &#92;sim O(S_t)" class="latex" />.</p>
<p>In our case, the observation at each timestep <img src="//s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="t" title="t" class="latex" /> is simply the history of all exchange events (described in the data section above) received up to time <img src="//s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=000&#038;s=0" alt="t" title="t" class="latex" />. This event history can be used to build up the current exchange state. However, in order for our agent to make decisions, there are a few other things that the observation must include, such as the current account balance, and open limit orders, if any.</p>
<p><strong>Time Scale</strong></p>
<p>We need to decide what time scale we want to act on. Days? Hours? Minutes? Seconds? Milliseconds? Nanoseconds? Variables scales? All of these require different approaches. Someone buying an asset and holding it for several days, weeks or months is often making a long-term bet based on analysis, such as &#8220;Will Bitcoin be successful?&#8221;. Often, these decisions are driven by external events, news, or a fundamental understanding of the assets value or potential. Because such an analysis typically requires an understanding of how the world works, it can be difficult to automate using Machine Learning techniques. On the opposite end, we have High Frequency Trading (HFT) techniques, where decisions are based almost entirely on market microstructure signals. Decisions are made on nanosecond timescales and trading strategies use dedicated connections to exchanges and extremely fast but simple algorithms running of FPGA hardware. Another way to think about these two extremes is in term of &#8220;humanness&#8221;. The former requires a big picture view and an understanding of how the world works, human intuition and high-level analysis, while the latter is all about simple, but extremely fast, pattern matching.</p>
<p>Neural Networks are popular because, given a lot of data, they can learn more complex representations than algorithms such as Linear Regression or Naive Bayes. But Deep Neural Nets are also slow, relatively speaking. They can&#8217;t make predictions on nanosecond time scales and thus cannot compete with the speed of HFT algorithms. That&#8217;s why I think the sweet spot is somewhere in the middle of these two extremes. We want to act on a time scale where we can analyze data faster than a human possibly could, but where being smarter allows us to beat the &#8220;fast but simple&#8221; algorithms. My guess, and it really is just a guess, is that this corresponds to acting on timescales somewhere between a few milliseconds and a few minutes. Humans traders can act on these timescales as well, but not as quickly as algorithms. And they certainly cannot synthesize the same amount of information that an algorithm can in that same time period. That&#8217;s our advantage.</p>
<p>Another reason to act on relatively short timescales is that patterns in the data may be more apparent. For example, because most human traders look at the exact same (limited) graphical user interfaces which have pre-defined market signals (like the <a href="https://www.investopedia.com/terms/m/macd.asp">MACD</a> signal that is built into many exchange GUIs), their actions are restricted to the information present in those signals, resulting in certain action patterns. Similarly, algorithms running in the market act based on certain patterns. Our hope is that Deep RL algorithms can pick up those patterns and exploit them.</p>
<p>Note that we could also act on variable time scales, based on some signal trigger. For example, we could decide to take an action whenever a large trade occurred in the market. Such as trigger-based agent would still roughly correspond to some time scale, depending on the frequency of the trigger event.</p>
<p><strong>Action Space</strong></p>
<p>In Reinforcement Learning, we make a distinction between discrete (finite) and continuous (infinite) action spaces. Depending on how complex we want our agent to be, we have a couple of choices here. The simplest approach would be to have three actions: Buy, Hold, and Sell. That works, but it limits us to placing market orders and to invest a deterministic amount of money at each step.  The next level of complexity would be to let our agent learn how much money to invest, for example, based on the uncertainty of our model. That would put us into a continuous action space, as we need to decide on both the (discrete) action and the (continuous) quantity. An even more complex scenario arises when we want our agent to be able to place limit orders. In that case our agent must decide the level (price) and the quantity of the order, both of which are continuous quantities. It must also be able to cancel open orders that have not yet been matched.</p>
<p><strong>Reward Function</strong></p>
<p>This is another tricky one. There are several possible reward functions we can pick from. An obvious one would the <em>Realized PnL</em> (Profit and Loss). The agent receives a reward whenever it <em>closes</em> a position, e.g. when it sells an asset it has previously bought, or buys an asset it has previously borrowed. The <em>net profit</em> from that trade can be positive or negative. That&#8217;s the reward signal. As the agent maximizes the total cumulative reward, it learns to trade profitably. This reward function is technically correct and leads to the optimal policy in the limit. However, rewards are sparse because buy and sell actions are relatively rare compared to doing nothing. Hence, it requires the agent to learn without receiving frequent feedback.</p>
<p>An alternative with more frequent feedback would be the <em>Unrealized PnL</em>, which the net profit the agent would get if it were to close all of its positions immediately. For example, if the price went down after the agent placed a buy order, it would receive a negative reward even though it hasn&#8217;t sold yet. Because the Unrealized PnL may change at each time step, it gives the agent more frequent feedback signals. However, the direct feedback may also bias the agent towards short-term actions when used in conjunction with a decay factor.</p>
<p>Both of these reward functions naively optimize for profit. In reality, a trader may want to minimize risk. A strategy with a slightly lower return but significantly lower volatility is preferably over a highly volatile but only slightly more profitable strategy. Using the <a href="https://www.quantinsti.com/blog/sharpe-ratio-applications-algorithmic-trading/">Sharpe Ratio</a> is one simple way to take risk into account, but there are many others. We may also want to take into account something like <a href="https://www.investopedia.com/terms/d/drawdown.asp">Maximum Drawdown</a>, described above.  One can image a wide range of complex reward function that trade-off between profit and risk.</p>
<h2>The Case for Reinforcement Learning</h2>
<p>Now that we have an idea of how Reinforcement Learning can be used in trading, let&#8217;s understand why we want to use it over supervised techniques. Developing trading strategies using RL looks something like this. Much simpler, and more principled than the approach we saw in the previous section.</p>
<p><img class="alignnone size-full wp-image-1799" src="http://www.wildml.com/wp-content/uploads/2018/02/rl_strat_dev.png" alt="Trading Strategy Development with RL" width="461" height="129" srcset="http://www.wildml.com/wp-content/uploads/2018/02/rl_strat_dev.png 461w, http://www.wildml.com/wp-content/uploads/2018/02/rl_strat_dev-300x84.png 300w" sizes="(max-width: 461px) 100vw, 461px" /></p>
<p><b>End-to-End Optimization of what we care about</b></p>
<p>In the traditional strategy development approach we must go through several steps, a pipeline, before we get to the metric we actually care about. For example, if we want to find a strategy with a maximum drawdown of 25%, we need to train supervised model, come up with a rule-based policy using the model, backtest the policy and optimize its hyperparameters, and finally assess its performance through simulation.</p>
<p>Reinforcement Learning allows for end-to-end optimization and maximizes (potentially delayed) rewards. By adding a term to the reward function, we can for example <em>directly</em> optimize for this drawdown, without needing to go through separate stages. For example, you could imagine giving a large negative reward whenever a drawdown of more than 25% happens, forcing the agent to look for a different policy. Of course, we can combine drawdown with many other metrics you care about. This is not only easier, but also a much more powerful model.</p>
<p><b>Learned Policies</b></p>
<p>Instead of needing to hand-code a rule-based policy, Reinforcement Learning directly learns a policy. There&#8217;s no need for us to specify rules and thresholds such as &#8220;buy when you are more than 75% sure that the market will move up&#8221;. That&#8217;s baked in the RL policy, which optimizes for the metric we care about. We&#8217;re removing a full step from the strategy development process! And because the policy can be parameterized by a complex model, such as a Deep Neural network, we can learn policies that are more complex and powerful than any rules a human trader could possibly come up with. And as we&#8217;ve seen above, the policies implicitly take into account metrics such as risk, if that&#8217;s something we&#8217;re optimizing for.</p>
<p><strong>Trained directly in Simulation Environments</strong></p>
<p>We needed separate backtesting and parameter optimization steps because it was difficult for our strategies to take into account environmental factors, such as order book liquidity, fee structures, latencies, and others, when using a supervised approach. It is not uncommon to come up with a strategy, only to find out much later that it does not work, perhaps because the latencies are too high and the market is moving too quickly so that you cannot get the trades you expected to get.</p>
<p>Because Reinforcement Learning agents are trained in a simulation, and that simulation can be as complex as you want, taking into account latencies, liquidity and fees, we don&#8217;t have this problem! Getting around environmental limitations is part of the optimization process. For example, if we simulate the latency in the Reinforcement Learning environment, and this results in the agent making a mistake, the agent will get a negative reward, forcing it to learn to work around the latencies.</p>
<p>We could take this a step further and simulate the response of the other agents in the same environment, to model impact of our own orders, for example. If the agent&#8217;s actions move the price in a simulation that&#8217;s based on historical data, we don&#8217;t know how the real market would have responded to this. Typically, simulators ignore this and assume that orders do not have market impact. However, by learning a model of the environment and performing rollouts using techniques like a Monte Carlo Tree Search (MCTS), we could take into account potential reactions of the market (other agents). By being smart about the data we collect from the live environment, we can continuously improve our model. There exists an interesting exploration/exploitation tradeoff here: Do we act optimally in the live environment to generate profits, or do we act suboptimally to gather interesting information that we can use to improve the model of our environment and other agents?</p>
<p><em>That&#8217;s a very powerful concept. By building an increasingly complex simulation environment that models the real world you can train very sophisticated agents that learn to take environment constraints into account.</em></p>
<p><strong>Learning to adapt to market conditions</strong></p>
<p>Intuitively, certain strategies and policies will work better in some market environments than others. For example, a strategy may work well in a bearish environment, but lose money in a bullish environment. Partly, this is due to the simplistic nature of the policy, which does not have a parameterization powerful enough to learn to adapt to changing market conditions.</p>
<p>Because RL agents are learning powerful policies parameterized by Neural Networks, they can also learn to adapt to various market conditions by seeing them in historical data, given that they are trained over a long time horizon and have sufficient memory. This allows them to be much more robust to changing markets. In facts, we can directly optimize them to become robust to changes in market conditions, by putting appropriate penalties into your reward function.</p>
<p><strong>Ability to model other agents</strong></p>
<p>A unique ability of Reinforcement Learning is that we can explicitly take into account other agents. So far we&#8217;ve always talked about &#8220;how the market reacts&#8221;, ignoring that the market is really just a group of agents and algorithms, just like us. However, if we explicitly modeled the other agents in the environment, our agent could learn to exploit their strategies. In essence, we are reformulating the problem from &#8220;market prediction&#8221; to &#8220;agent exploitation&#8221;. This is much more similar to what we are doing in multiplayer games, like DotA.</p>
<h2>The Case for Trading Agent Research</h2>
<p>My goal with this post is not only to give an introduction to Reinforcement Learning for Trading, but also to convince more researchers to take a look at the problem. Let&#8217;s take a look what makes Trading an interesting research problem.</p>
<p><strong>Live Testing and Fast Iteration Cycle</strong></p>
<p>When training Reinforcement Learning agents, it is often difficult or expensive to deploy them in the real world and get feedback. For example, if you trained an agent to play Starcraft 2, how would you let it play against a larger number of human players? Same for Chess, Poker, or any other game that is popular in the RL community. You would probably need to somehow enter a tournament and let your agent play there.</p>
<p>Trading agents have characteristics very similar to those for multiplayer games. But you can easily test them live! You can deploy your agent on an exchange through their API and immediately get real-world market feedback. If your agent does not generalize and loses money you know that you have probably overfit to the training data. In other words, the iteration cycle can be extremely fast.</p>
<p><strong>Large Multiplayer Environments</strong></p>
<p>The trading environment is essentially a multiplayer game with thousands of agents acting simultaneously. This is an active research area. We are now making progress at multiplayer games such as Poker, Dota2, and others, and many of the same techniques will apply here. In fact, the trading problem is a much more difficult one due to the sheer number of simultaneous agents who can leave or join the game at any time. Understanding how to build models of other agents is only one possible direction one can go into. As mentioned earlier, one could choose to perform actions in a live environment with the goal maximizing the information gain with respect to kind policies the other agents may be following.</p>
<p><strong>Learning to Exploit other Agents &amp; Manipulate the Market</strong></p>
<p>Closely related is the question of whether we can learn to exploit other agents acting in the environment. For example, if we knew exactly what algorithms were running in the market we can trick them into taking actions they should not take and profit from their mistakes. This also applies to human traders, who typically act based on a combination of well-known market signals, such as exponential moving averages or order book pressures.</p>
<p></p>
<p><small>Disclaimer: Don&#8217;t allow your agent to do anything illegal! Do comply with all applicable laws in your jurisdiction. And finally, past performance is no guarantee of future results.</small></p>
<p><strong>Sparse Rewards &amp; Exploration</strong></p>
<p>Trading agents typically receive sparse rewards from the market. Most of the time you will do nothing. Buy and sell actions typically account for a tiny fraction of all actions you take. Naively applying &#8220;reward-hungry&#8221; Reinforcement Learning algorithms will fail. This opens up the possibility for new algorithms and techniques, especially model-based ones, that can efficiently deal with sparse rewards.</p>
<p>A similar argument can be made for exploration. Many of today&#8217;s standard algorithms, such as DQN or A3C, use a very naive approach to exploration, basically adding random noise to the policy. However, in the trading case, most states in the environment are bad, and there are only a few good ones. A naive random approach to exploration will almost never stumble upon those good state-actions pairs. A new approach is necessary here.</p>
<p><b>Multi-Agent Self-Play</b></p>
<p>Similar to how self-play is applied to two-player games such as Chess or Go, one could apply self-play techniques to a multiplayer environment. For example, you could imagine simultaneously training a large number of competing agents, and investigate whether the resulting market dynamic somehow resembles the dynamics found in the real world. You could also mix the types of agents you are training, from different RL algorithms, the evolution-based ones, and deterministic ones. One could also use the real-world market data as a supervised feedback signal to &#8220;force&#8221; the agents in the simulation to collectively behave like the real world.</p>
<p><b>Continuous Time</b></p>
<p>Because markets change on micro- to milliseconds times scales, the trading domain is a good approximation of a continuous time domain. In our example above we&#8217;ve fixed a time period and made that decision for the agent. However, you could imagine making this part of the agent training. Thus, the agent would not only decide what actions to take, but also <em>when</em> to take an action.  Again, this is an active research area useful for many other domains, including robotics.</p>
<p><b>Nonstationary, Lifelong Learning, and Catastrophic Forgetting</b></p>
<p>The trading environment is inherently nonstationary. Market conditions change and other agent join, leave, and constantly change their strategies. Can we train agents that learn to automatically adjust to changing market conditions, without &#8220;forgetting&#8221; what they have learned before? For example, can an agent successfully transition from a bear to a bull market and then back to a bear market, without needing to be re-trained? Can an agent adjust to other agent joining and learning to exploit them automatically?</p>
<p><strong>Transfer Learning and Auxiliary Tasks</strong></p>
<p>Training Reinforcement Learning from scratch in complex domains can take a very long time because they not only need to learn to make good decisions, but they also need to learn the &#8220;rules of the game&#8221;. There are many ways to speed up the training of Reinforcement Learning agents, including transfer learning, and using auxiliary tasks. For example, we could imagine pre-training an agent with an expert policy, or adding <a href="https://arxiv.org/abs/1611.05397">auxiliary tasks</a>, such as price prediction, to the agent&#8217;s training objective, to speed up the learning.</p>
<h2>Conclusion</h2>
<p>The goal was to give an introduction to Reinforcement Learning based trading agents, make an argument for why they are superior to current trading strategy development models, and make an argument for why I believe more researcher should be working on this. I hope I achieved some this in this post. Please let me know in the comments what you think, and feel free to get in touch to ask questions.</p>
<p>Thanks for reading all the way to the end :)</p>
</div><!-- .entry-content -->
</article><!-- #post-## -->
<article id="post-1022" class="post-1022 post type-post status-publish format-standard hentry category-news category-reinforcement-learning">
<header class="entry-header">
<div class="entry-meta"><span class="screen-reader-text">Posted on</span> <a href="http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/" rel="bookmark"><time class="entry-date published" datetime="2017-08-12T00:46:28-07:00">August 12, 2017</time><time class="updated" datetime="2017-08-16T16:53:28-07:00">August 16, 2017</time></a></div><!-- .entry-meta --><h2 class="entry-title"><a href="http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/" rel="bookmark">Hype or Not? Some Perspective on OpenAI&#8217;s DotA 2 Bot</a></h2>	</header><!-- .entry-header -->
<div class="entry-content">
<p><a href="http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/">See the Hacker News Discussion for additional context.</a></p>
<p><strong>Update (August 17th, 2017)</strong>: OpenAI has <a href="https://blog.openai.com/more-on-dota-2/">published a blog post</a> with more details about the bot. Almost everything of the post below still holds true, however. OpenAI&#8217;s post is sparse on technical details as they &#8220;not ready to talk about agent internals — the team is focused on solving 5v5 first.&#8221;. See <a href="https://twitter.com/Smerity/status/897959187480625153">this tweetstorm by @smerity</a> for a good analysis.</p>
<p>When I read today&#8217;s news about <a href="https://blog.openai.com/dota-2/">OpenAI&#8217;s DotA 2 bot</a> beating human players at <a href="http://www.dota2.com/international/overview/">The International</a>, an eSports tournament with a prize pool of over $24M, I was jumping with excitement. For one, I am a big eSports fan. I have never played DotA 2, but I regularly watch other eSports competitions on Twitch and even played semi-professionally when I was in high school. But more importantly, multiplayer online battle arena (MOBA) games like DotA and real-time strategy (RTS) games like Starcraft 2, are seen as being way beyond the capabilities of current Artificial Intelligence techniques. These games require long-term strategic decision making, multiplayer cooperation, and have significantly more complex state and action spaces than Chess, Go, or Atari, all of which have been &#8220;solved&#8221; by AI techniques over the past decades. DeepMind has been working on Starcraft 2 for a while and just recently <a href="https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/">released</a> their research environment. So far no researchers have managed to make significant breakthroughs. It is thought that we are at least 1-2 years away from beating good human players at Starcraft 2.</p>
<p>That&#8217;s why the OpenAI news came as such a shock. How can this be true? Have there been recent breakthroughs that I wasn&#8217;t aware of? As I started looking more into what exactly the DotA 2 bot was doing, how it was trained, and what game environment it was in, I came to the conclusion that it&#8217;s an impressive achievement, but not the AI breakthrough the press would like you to believe it is. That&#8217;s what this post is about. I would like to offer a sober explanation of what&#8217;s actually new. There is a real danger of overhyping Artificial Intelligence progress, nicely captured by misleading tweets like these:</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">OpenAI first ever to defeat world&#39;s best players in competitive eSports. Vastly more complex than traditional board games like chess &amp; Go.</p>
<p>&mdash; Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/896163163581825025">August 12, 2017</a></p></blockquote>
<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">Nobody likes being regulated, but everything (cars, planes, food, drugs, etc) that&#39;s a danger to the public is regulated. AI should be too.</p>
<p>&mdash; Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/896169801277517824">August 12, 2017</a></p></blockquote>
<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>Let me start out by saying that none of the hype or incorrect assumptions is the fault of OpenAI researchers. OpenAI has traditionally been very straightforward and explicit about the limitations of their research contributions. I am sure it will be the same in this case. OpenAI has not yet published technical details of their solution, so it is easy to jump to wrong conclusions for people not in the field.</p>
<p>Let&#8217;s start out by looking at how difficult the problem that the DotA 2 bot is solving actually is. How does it compare to something like <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a>?</p>
<ul>
<li><strong>1v1 is not comparable to 5v5</strong>. In a typical game of DotA 2, a team of 5 plays against another team of 5 players. These games require high-level strategy, team communication and coordination, and typically take around 45 minutes. 1v1 games are much more restricted. Two players basically move down a single lane and try to kill each other. It&#8217;s typically over in a few minutes. Beating an opponent in 1v1 requires mechanical skill and short-term tactics, but none of the things, like long term planning or coordination, that are challenging for current AI techniques. In fact, the number of useful actions you can take is less than in a game of Go. The effective state space (the player&#8217;s idea of what&#8217;s currently going on in the game), if represented in a smart way, should be smaller than in Go as well.</li>
<li><strong>Bots have access to more information</strong>: The OpenAI bot was built on top of the <a href="https://developer.valvesoftware.com/wiki/Dota_Bot_Scripting">game&#8217;s bot API</a>, giving it access to all kinds of information humans do not have access to. Even if OpenAI researchers restricted access to certain kinds of information, the bot still has access to more exact information than humans. For example, a skill may only hit an opponent within a certain range and a human player must look at the screen and estimate the current distance to the opponent. That takes practice. The bot knows the exact distance and can make an immediate decision to use the skill or not. Having access to all kinds of exact numerical information is a big advantage. In fact, during the game, one could see the bot executing skills at the maximum distance several times.</li>
<li><strong>Reaction Times</strong>: Bots can react instantly, human&#8217;s can&#8217;t. Coupled with the information advantage from above this is another big advantage. For example, once the opponent is out of range for a specific skill a bot can immediately cancel it. </li>
<li><strong>Learning to play a single specific character</strong>: There are 100 different characters with different innate abilities and strengths. The only character the bot learns to play, Shadow Fiend, generally does immediate attacks (as opposed to more complex skills lasting over a period of time) and benefits from knowing exact distances and having fast reactions times &#8211; exactly what a bot is good at.</li>
<li><strong>Hard-coded restrictions</strong>: The bot was not trained from scratch knowing nothing about the game. Item choices <a href="https://news.ycombinator.com/item?id=15001521">were hardcoded</a>, and so were certain techniques, such as creep block, that were deemed necessary to win. It seems like what was learned is mostly the interaction with the opponent. </li>
</ul>
<p>Given that 1v1 is mostly a game of mechanical skill, it is not surprising that a bot beats human players. And given the severely restricted environment, the artificially restricted set of possible actions, and that there was little to no need for long-term planning or coordination, I come to the conclusion that this problem was actually <strong>significantly easier than beating a human champion in the game of Go</strong>. <strong>We did not make sudden progress in AI because our algorithms are so smart &#8211; it worked because our researchers are smart about setting up the problem in just the right way to work around the limitations of current techniques</strong>. The training time for the bot, said to be around 2 weeks, suggests the same. AlphaGo required several months of highly distributed large-scale training on Google&#8217;s GPU clusters. We&#8217;ve made some progress since then, but not something that reduces computational requirements by an order of magnitude.</p>
<p>Now, enough with the criticism. The work may be a little overhyped by the press, but there are in fact some extremely cool and surprising things about it. And clearly, a large amount of challenging engineering work and partnership building must have gone into making this happen.</p>
<ul>
<li><strong>Trained entirely through self-play</strong>: The bot does not need any training data. It does not learn from human demonstrations either. It starts out completely random and keeps playing against itself. While this technique is nothing new, it is surprising (at least to me) that the bot learns techniques that human players are also known to use, as suggested by comments (<a href="https://news.ycombinator.com/item?id=14996448">here</a> and <a href="https://www.reddit.com/r/MachineLearning/comments/6t58ks/n_openai_bot_beat_best_dota_2_players_in_1v1_at/dli3zpp/">here</a>). I don&#8217;t know enough about the DotA 2 to judge this, but I think it&#8217;s extremely cool. There may be other techniques the bot has learned but humans are not even aware of. This is similar to what we&#8217;ve seen with AlphaGo, where human players started to learn from its unintuitive moves and adjusted their own game play. (Update: It has been <a href="https://news.ycombinator.com/item?id=15001521">confirmed</a> that certain techniques were hardcoded, so it is unclear what exactly is learned)</li>
<li><strong>A major step for AI + eSports</strong>: Having challenging environments, such as DotA 2 and Starcraft 2, to test new AI techniques on is extremely important. If we can convince the eSports community and game publishers that we can provide value by applying AI techniques to games, we can expect a lot of support in return, and this may result in much faster AI progress.</li>
<li><strong>Partially Observable environments</strong>: While the details of how OpenAI researchers handled this with the API are unclear, a human player only sees what&#8217;s on the screen and may have a restricted set of view e.g. uphill. This means, unlike with games like Go or Chess or Atari (and more like Poker) we are in a partially observable environment &#8211; we don&#8217;t have access to full information about the current game state. Such problems are typically much harder to solve and an active area of research where progress is severely needed. That being said, it is unclear how much partial observability in a 1v1 DotA 2 match really matters &#8211; there isn&#8217;t too much to strategize about.</li>
</ul>
<p>Above all, I&#8217;m very excited to read OpenAI&#8217;s technical report of what actually went into building this.</p>
<p>Thanks to <a href="https://twitter.com/Smerity/">@smerity</a> for useful feedback, suggestions, and DotA knowledge.</p>
</div><!-- .entry-content -->
</article><!-- #post-## -->
<article id="post-843" class="post-843 post type-post status-publish format-standard hentry category-deep-learning category-reinforcement-learning">
<header class="entry-header">
<div class="entry-meta"><span class="screen-reader-text">Posted on</span> <a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/" rel="bookmark"><time class="entry-date published" datetime="2016-10-02T08:04:47-07:00">October 2, 2016</time><time class="updated" datetime="2017-06-11T13:01:39-07:00">June 11, 2017</time></a></div><!-- .entry-meta --><h2 class="entry-title"><a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/" rel="bookmark">Learning Reinforcement Learning (with Code, Exercises and Solutions)</a></h2>	</header><!-- .entry-header -->
<div class="entry-content">
<p><a href="https://github.com/dennybritz/reinforcement-learning"><strong>Skip all the talk and go directly to the Github Repo with code and exercises.</strong></a></p>
<h4>Why Study Reinforcement Learning</h4>
<p>Reinforcement Learning is one of the fields I&#8217;m most excited about. Over the past few years amazing results like <a href="http://ir.hit.edu.cn/~jguo/docs/notes/dqn-atari.pdf">learning to play Atari Games from raw pixels</a> and <a href="https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf">Mastering the Game of Go</a> have gotten a lot of attention, but RL is also widely used in Robotics, Image Processing and Natural Language Processing.</p>
<p>Combining Reinforcement Learning and Deep Learning techniques works extremely well. Both fields heavily influence each other. On the Reinforcement Learning side Deep Neural Networks are used as function approximators to learn good representations, e.g. to process Atari game images or to understand the board state of Go. In the other direction, RL techniques are making their way into supervised problems usually tackled by Deep Learning. For example, RL techniques are used to implement attention mechanisms in image processing, or to optimize long-term rewards in conversational interfaces and neural translation systems. Finally, as Reinforcement Learning is concerned with making optimal decisions it has some extremely interesting parallels to human Psychology and Neuroscience (and many other fields).</p>
<p>With lots of open problems and opportunities for fundamental research I think we&#8217;ll be seeing multiple Reinforcement Learning breakthroughs in the coming years. And what could be more fun than teaching machines to play Starcraft and Doom?</p>
<h4>How to Study Reinforcement Learning</h4>
<p>There are many excellent Reinforcement Learning resources out there. Two I recommend the most are:</p>
<ul>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver&#8217;s Reinforcement Learning Course</a></li>
<li><a href="http://incompleteideas.net/sutton/book/bookdraft2017june.pdf">Richard Sutton&#8217;s &amp; Andrew Barto&#8217;s Reinforcement Learning: An Introduction (2nd Edition)</a> book.</li>
</ul>
<p>The latter is still work in progress but it&#8217;s ~80% complete. The course is based on the book so the two work quite well together. In fact, these two cover almost everything you need to know to understand most of the recent research papers. The prerequisites are basic Math and some knowledge of Machine Learning.</p>
<p>That covers the theory. But what about practical resources? What about actually implementing the algorithms that are covered in the book/course? That&#8217;s where this post and the <a href="https://github.com/dennybritz/reinforcement-learning">Github repository</a> comes in. I&#8217;ve tried to implement most of the standard Reinforcement Algorithms using Python, <a href="https://gym.openai.com/">OpenAI Gym</a> and Tensorflow. I separated them into chapters (with brief summaries) and exercises and solutions so that you can use them to supplement the theoretical material above. <a href="https://github.com/dennybritz/reinforcement-learning">All of this is in the Github repository</a>.</p>
<p>Some of the more time-intensive algorithms are still work in progress, so feel free to contribute. I&#8217;ll update this post as I implement them.</p>
<h4>Table of Contents</h4>
<ul>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/Introduction/">Introduction to RL problems, OpenAI gym</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/MDP/">MDPs and Bellman Equations</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP/">Dynamic Programming: Model-Based RL, Policy Iteration and Value Iteration</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/MC/">Monte Carlo Model-Free Prediction &amp; Control</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/TD/">Temporal Difference Model-Free Prediction &amp; Control</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/FA/">Function Approximation</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DQN/">Deep Q Learning</a> (WIP)</li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/">Policy Gradient Methods</a> (WIP)</li>
<li>Learning and Planning (WIP)</li>
<li>Exploration and Exploitation (WIP)</li>
</ul>
<h4>List of Implemented Algorithms</h4>
<ul>
<li>
<p><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP/Policy%20Evaluation%20Solution.ipynb">Dynamic Programming Policy Evaluation</a></p>
</li>
<li>
<p><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP/Policy%20Iteration%20Solution.ipynb">Dynamic Programming Policy Iteration</a></p>
</li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DP/Value%20Iteration%20Solution.ipynb">Dynamic Programming Value Iteration</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/MC/MC%20Prediction%20Solution.ipynb">Monte Carlo Prediction</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies%20Solution.ipynb">Monte Carlo Control with Epsilon-Greedy Policies</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb">Monte Carlo Off-Policy Control with Importance Sampling</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/TD/SARSA%20Solution.ipynb">SARSA (On Policy TD Learning)</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/TD/Q-Learning%20Solution.ipynb">Q-Learning (Off Policy TD Learning)</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb">Q-Learning with Linear Function Approximation</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DQN/Deep%20Q%20Learning%20Solution.ipynb">Deep Q-Learning for Atari Games</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/DQN/Double%20DQN%20Solution.ipynb">Double Deep-Q Learning for Atari Games</a></li>
<li>Deep Q-Learning with Prioritized Experience Replay (WIP)</li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb">Policy Gradient: REINFORCE with Baseline</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb">Policy Gradient: Actor Critic with Baseline</a></li>
<li><a href="https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb">Policy Gradient: Actor Critic with Baseline for Continuous Action Spaces</a></li>
<li>Deterministic Policy Gradients for Continuous Action Spaces (WIP)</li>
<li>Deep Deterministic Policy Gradients (DDPG) (WIP)</li>
<li>Asynchronous Advantage Actor Critic (A3C) (WIP)</li>
</ul>
</div><!-- .entry-content -->
</article><!-- #post-## -->
</main><!-- #main -->
</div><!-- #primary -->
<aside id="secondary" class="widget-area" role="complementary" aria-label="Blog Sidebar">
<section id="blog_subscription-2" class="widget widget_blog_subscription jetpack_subscription_widget"><h2 class="widget-title">Subscribe to Blog via Email</h2>
<form action="#" method="post" accept-charset="utf-8" id="subscribe-blog-blog_subscription-2">
<div id="subscribe-text"><p>Enter your email address to subscribe to this blog and receive notifications of new posts by email.</p>
</div>                    <p id="subscribe-email">
<label id="jetpack-subscribe-label"
class="screen-reader-text"
for="subscribe-field-blog_subscription-2">
Email Address                        </label>
<input type="email" name="email" required="required" class="required"
value=""
id="subscribe-field-blog_subscription-2"
placeholder="Email Address"/>
</p>
<p id="subscribe-submit">
<input type="hidden" name="action" value="subscribe"/>
<input type="hidden" name="source" value="http://www.wildml.com/category/reinforcement-learning/"/>
<input type="hidden" name="sub-type" value="widget"/>
<input type="hidden" name="redirect_fragment" value="blog_subscription-2"/>
<button type="submit"
name="jetpack_subscriptions_widget"
>
Subscribe                        </button>
</p>
</form>
</section>		<section id="recent-posts-2" class="widget widget_recent_entries">		<h2 class="widget-title">Recent Posts</h2>		<ul>
<li>
<a href="http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/">Introduction to Learning to Trade with Reinforcement Learning</a>
</li>
<li>
<a href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/">AI and Deep Learning in 2017 &#8211; A Year in Review</a>
</li>
<li>
<a href="http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/">Hype or Not? Some Perspective on OpenAI&#8217;s DotA 2 Bot</a>
</li>
<li>
<a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/">Learning Reinforcement Learning (with Code, Exercises and Solutions)</a>
</li>
<li>
<a href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/">RNNs in Tensorflow, a Practical Guide and Undocumented Features</a>
</li>
<li>
<a href="http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/">Deep Learning for Chatbots, Part 2 &#8211; Implementing a Retrieval-Based Model in Tensorflow</a>
</li>
<li>
<a href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/">Deep Learning for Chatbots, Part 1 &#8211; Introduction</a>
</li>
<li>
<a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">Attention and Memory in Deep Learning and NLP</a>
</li>
</ul>
</section><section id="archives-2" class="widget widget_archive"><h2 class="widget-title">Archives</h2>		<ul>
<li><a href='http://www.wildml.com/2018/02/'>February 2018</a></li>
<li><a href='http://www.wildml.com/2017/12/'>December 2017</a></li>
<li><a href='http://www.wildml.com/2017/08/'>August 2017</a></li>
<li><a href='http://www.wildml.com/2016/10/'>October 2016</a></li>
<li><a href='http://www.wildml.com/2016/08/'>August 2016</a></li>
<li><a href='http://www.wildml.com/2016/07/'>July 2016</a></li>
<li><a href='http://www.wildml.com/2016/04/'>April 2016</a></li>
<li><a href='http://www.wildml.com/2016/01/'>January 2016</a></li>
<li><a href='http://www.wildml.com/2015/12/'>December 2015</a></li>
<li><a href='http://www.wildml.com/2015/11/'>November 2015</a></li>
<li><a href='http://www.wildml.com/2015/10/'>October 2015</a></li>
<li><a href='http://www.wildml.com/2015/09/'>September 2015</a></li>
</ul>
</section><section id="categories-2" class="widget widget_categories"><h2 class="widget-title">Categories</h2>		<ul>
<li class="cat-item cat-item-15"><a href="http://www.wildml.com/category/conversational-agents/" >Conversational Agents</a>
</li>
<li class="cat-item cat-item-8"><a href="http://www.wildml.com/category/neural-networks/convolutional-neural-networks/" >Convolutional Neural Networks</a>
</li>
<li class="cat-item cat-item-3"><a href="http://www.wildml.com/category/deep-learning/" >Deep Learning</a>
</li>
<li class="cat-item cat-item-4"><a href="http://www.wildml.com/category/gpu/" >GPU</a>
</li>
<li class="cat-item cat-item-7"><a href="http://www.wildml.com/category/language-modeling/" >Language Modeling</a>
</li>
<li class="cat-item cat-item-11"><a href="http://www.wildml.com/category/memory/" >Memory</a>
</li>
<li class="cat-item cat-item-2"><a href="http://www.wildml.com/category/neural-networks/" >Neural Networks</a>
</li>
<li class="cat-item cat-item-19"><a href="http://www.wildml.com/category/news/" >News</a>
</li>
<li class="cat-item cat-item-9"><a href="http://www.wildml.com/category/nlp/" >NLP</a>
</li>
<li class="cat-item cat-item-6"><a href="http://www.wildml.com/category/neural-networks/recurrent-neural-networks/" >Recurrent Neural Networks</a>
</li>
<li class="cat-item cat-item-17 current-cat"><a href="http://www.wildml.com/category/reinforcement-learning/" >Reinforcement Learning</a>
</li>
<li class="cat-item cat-item-5"><a href="http://www.wildml.com/category/rnns/" >RNNs</a>
</li>
<li class="cat-item cat-item-16"><a href="http://www.wildml.com/category/tensorflow/" >Tensorflow</a>
</li>
<li class="cat-item cat-item-20"><a href="http://www.wildml.com/category/trading/" >Trading</a>
</li>
<li class="cat-item cat-item-1"><a href="http://www.wildml.com/category/uncategorized/" >Uncategorized</a>
</li>
</ul>
</section><section id="meta-2" class="widget widget_meta"><h2 class="widget-title">Meta</h2>			<ul>
<li><a href="http://www.wildml.com/wp-login.php">Log in</a></li>
<li><a href="http://www.wildml.com/feed/">Entries <abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="http://www.wildml.com/comments/feed/">Comments <abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="https://wordpress.org/" title="Powered by WordPress, state-of-the-art semantic personal publishing platform.">WordPress.org</a></li>			</ul>
</section></aside><!-- #secondary -->
</div><!-- .wrap -->

</div><!-- #content -->
<footer id="colophon" class="site-footer" role="contentinfo">
<div class="wrap">
<aside class="widget-area" role="complementary" aria-label="Footer">
<div class="widget-column footer-widget-1">
</div>
<div class="widget-column footer-widget-2">
</div>
</aside><!-- .widget-area -->
<div class="site-info">
<a href="https://wordpress.org/" class="imprint">
Proudly powered by WordPress	</a>
</div><!-- .site-info -->
</div><!-- .wrap -->
</footer><!-- #colophon -->
</div><!-- .site-content-contain -->
</div><!-- #page -->
<div style="display:none">
</div>
<script type='text/javascript'>
/* <![CDATA[ */
var countVars = {"disqusShortname":"wildml"};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/plugins/disqus-comment-system/public/js/comment_count.js?ver=3.0.17'></script>
<script type='text/javascript' src='https://s0.wp.com/wp-content/js/devicepx-jetpack.js?ver=201916'></script>
<script type='text/javascript' src='https://secure.gravatar.com/js/gprofiles.js?ver=2019Apraa'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/plugins/jetpack/modules/wpgroho.js?ver=5.1.1'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var twentyseventeenScreenReaderText = {"quote":"<svg class=\"icon icon-quote-right\" aria-hidden=\"true\" role=\"img\"> <use href=\"#icon-quote-right\" xlink:href=\"#icon-quote-right\"><\/use> <\/svg>","expand":"Expand child menu","collapse":"Collapse child menu","icon":"<svg class=\"icon icon-angle-down\" aria-hidden=\"true\" role=\"img\"> <use href=\"#icon-angle-down\" xlink:href=\"#icon-angle-down\"><\/use> <span class=\"svg-fallback icon-angle-down\"><\/span><\/svg>"};
/* ]]> */
</script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/js/skip-link-focus-fix.js?ver=1.0'></script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/js/navigation.js?ver=1.0'></script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/js/global.js?ver=1.0'></script>
<script type='text/javascript' src='http://www.wildml.com/wp-content/themes/twentyseventeen/assets/js/jquery.scrollTo.js?ver=2.1.2'></script>
<script type='text/javascript' src='http://www.wildml.com/wp-includes/js/wp-embed.min.js?ver=5.1.1'></script>
<script type='text/javascript' src='https://stats.wp.com/e-201916.js' async='async' defer='defer'></script>
<script type='text/javascript'>
_stq = window._stq || [];
_stq.push([ 'view', {v:'ext',j:'1:7.1.1',blog:'98789418',post:'0',tz:'-7',srv:'www.wildml.com'} ]);
_stq.push([ 'clickTrackerInit', '98789418', '0' ]);
</script>
<svg style="position: absolute; width: 0; height: 0; overflow: hidden;" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<defs>
<symbol id="icon-behance" viewBox="0 0 37 32">
<path class="path1" d="M33 6.054h-9.125v2.214h9.125v-2.214zM28.5 13.661q-1.607 0-2.607 0.938t-1.107 2.545h7.286q-0.321-3.482-3.571-3.482zM28.786 24.107q1.125 0 2.179-0.571t1.357-1.554h3.946q-1.786 5.482-7.625 5.482-3.821 0-6.080-2.357t-2.259-6.196q0-3.714 2.33-6.17t6.009-2.455q2.464 0 4.295 1.214t2.732 3.196 0.902 4.429q0 0.304-0.036 0.839h-11.75q0 1.982 1.027 3.063t2.973 1.080zM4.946 23.214h5.286q3.661 0 3.661-2.982 0-3.214-3.554-3.214h-5.393v6.196zM4.946 13.625h5.018q1.393 0 2.205-0.652t0.813-2.027q0-2.571-3.393-2.571h-4.643v5.25zM0 4.536h10.607q1.554 0 2.768 0.25t2.259 0.848 1.607 1.723 0.563 2.75q0 3.232-3.071 4.696 2.036 0.571 3.071 2.054t1.036 3.643q0 1.339-0.438 2.438t-1.179 1.848-1.759 1.268-2.161 0.75-2.393 0.232h-10.911v-22.5z"></path>
</symbol>
<symbol id="icon-deviantart" viewBox="0 0 18 32">
<path class="path1" d="M18.286 5.411l-5.411 10.393 0.429 0.554h4.982v7.411h-9.054l-0.786 0.536-2.536 4.875-0.536 0.536h-5.375v-5.411l5.411-10.411-0.429-0.536h-4.982v-7.411h9.054l0.786-0.536 2.536-4.875 0.536-0.536h5.375v5.411z"></path>
</symbol>
<symbol id="icon-medium" viewBox="0 0 32 32">
<path class="path1" d="M10.661 7.518v20.946q0 0.446-0.223 0.759t-0.652 0.313q-0.304 0-0.589-0.143l-8.304-4.161q-0.375-0.179-0.634-0.598t-0.259-0.83v-20.357q0-0.357 0.179-0.607t0.518-0.25q0.25 0 0.786 0.268l9.125 4.571q0.054 0.054 0.054 0.089zM11.804 9.321l9.536 15.464-9.536-4.75v-10.714zM32 9.643v18.821q0 0.446-0.25 0.723t-0.679 0.277-0.839-0.232l-7.875-3.929zM31.946 7.5q0 0.054-4.58 7.491t-5.366 8.705l-6.964-11.321 5.786-9.411q0.304-0.5 0.929-0.5 0.25 0 0.464 0.107l9.661 4.821q0.071 0.036 0.071 0.107z"></path>
</symbol>
<symbol id="icon-slideshare" viewBox="0 0 32 32">
<path class="path1" d="M15.589 13.214q0 1.482-1.134 2.545t-2.723 1.063-2.723-1.063-1.134-2.545q0-1.5 1.134-2.554t2.723-1.054 2.723 1.054 1.134 2.554zM24.554 13.214q0 1.482-1.125 2.545t-2.732 1.063q-1.589 0-2.723-1.063t-1.134-2.545q0-1.5 1.134-2.554t2.723-1.054q1.607 0 2.732 1.054t1.125 2.554zM28.571 16.429v-11.911q0-1.554-0.571-2.205t-1.982-0.652h-19.857q-1.482 0-2.009 0.607t-0.527 2.25v12.018q0.768 0.411 1.58 0.714t1.446 0.5 1.446 0.33 1.268 0.196 1.25 0.071 1.045 0.009 1.009-0.036 0.795-0.036q1.214-0.018 1.696 0.482 0.107 0.107 0.179 0.161 0.464 0.446 1.089 0.911 0.125-1.625 2.107-1.554 0.089 0 0.652 0.027t0.768 0.036 0.813 0.018 0.946-0.018 0.973-0.080 1.089-0.152 1.107-0.241 1.196-0.348 1.205-0.482 1.286-0.616zM31.482 16.339q-2.161 2.661-6.643 4.5 1.5 5.089-0.411 8.304-1.179 2.018-3.268 2.643-1.857 0.571-3.25-0.268-1.536-0.911-1.464-2.929l-0.018-5.821v-0.018q-0.143-0.036-0.438-0.107t-0.42-0.089l-0.018 6.036q0.071 2.036-1.482 2.929-1.411 0.839-3.268 0.268-2.089-0.643-3.25-2.679-1.875-3.214-0.393-8.268-4.482-1.839-6.643-4.5-0.446-0.661-0.071-1.125t1.071 0.018q0.054 0.036 0.196 0.125t0.196 0.143v-12.393q0-1.286 0.839-2.196t2.036-0.911h22.446q1.196 0 2.036 0.911t0.839 2.196v12.393l0.375-0.268q0.696-0.482 1.071-0.018t-0.071 1.125z"></path>
</symbol>
<symbol id="icon-snapchat-ghost" viewBox="0 0 30 32">
<path class="path1" d="M15.143 2.286q2.393-0.018 4.295 1.223t2.92 3.438q0.482 1.036 0.482 3.196 0 0.839-0.161 3.411 0.25 0.125 0.5 0.125 0.321 0 0.911-0.241t0.911-0.241q0.518 0 1 0.321t0.482 0.821q0 0.571-0.563 0.964t-1.232 0.563-1.232 0.518-0.563 0.848q0 0.268 0.214 0.768 0.661 1.464 1.83 2.679t2.58 1.804q0.5 0.214 1.429 0.411 0.5 0.107 0.5 0.625 0 1.25-3.911 1.839-0.125 0.196-0.196 0.696t-0.25 0.83-0.589 0.33q-0.357 0-1.107-0.116t-1.143-0.116q-0.661 0-1.107 0.089-0.571 0.089-1.125 0.402t-1.036 0.679-1.036 0.723-1.357 0.598-1.768 0.241q-0.929 0-1.723-0.241t-1.339-0.598-1.027-0.723-1.036-0.679-1.107-0.402q-0.464-0.089-1.125-0.089-0.429 0-1.17 0.134t-1.045 0.134q-0.446 0-0.625-0.33t-0.25-0.848-0.196-0.714q-3.911-0.589-3.911-1.839 0-0.518 0.5-0.625 0.929-0.196 1.429-0.411 1.393-0.571 2.58-1.804t1.83-2.679q0.214-0.5 0.214-0.768 0-0.5-0.563-0.848t-1.241-0.527-1.241-0.563-0.563-0.938q0-0.482 0.464-0.813t0.982-0.33q0.268 0 0.857 0.232t0.946 0.232q0.321 0 0.571-0.125-0.161-2.536-0.161-3.393 0-2.179 0.482-3.214 1.143-2.446 3.071-3.536t4.714-1.125z"></path>
</symbol>
<symbol id="icon-yelp" viewBox="0 0 27 32">
<path class="path1" d="M13.804 23.554v2.268q-0.018 5.214-0.107 5.446-0.214 0.571-0.911 0.714-0.964 0.161-3.241-0.679t-2.902-1.589q-0.232-0.268-0.304-0.643-0.018-0.214 0.071-0.464 0.071-0.179 0.607-0.839t3.232-3.857q0.018 0 1.071-1.25 0.268-0.339 0.705-0.438t0.884 0.063q0.429 0.179 0.67 0.518t0.223 0.75zM11.143 19.071q-0.054 0.982-0.929 1.25l-2.143 0.696q-4.911 1.571-5.214 1.571-0.625-0.036-0.964-0.643-0.214-0.446-0.304-1.339-0.143-1.357 0.018-2.973t0.536-2.223 1-0.571q0.232 0 3.607 1.375 1.25 0.518 2.054 0.839l1.5 0.607q0.411 0.161 0.634 0.545t0.205 0.866zM25.893 24.375q-0.125 0.964-1.634 2.875t-2.42 2.268q-0.661 0.25-1.125-0.125-0.25-0.179-3.286-5.125l-0.839-1.375q-0.25-0.375-0.205-0.821t0.348-0.821q0.625-0.768 1.482-0.464 0.018 0.018 2.125 0.714 3.625 1.179 4.321 1.42t0.839 0.366q0.5 0.393 0.393 1.089zM13.893 13.089q0.089 1.821-0.964 2.179-1.036 0.304-2.036-1.268l-6.75-10.679q-0.143-0.625 0.339-1.107 0.732-0.768 3.705-1.598t4.009-0.563q0.714 0.179 0.875 0.804 0.054 0.321 0.393 5.455t0.429 6.777zM25.714 15.018q0.054 0.696-0.464 1.054-0.268 0.179-5.875 1.536-1.196 0.268-1.625 0.411l0.018-0.036q-0.411 0.107-0.821-0.071t-0.661-0.571q-0.536-0.839 0-1.554 0.018-0.018 1.339-1.821 2.232-3.054 2.679-3.643t0.607-0.696q0.5-0.339 1.161-0.036 0.857 0.411 2.196 2.384t1.446 2.991v0.054z"></path>
</symbol>
<symbol id="icon-vine" viewBox="0 0 27 32">
<path class="path1" d="M26.732 14.768v3.536q-1.804 0.411-3.536 0.411-1.161 2.429-2.955 4.839t-3.241 3.848-2.286 1.902q-1.429 0.804-2.893-0.054-0.5-0.304-1.080-0.777t-1.518-1.491-1.83-2.295-1.92-3.286-1.884-4.357-1.634-5.616-1.259-6.964h5.054q0.464 3.893 1.25 7.116t1.866 5.661 2.17 4.205 2.5 3.482q3.018-3.018 5.125-7.25-2.536-1.286-3.982-3.929t-1.446-5.946q0-3.429 1.857-5.616t5.071-2.188q3.179 0 4.875 1.884t1.696 5.313q0 2.839-1.036 5.107-0.125 0.018-0.348 0.054t-0.821 0.036-1.125-0.107-1.107-0.455-0.902-0.92q0.554-1.839 0.554-3.286 0-1.554-0.518-2.357t-1.411-0.804q-0.946 0-1.518 0.884t-0.571 2.509q0 3.321 1.875 5.241t4.768 1.92q1.107 0 2.161-0.25z"></path>
</symbol>
<symbol id="icon-vk" viewBox="0 0 35 32">
<path class="path1" d="M34.232 9.286q0.411 1.143-2.679 5.25-0.429 0.571-1.161 1.518-1.393 1.786-1.607 2.339-0.304 0.732 0.25 1.446 0.304 0.375 1.446 1.464h0.018l0.071 0.071q2.518 2.339 3.411 3.946 0.054 0.089 0.116 0.223t0.125 0.473-0.009 0.607-0.446 0.491-1.054 0.223l-4.571 0.071q-0.429 0.089-1-0.089t-0.929-0.393l-0.357-0.214q-0.536-0.375-1.25-1.143t-1.223-1.384-1.089-1.036-1.009-0.277q-0.054 0.018-0.143 0.063t-0.304 0.259-0.384 0.527-0.304 0.929-0.116 1.384q0 0.268-0.063 0.491t-0.134 0.33l-0.071 0.089q-0.321 0.339-0.946 0.393h-2.054q-1.268 0.071-2.607-0.295t-2.348-0.946-1.839-1.179-1.259-1.027l-0.446-0.429q-0.179-0.179-0.491-0.536t-1.277-1.625-1.893-2.696-2.188-3.768-2.33-4.857q-0.107-0.286-0.107-0.482t0.054-0.286l0.071-0.107q0.268-0.339 1.018-0.339l4.893-0.036q0.214 0.036 0.411 0.116t0.286 0.152l0.089 0.054q0.286 0.196 0.429 0.571 0.357 0.893 0.821 1.848t0.732 1.455l0.286 0.518q0.518 1.071 1 1.857t0.866 1.223 0.741 0.688 0.607 0.25 0.482-0.089q0.036-0.018 0.089-0.089t0.214-0.393 0.241-0.839 0.17-1.446 0-2.232q-0.036-0.714-0.161-1.304t-0.25-0.821l-0.107-0.214q-0.446-0.607-1.518-0.768-0.232-0.036 0.089-0.429 0.304-0.339 0.679-0.536 0.946-0.464 4.268-0.429 1.464 0.018 2.411 0.232 0.357 0.089 0.598 0.241t0.366 0.429 0.188 0.571 0.063 0.813-0.018 0.982-0.045 1.259-0.027 1.473q0 0.196-0.018 0.75t-0.009 0.857 0.063 0.723 0.205 0.696 0.402 0.438q0.143 0.036 0.304 0.071t0.464-0.196 0.679-0.616 0.929-1.196 1.214-1.92q1.071-1.857 1.911-4.018 0.071-0.179 0.179-0.313t0.196-0.188l0.071-0.054 0.089-0.045t0.232-0.054 0.357-0.009l5.143-0.036q0.696-0.089 1.143 0.045t0.554 0.295z"></path>
</symbol>
<symbol id="icon-search" viewBox="0 0 30 32">
<path class="path1" d="M20.571 14.857q0-3.304-2.348-5.652t-5.652-2.348-5.652 2.348-2.348 5.652 2.348 5.652 5.652 2.348 5.652-2.348 2.348-5.652zM29.714 29.714q0 0.929-0.679 1.607t-1.607 0.679q-0.964 0-1.607-0.679l-6.125-6.107q-3.196 2.214-7.125 2.214-2.554 0-4.884-0.991t-4.018-2.679-2.679-4.018-0.991-4.884 0.991-4.884 2.679-4.018 4.018-2.679 4.884-0.991 4.884 0.991 4.018 2.679 2.679 4.018 0.991 4.884q0 3.929-2.214 7.125l6.125 6.125q0.661 0.661 0.661 1.607z"></path>
</symbol>
<symbol id="icon-envelope-o" viewBox="0 0 32 32">
<path class="path1" d="M29.714 26.857v-13.714q-0.571 0.643-1.232 1.179-4.786 3.679-7.607 6.036-0.911 0.768-1.482 1.196t-1.545 0.866-1.83 0.438h-0.036q-0.857 0-1.83-0.438t-1.545-0.866-1.482-1.196q-2.821-2.357-7.607-6.036-0.661-0.536-1.232-1.179v13.714q0 0.232 0.17 0.402t0.402 0.17h26.286q0.232 0 0.402-0.17t0.17-0.402zM29.714 8.089v-0.438t-0.009-0.232-0.054-0.223-0.098-0.161-0.161-0.134-0.25-0.045h-26.286q-0.232 0-0.402 0.17t-0.17 0.402q0 3 2.625 5.071 3.446 2.714 7.161 5.661 0.107 0.089 0.625 0.527t0.821 0.67 0.795 0.563 0.902 0.491 0.768 0.161h0.036q0.357 0 0.768-0.161t0.902-0.491 0.795-0.563 0.821-0.67 0.625-0.527q3.714-2.946 7.161-5.661 0.964-0.768 1.795-2.063t0.83-2.348zM32 7.429v19.429q0 1.179-0.839 2.018t-2.018 0.839h-26.286q-1.179 0-2.018-0.839t-0.839-2.018v-19.429q0-1.179 0.839-2.018t2.018-0.839h26.286q1.179 0 2.018 0.839t0.839 2.018z"></path>
</symbol>
<symbol id="icon-close" viewBox="0 0 25 32">
<path class="path1" d="M23.179 23.607q0 0.714-0.5 1.214l-2.429 2.429q-0.5 0.5-1.214 0.5t-1.214-0.5l-5.25-5.25-5.25 5.25q-0.5 0.5-1.214 0.5t-1.214-0.5l-2.429-2.429q-0.5-0.5-0.5-1.214t0.5-1.214l5.25-5.25-5.25-5.25q-0.5-0.5-0.5-1.214t0.5-1.214l2.429-2.429q0.5-0.5 1.214-0.5t1.214 0.5l5.25 5.25 5.25-5.25q0.5-0.5 1.214-0.5t1.214 0.5l2.429 2.429q0.5 0.5 0.5 1.214t-0.5 1.214l-5.25 5.25 5.25 5.25q0.5 0.5 0.5 1.214z"></path>
</symbol>
<symbol id="icon-angle-down" viewBox="0 0 21 32">
<path class="path1" d="M19.196 13.143q0 0.232-0.179 0.411l-8.321 8.321q-0.179 0.179-0.411 0.179t-0.411-0.179l-8.321-8.321q-0.179-0.179-0.179-0.411t0.179-0.411l0.893-0.893q0.179-0.179 0.411-0.179t0.411 0.179l7.018 7.018 7.018-7.018q0.179-0.179 0.411-0.179t0.411 0.179l0.893 0.893q0.179 0.179 0.179 0.411z"></path>
</symbol>
<symbol id="icon-folder-open" viewBox="0 0 34 32">
<path class="path1" d="M33.554 17q0 0.554-0.554 1.179l-6 7.071q-0.768 0.911-2.152 1.545t-2.563 0.634h-19.429q-0.607 0-1.080-0.232t-0.473-0.768q0-0.554 0.554-1.179l6-7.071q0.768-0.911 2.152-1.545t2.563-0.634h19.429q0.607 0 1.080 0.232t0.473 0.768zM27.429 10.857v2.857h-14.857q-1.679 0-3.518 0.848t-2.929 2.134l-6.107 7.179q0-0.071-0.009-0.223t-0.009-0.223v-17.143q0-1.643 1.179-2.821t2.821-1.179h5.714q1.643 0 2.821 1.179t1.179 2.821v0.571h9.714q1.643 0 2.821 1.179t1.179 2.821z"></path>
</symbol>
<symbol id="icon-twitter" viewBox="0 0 30 32">
<path class="path1" d="M28.929 7.286q-1.196 1.75-2.893 2.982 0.018 0.25 0.018 0.75 0 2.321-0.679 4.634t-2.063 4.437-3.295 3.759-4.607 2.607-5.768 0.973q-4.839 0-8.857-2.589 0.625 0.071 1.393 0.071 4.018 0 7.161-2.464-1.875-0.036-3.357-1.152t-2.036-2.848q0.589 0.089 1.089 0.089 0.768 0 1.518-0.196-2-0.411-3.313-1.991t-1.313-3.67v-0.071q1.214 0.679 2.607 0.732-1.179-0.786-1.875-2.054t-0.696-2.75q0-1.571 0.786-2.911 2.161 2.661 5.259 4.259t6.634 1.777q-0.143-0.679-0.143-1.321 0-2.393 1.688-4.080t4.080-1.688q2.5 0 4.214 1.821 1.946-0.375 3.661-1.393-0.661 2.054-2.536 3.179 1.661-0.179 3.321-0.893z"></path>
</symbol>
<symbol id="icon-facebook" viewBox="0 0 19 32">
<path class="path1" d="M17.125 0.214v4.714h-2.804q-1.536 0-2.071 0.643t-0.536 1.929v3.375h5.232l-0.696 5.286h-4.536v13.554h-5.464v-13.554h-4.554v-5.286h4.554v-3.893q0-3.321 1.857-5.152t4.946-1.83q2.625 0 4.071 0.214z"></path>
</symbol>
<symbol id="icon-github" viewBox="0 0 27 32">
<path class="path1" d="M13.714 2.286q3.732 0 6.884 1.839t4.991 4.991 1.839 6.884q0 4.482-2.616 8.063t-6.759 4.955q-0.482 0.089-0.714-0.125t-0.232-0.536q0-0.054 0.009-1.366t0.009-2.402q0-1.732-0.929-2.536 1.018-0.107 1.83-0.321t1.679-0.696 1.446-1.188 0.946-1.875 0.366-2.688q0-2.125-1.411-3.679 0.661-1.625-0.143-3.643-0.5-0.161-1.446 0.196t-1.643 0.786l-0.679 0.429q-1.661-0.464-3.429-0.464t-3.429 0.464q-0.286-0.196-0.759-0.482t-1.491-0.688-1.518-0.241q-0.804 2.018-0.143 3.643-1.411 1.554-1.411 3.679 0 1.518 0.366 2.679t0.938 1.875 1.438 1.196 1.679 0.696 1.83 0.321q-0.696 0.643-0.875 1.839-0.375 0.179-0.804 0.268t-1.018 0.089-1.17-0.384-0.991-1.116q-0.339-0.571-0.866-0.929t-0.884-0.429l-0.357-0.054q-0.375 0-0.518 0.080t-0.089 0.205 0.161 0.25 0.232 0.214l0.125 0.089q0.393 0.179 0.777 0.679t0.563 0.911l0.179 0.411q0.232 0.679 0.786 1.098t1.196 0.536 1.241 0.125 0.991-0.063l0.411-0.071q0 0.679 0.009 1.58t0.009 0.973q0 0.321-0.232 0.536t-0.714 0.125q-4.143-1.375-6.759-4.955t-2.616-8.063q0-3.732 1.839-6.884t4.991-4.991 6.884-1.839zM5.196 21.982q0.054-0.125-0.125-0.214-0.179-0.054-0.232 0.036-0.054 0.125 0.125 0.214 0.161 0.107 0.232-0.036zM5.75 22.589q0.125-0.089-0.036-0.286-0.179-0.161-0.286-0.054-0.125 0.089 0.036 0.286 0.179 0.179 0.286 0.054zM6.286 23.393q0.161-0.125 0-0.339-0.143-0.232-0.304-0.107-0.161 0.089 0 0.321t0.304 0.125zM7.036 24.143q0.143-0.143-0.071-0.339-0.214-0.214-0.357-0.054-0.161 0.143 0.071 0.339 0.214 0.214 0.357 0.054zM8.054 24.589q0.054-0.196-0.232-0.286-0.268-0.071-0.339 0.125t0.232 0.268q0.268 0.107 0.339-0.107zM9.179 24.679q0-0.232-0.304-0.196-0.286 0-0.286 0.196 0 0.232 0.304 0.196 0.286 0 0.286-0.196zM10.214 24.5q-0.036-0.196-0.321-0.161-0.286 0.054-0.25 0.268t0.321 0.143 0.25-0.25z"></path>
</symbol>
<symbol id="icon-bars" viewBox="0 0 27 32">
<path class="path1" d="M27.429 24v2.286q0 0.464-0.339 0.804t-0.804 0.339h-25.143q-0.464 0-0.804-0.339t-0.339-0.804v-2.286q0-0.464 0.339-0.804t0.804-0.339h25.143q0.464 0 0.804 0.339t0.339 0.804zM27.429 14.857v2.286q0 0.464-0.339 0.804t-0.804 0.339h-25.143q-0.464 0-0.804-0.339t-0.339-0.804v-2.286q0-0.464 0.339-0.804t0.804-0.339h25.143q0.464 0 0.804 0.339t0.339 0.804zM27.429 5.714v2.286q0 0.464-0.339 0.804t-0.804 0.339h-25.143q-0.464 0-0.804-0.339t-0.339-0.804v-2.286q0-0.464 0.339-0.804t0.804-0.339h25.143q0.464 0 0.804 0.339t0.339 0.804z"></path>
</symbol>
<symbol id="icon-google-plus" viewBox="0 0 41 32">
<path class="path1" d="M25.661 16.304q0 3.714-1.554 6.616t-4.429 4.536-6.589 1.634q-2.661 0-5.089-1.036t-4.179-2.786-2.786-4.179-1.036-5.089 1.036-5.089 2.786-4.179 4.179-2.786 5.089-1.036q5.107 0 8.768 3.429l-3.554 3.411q-2.089-2.018-5.214-2.018-2.196 0-4.063 1.107t-2.955 3.009-1.089 4.152 1.089 4.152 2.955 3.009 4.063 1.107q1.482 0 2.723-0.411t2.045-1.027 1.402-1.402 0.875-1.482 0.384-1.321h-7.429v-4.5h12.357q0.214 1.125 0.214 2.179zM41.143 14.125v3.75h-3.732v3.732h-3.75v-3.732h-3.732v-3.75h3.732v-3.732h3.75v3.732h3.732z"></path>
</symbol>
<symbol id="icon-linkedin" viewBox="0 0 27 32">
<path class="path1" d="M6.232 11.161v17.696h-5.893v-17.696h5.893zM6.607 5.696q0.018 1.304-0.902 2.179t-2.42 0.875h-0.036q-1.464 0-2.357-0.875t-0.893-2.179q0-1.321 0.92-2.188t2.402-0.866 2.375 0.866 0.911 2.188zM27.429 18.714v10.143h-5.875v-9.464q0-1.875-0.723-2.938t-2.259-1.063q-1.125 0-1.884 0.616t-1.134 1.527q-0.196 0.536-0.196 1.446v9.875h-5.875q0.036-7.125 0.036-11.554t-0.018-5.286l-0.018-0.857h5.875v2.571h-0.036q0.357-0.571 0.732-1t1.009-0.929 1.554-0.777 2.045-0.277q3.054 0 4.911 2.027t1.857 5.938z"></path>
</symbol>
<symbol id="icon-quote-right" viewBox="0 0 30 32">
<path class="path1" d="M13.714 5.714v12.571q0 1.857-0.723 3.545t-1.955 2.92-2.92 1.955-3.545 0.723h-1.143q-0.464 0-0.804-0.339t-0.339-0.804v-2.286q0-0.464 0.339-0.804t0.804-0.339h1.143q1.893 0 3.232-1.339t1.339-3.232v-0.571q0-0.714-0.5-1.214t-1.214-0.5h-4q-1.429 0-2.429-1t-1-2.429v-6.857q0-1.429 1-2.429t2.429-1h6.857q1.429 0 2.429 1t1 2.429zM29.714 5.714v12.571q0 1.857-0.723 3.545t-1.955 2.92-2.92 1.955-3.545 0.723h-1.143q-0.464 0-0.804-0.339t-0.339-0.804v-2.286q0-0.464 0.339-0.804t0.804-0.339h1.143q1.893 0 3.232-1.339t1.339-3.232v-0.571q0-0.714-0.5-1.214t-1.214-0.5h-4q-1.429 0-2.429-1t-1-2.429v-6.857q0-1.429 1-2.429t2.429-1h6.857q1.429 0 2.429 1t1 2.429z"></path>
</symbol>
<symbol id="icon-mail-reply" viewBox="0 0 32 32">
<path class="path1" d="M32 20q0 2.964-2.268 8.054-0.054 0.125-0.188 0.429t-0.241 0.536-0.232 0.393q-0.214 0.304-0.5 0.304-0.268 0-0.42-0.179t-0.152-0.446q0-0.161 0.045-0.473t0.045-0.42q0.089-1.214 0.089-2.196 0-1.804-0.313-3.232t-0.866-2.473-1.429-1.804-1.884-1.241-2.375-0.759-2.75-0.384-3.134-0.107h-4v4.571q0 0.464-0.339 0.804t-0.804 0.339-0.804-0.339l-9.143-9.143q-0.339-0.339-0.339-0.804t0.339-0.804l9.143-9.143q0.339-0.339 0.804-0.339t0.804 0.339 0.339 0.804v4.571h4q12.732 0 15.625 7.196 0.946 2.393 0.946 5.946z"></path>
</symbol>
<symbol id="icon-youtube" viewBox="0 0 27 32">
<path class="path1" d="M17.339 22.214v3.768q0 1.196-0.696 1.196-0.411 0-0.804-0.393v-5.375q0.393-0.393 0.804-0.393 0.696 0 0.696 1.196zM23.375 22.232v0.821h-1.607v-0.821q0-1.214 0.804-1.214t0.804 1.214zM6.125 18.339h1.911v-1.679h-5.571v1.679h1.875v10.161h1.786v-10.161zM11.268 28.5h1.589v-8.821h-1.589v6.75q-0.536 0.75-1.018 0.75-0.321 0-0.375-0.375-0.018-0.054-0.018-0.625v-6.5h-1.589v6.982q0 0.875 0.143 1.304 0.214 0.661 1.036 0.661 0.857 0 1.821-1.089v0.964zM18.929 25.857v-3.518q0-1.304-0.161-1.768-0.304-1-1.268-1-0.893 0-1.661 0.964v-3.875h-1.589v11.839h1.589v-0.857q0.804 0.982 1.661 0.982 0.964 0 1.268-0.982 0.161-0.482 0.161-1.786zM24.964 25.679v-0.232h-1.625q0 0.911-0.036 1.089-0.125 0.643-0.714 0.643-0.821 0-0.821-1.232v-1.554h3.196v-1.839q0-1.411-0.482-2.071-0.696-0.911-1.893-0.911-1.214 0-1.911 0.911-0.5 0.661-0.5 2.071v3.089q0 1.411 0.518 2.071 0.696 0.911 1.929 0.911 1.286 0 1.929-0.946 0.321-0.482 0.375-0.964 0.036-0.161 0.036-1.036zM14.107 9.375v-3.75q0-1.232-0.768-1.232t-0.768 1.232v3.75q0 1.25 0.768 1.25t0.768-1.25zM26.946 22.786q0 4.179-0.464 6.25-0.25 1.054-1.036 1.768t-1.821 0.821q-3.286 0.375-9.911 0.375t-9.911-0.375q-1.036-0.107-1.83-0.821t-1.027-1.768q-0.464-2-0.464-6.25 0-4.179 0.464-6.25 0.25-1.054 1.036-1.768t1.839-0.839q3.268-0.357 9.893-0.357t9.911 0.357q1.036 0.125 1.83 0.839t1.027 1.768q0.464 2 0.464 6.25zM9.125 0h1.821l-2.161 7.125v4.839h-1.786v-4.839q-0.25-1.321-1.089-3.786-0.661-1.839-1.161-3.339h1.893l1.268 4.696zM15.732 5.946v3.125q0 1.446-0.5 2.107-0.661 0.911-1.893 0.911-1.196 0-1.875-0.911-0.5-0.679-0.5-2.107v-3.125q0-1.429 0.5-2.089 0.679-0.911 1.875-0.911 1.232 0 1.893 0.911 0.5 0.661 0.5 2.089zM21.714 3.054v8.911h-1.625v-0.982q-0.946 1.107-1.839 1.107-0.821 0-1.054-0.661-0.143-0.429-0.143-1.339v-7.036h1.625v6.554q0 0.589 0.018 0.625 0.054 0.393 0.375 0.393 0.482 0 1.018-0.768v-6.804h1.625z"></path>
</symbol>
<symbol id="icon-dropbox" viewBox="0 0 32 32">
<path class="path1" d="M7.179 12.625l8.821 5.446-6.107 5.089-8.75-5.696zM24.786 22.536v1.929l-8.75 5.232v0.018l-0.018-0.018-0.018 0.018v-0.018l-8.732-5.232v-1.929l2.625 1.714 6.107-5.071v-0.036l0.018 0.018 0.018-0.018v0.036l6.125 5.071zM9.893 2.107l6.107 5.089-8.821 5.429-6.036-4.821zM24.821 12.625l6.036 4.839-8.732 5.696-6.125-5.089zM22.125 2.107l8.732 5.696-6.036 4.821-8.821-5.429z"></path>
</symbol>
<symbol id="icon-instagram" viewBox="0 0 27 32">
<path class="path1" d="M18.286 16q0-1.893-1.339-3.232t-3.232-1.339-3.232 1.339-1.339 3.232 1.339 3.232 3.232 1.339 3.232-1.339 1.339-3.232zM20.75 16q0 2.929-2.054 4.982t-4.982 2.054-4.982-2.054-2.054-4.982 2.054-4.982 4.982-2.054 4.982 2.054 2.054 4.982zM22.679 8.679q0 0.679-0.482 1.161t-1.161 0.482-1.161-0.482-0.482-1.161 0.482-1.161 1.161-0.482 1.161 0.482 0.482 1.161zM13.714 4.75q-0.125 0-1.366-0.009t-1.884 0-1.723 0.054-1.839 0.179-1.277 0.33q-0.893 0.357-1.571 1.036t-1.036 1.571q-0.196 0.518-0.33 1.277t-0.179 1.839-0.054 1.723 0 1.884 0.009 1.366-0.009 1.366 0 1.884 0.054 1.723 0.179 1.839 0.33 1.277q0.357 0.893 1.036 1.571t1.571 1.036q0.518 0.196 1.277 0.33t1.839 0.179 1.723 0.054 1.884 0 1.366-0.009 1.366 0.009 1.884 0 1.723-0.054 1.839-0.179 1.277-0.33q0.893-0.357 1.571-1.036t1.036-1.571q0.196-0.518 0.33-1.277t0.179-1.839 0.054-1.723 0-1.884-0.009-1.366 0.009-1.366 0-1.884-0.054-1.723-0.179-1.839-0.33-1.277q-0.357-0.893-1.036-1.571t-1.571-1.036q-0.518-0.196-1.277-0.33t-1.839-0.179-1.723-0.054-1.884 0-1.366 0.009zM27.429 16q0 4.089-0.089 5.661-0.179 3.714-2.214 5.75t-5.75 2.214q-1.571 0.089-5.661 0.089t-5.661-0.089q-3.714-0.179-5.75-2.214t-2.214-5.75q-0.089-1.571-0.089-5.661t0.089-5.661q0.179-3.714 2.214-5.75t5.75-2.214q1.571-0.089 5.661-0.089t5.661 0.089q3.714 0.179 5.75 2.214t2.214 5.75q0.089 1.571 0.089 5.661z"></path>
</symbol>
<symbol id="icon-flickr" viewBox="0 0 27 32">
<path class="path1" d="M22.286 2.286q2.125 0 3.634 1.509t1.509 3.634v17.143q0 2.125-1.509 3.634t-3.634 1.509h-17.143q-2.125 0-3.634-1.509t-1.509-3.634v-17.143q0-2.125 1.509-3.634t3.634-1.509h17.143zM12.464 16q0-1.571-1.107-2.679t-2.679-1.107-2.679 1.107-1.107 2.679 1.107 2.679 2.679 1.107 2.679-1.107 1.107-2.679zM22.536 16q0-1.571-1.107-2.679t-2.679-1.107-2.679 1.107-1.107 2.679 1.107 2.679 2.679 1.107 2.679-1.107 1.107-2.679z"></path>
</symbol>
<symbol id="icon-tumblr" viewBox="0 0 19 32">
<path class="path1" d="M16.857 23.732l1.429 4.232q-0.411 0.625-1.982 1.179t-3.161 0.571q-1.857 0.036-3.402-0.464t-2.545-1.321-1.696-1.893-0.991-2.143-0.295-2.107v-9.714h-3v-3.839q1.286-0.464 2.304-1.241t1.625-1.607 1.036-1.821 0.607-1.768 0.268-1.58q0.018-0.089 0.080-0.152t0.134-0.063h4.357v7.571h5.946v4.5h-5.964v9.25q0 0.536 0.116 1t0.402 0.938 0.884 0.741 1.455 0.25q1.393-0.036 2.393-0.518z"></path>
</symbol>
<symbol id="icon-dockerhub" viewBox="0 0 24 28">
<path class="path1" d="M1.597 10.257h2.911v2.83H1.597v-2.83zm3.573 0h2.91v2.83H5.17v-2.83zm0-3.627h2.91v2.829H5.17V6.63zm3.57 3.627h2.912v2.83H8.74v-2.83zm0-3.627h2.912v2.829H8.74V6.63zm3.573 3.627h2.911v2.83h-2.911v-2.83zm0-3.627h2.911v2.829h-2.911V6.63zm3.572 3.627h2.911v2.83h-2.911v-2.83zM12.313 3h2.911v2.83h-2.911V3zm-6.65 14.173c-.449 0-.812.354-.812.788 0 .435.364.788.812.788.447 0 .811-.353.811-.788 0-.434-.363-.788-.811-.788"></path>
<path class="path2" d="M28.172 11.721c-.978-.549-2.278-.624-3.388-.306-.136-1.146-.91-2.149-1.83-2.869l-.366-.286-.307.345c-.618.692-.8 1.845-.718 2.73.063.651.273 1.312.685 1.834-.313.183-.668.328-.985.434-.646.212-1.347.33-2.028.33H.083l-.042.429c-.137 1.432.065 2.866.674 4.173l.262.519.03.048c1.8 2.973 4.963 4.225 8.41 4.225 6.672 0 12.174-2.896 14.702-9.015 1.689.085 3.417-.4 4.243-1.968l.211-.4-.401-.223zM5.664 19.458c-.85 0-1.542-.671-1.542-1.497 0-.825.691-1.498 1.541-1.498.849 0 1.54.672 1.54 1.497s-.69 1.498-1.539 1.498z"></path>
</symbol>
<symbol id="icon-dribbble" viewBox="0 0 27 32">
<path class="path1" d="M18.286 26.786q-0.75-4.304-2.5-8.893h-0.036l-0.036 0.018q-0.286 0.107-0.768 0.295t-1.804 0.875-2.446 1.464-2.339 2.045-1.839 2.643l-0.268-0.196q3.286 2.679 7.464 2.679 2.357 0 4.571-0.929zM14.982 15.946q-0.375-0.875-0.946-1.982-5.554 1.661-12.018 1.661-0.018 0.125-0.018 0.375 0 2.214 0.786 4.223t2.214 3.598q0.893-1.589 2.205-2.973t2.545-2.223 2.33-1.446 1.777-0.857l0.661-0.232q0.071-0.018 0.232-0.063t0.232-0.080zM13.071 12.161q-2.143-3.804-4.357-6.75-2.464 1.161-4.179 3.321t-2.286 4.857q5.393 0 10.821-1.429zM25.286 17.857q-3.75-1.071-7.304-0.518 1.554 4.268 2.286 8.375 1.982-1.339 3.304-3.384t1.714-4.473zM10.911 4.625q-0.018 0-0.036 0.018 0.018-0.018 0.036-0.018zM21.446 7.214q-3.304-2.929-7.732-2.929-1.357 0-2.768 0.339 2.339 3.036 4.393 6.821 1.232-0.464 2.321-1.080t1.723-1.098 1.17-1.018 0.67-0.723zM25.429 15.875q-0.054-4.143-2.661-7.321l-0.018 0.018q-0.161 0.214-0.339 0.438t-0.777 0.795-1.268 1.080-1.786 1.161-2.348 1.152q0.446 0.946 0.786 1.696 0.036 0.107 0.116 0.313t0.134 0.295q0.643-0.089 1.33-0.125t1.313-0.036 1.232 0.027 1.143 0.071 1.009 0.098 0.857 0.116 0.652 0.107 0.446 0.080zM27.429 16q0 3.732-1.839 6.884t-4.991 4.991-6.884 1.839-6.884-1.839-4.991-4.991-1.839-6.884 1.839-6.884 4.991-4.991 6.884-1.839 6.884 1.839 4.991 4.991 1.839 6.884z"></path>
</symbol>
<symbol id="icon-skype" viewBox="0 0 27 32">
<path class="path1" d="M20.946 18.982q0-0.893-0.348-1.634t-0.866-1.223-1.304-0.875-1.473-0.607-1.563-0.411l-1.857-0.429q-0.536-0.125-0.786-0.188t-0.625-0.205-0.536-0.286-0.295-0.375-0.134-0.536q0-1.375 2.571-1.375 0.768 0 1.375 0.214t0.964 0.509 0.679 0.598 0.714 0.518 0.857 0.214q0.839 0 1.348-0.571t0.509-1.375q0-0.982-1-1.777t-2.536-1.205-3.25-0.411q-1.214 0-2.357 0.277t-2.134 0.839-1.589 1.554-0.598 2.295q0 1.089 0.339 1.902t1 1.348 1.429 0.866 1.839 0.58l2.607 0.643q1.607 0.393 2 0.643 0.571 0.357 0.571 1.071 0 0.696-0.714 1.152t-1.875 0.455q-0.911 0-1.634-0.286t-1.161-0.688-0.813-0.804-0.821-0.688-0.964-0.286q-0.893 0-1.348 0.536t-0.455 1.339q0 1.643 2.179 2.813t5.196 1.17q1.304 0 2.5-0.33t2.188-0.955 1.58-1.67 0.589-2.348zM27.429 22.857q0 2.839-2.009 4.848t-4.848 2.009q-2.321 0-4.179-1.429-1.375 0.286-2.679 0.286-2.554 0-4.884-0.991t-4.018-2.679-2.679-4.018-0.991-4.884q0-1.304 0.286-2.679-1.429-1.857-1.429-4.179 0-2.839 2.009-4.848t4.848-2.009q2.321 0 4.179 1.429 1.375-0.286 2.679-0.286 2.554 0 4.884 0.991t4.018 2.679 2.679 4.018 0.991 4.884q0 1.304-0.286 2.679 1.429 1.857 1.429 4.179z"></path>
</symbol>
<symbol id="icon-foursquare" viewBox="0 0 23 32">
<path class="path1" d="M17.857 7.75l0.661-3.464q0.089-0.411-0.161-0.714t-0.625-0.304h-12.714q-0.411 0-0.688 0.304t-0.277 0.661v19.661q0 0.125 0.107 0.018l5.196-6.286q0.411-0.464 0.679-0.598t0.857-0.134h4.268q0.393 0 0.661-0.259t0.321-0.527q0.429-2.321 0.661-3.411 0.071-0.375-0.205-0.714t-0.652-0.339h-5.25q-0.518 0-0.857-0.339t-0.339-0.857v-0.75q0-0.518 0.339-0.848t0.857-0.33h6.179q0.321 0 0.625-0.241t0.357-0.527zM21.911 3.786q-0.268 1.304-0.955 4.759t-1.241 6.25-0.625 3.098q-0.107 0.393-0.161 0.58t-0.25 0.58-0.438 0.589-0.688 0.375-1.036 0.179h-4.839q-0.232 0-0.393 0.179-0.143 0.161-7.607 8.821-0.393 0.446-1.045 0.509t-0.866-0.098q-0.982-0.393-0.982-1.75v-25.179q0-0.982 0.679-1.83t2.143-0.848h15.857q1.696 0 2.268 0.946t0.179 2.839zM21.911 3.786l-2.821 14.107q0.071-0.304 0.625-3.098t1.241-6.25 0.955-4.759z"></path>
</symbol>
<symbol id="icon-wordpress" viewBox="0 0 32 32">
<path class="path1" d="M2.268 16q0-2.911 1.196-5.589l6.554 17.946q-3.5-1.696-5.625-5.018t-2.125-7.339zM25.268 15.304q0 0.339-0.045 0.688t-0.179 0.884-0.205 0.786-0.313 1.054-0.313 1.036l-1.357 4.571-4.964-14.75q0.821-0.054 1.571-0.143 0.339-0.036 0.464-0.33t-0.045-0.554-0.509-0.241l-3.661 0.179q-1.339-0.018-3.607-0.179-0.214-0.018-0.366 0.089t-0.205 0.268-0.027 0.33 0.161 0.295 0.348 0.143l1.429 0.143 2.143 5.857-3 9-5-14.857q0.821-0.054 1.571-0.143 0.339-0.036 0.464-0.33t-0.045-0.554-0.509-0.241l-3.661 0.179q-0.125 0-0.411-0.009t-0.464-0.009q1.875-2.857 4.902-4.527t6.563-1.67q2.625 0 5.009 0.946t4.259 2.661h-0.179q-0.982 0-1.643 0.723t-0.661 1.705q0 0.214 0.036 0.429t0.071 0.384 0.143 0.411 0.161 0.375 0.214 0.402 0.223 0.375 0.259 0.429 0.25 0.411q1.125 1.911 1.125 3.786zM16.232 17.196l4.232 11.554q0.018 0.107 0.089 0.196-2.25 0.786-4.554 0.786-2 0-3.875-0.571zM28.036 9.411q1.696 3.107 1.696 6.589 0 3.732-1.857 6.884t-4.982 4.973l4.196-12.107q1.054-3.018 1.054-4.929 0-0.75-0.107-1.411zM16 0q3.25 0 6.214 1.268t5.107 3.411 3.411 5.107 1.268 6.214-1.268 6.214-3.411 5.107-5.107 3.411-6.214 1.268-6.214-1.268-5.107-3.411-3.411-5.107-1.268-6.214 1.268-6.214 3.411-5.107 5.107-3.411 6.214-1.268zM16 31.268q3.089 0 5.92-1.214t4.875-3.259 3.259-4.875 1.214-5.92-1.214-5.92-3.259-4.875-4.875-3.259-5.92-1.214-5.92 1.214-4.875 3.259-3.259 4.875-1.214 5.92 1.214 5.92 3.259 4.875 4.875 3.259 5.92 1.214z"></path>
</symbol>
<symbol id="icon-stumbleupon" viewBox="0 0 34 32">
<path class="path1" d="M18.964 12.714v-2.107q0-0.75-0.536-1.286t-1.286-0.536-1.286 0.536-0.536 1.286v10.929q0 3.125-2.25 5.339t-5.411 2.214q-3.179 0-5.42-2.241t-2.241-5.42v-4.75h5.857v4.679q0 0.768 0.536 1.295t1.286 0.527 1.286-0.527 0.536-1.295v-11.071q0-3.054 2.259-5.214t5.384-2.161q3.143 0 5.393 2.179t2.25 5.25v2.429l-3.482 1.036zM28.429 16.679h5.857v4.75q0 3.179-2.241 5.42t-5.42 2.241q-3.161 0-5.411-2.223t-2.25-5.366v-4.786l2.339 1.089 3.482-1.036v4.821q0 0.75 0.536 1.277t1.286 0.527 1.286-0.527 0.536-1.277v-4.911z"></path>
</symbol>
<symbol id="icon-digg" viewBox="0 0 37 32">
<path class="path1" d="M5.857 5.036h3.643v17.554h-9.5v-12.446h5.857v-5.107zM5.857 19.661v-6.589h-2.196v6.589h2.196zM10.964 10.143v12.446h3.661v-12.446h-3.661zM10.964 5.036v3.643h3.661v-3.643h-3.661zM16.089 10.143h9.518v16.821h-9.518v-2.911h5.857v-1.464h-5.857v-12.446zM21.946 19.661v-6.589h-2.196v6.589h2.196zM27.071 10.143h9.5v16.821h-9.5v-2.911h5.839v-1.464h-5.839v-12.446zM32.911 19.661v-6.589h-2.196v6.589h2.196z"></path>
</symbol>
<symbol id="icon-spotify" viewBox="0 0 27 32">
<path class="path1" d="M20.125 21.607q0-0.571-0.536-0.911-3.446-2.054-7.982-2.054-2.375 0-5.125 0.607-0.75 0.161-0.75 0.929 0 0.357 0.241 0.616t0.634 0.259q0.089 0 0.661-0.143 2.357-0.482 4.339-0.482 4.036 0 7.089 1.839 0.339 0.196 0.589 0.196 0.339 0 0.589-0.241t0.25-0.616zM21.839 17.768q0-0.714-0.625-1.089-4.232-2.518-9.786-2.518-2.732 0-5.411 0.75-0.857 0.232-0.857 1.143 0 0.446 0.313 0.759t0.759 0.313q0.125 0 0.661-0.143 2.179-0.589 4.482-0.589 4.982 0 8.714 2.214 0.429 0.232 0.679 0.232 0.446 0 0.759-0.313t0.313-0.759zM23.768 13.339q0-0.839-0.714-1.25-2.25-1.304-5.232-1.973t-6.125-0.67q-3.643 0-6.5 0.839-0.411 0.125-0.688 0.455t-0.277 0.866q0 0.554 0.366 0.929t0.92 0.375q0.196 0 0.714-0.143 2.375-0.661 5.482-0.661 2.839 0 5.527 0.607t4.527 1.696q0.375 0.214 0.714 0.214 0.518 0 0.902-0.366t0.384-0.92zM27.429 16q0 3.732-1.839 6.884t-4.991 4.991-6.884 1.839-6.884-1.839-4.991-4.991-1.839-6.884 1.839-6.884 4.991-4.991 6.884-1.839 6.884 1.839 4.991 4.991 1.839 6.884z"></path>
</symbol>
<symbol id="icon-soundcloud" viewBox="0 0 41 32">
<path class="path1" d="M14 24.5l0.286-4.304-0.286-9.339q-0.018-0.179-0.134-0.304t-0.295-0.125q-0.161 0-0.286 0.125t-0.125 0.304l-0.25 9.339 0.25 4.304q0.018 0.179 0.134 0.295t0.277 0.116q0.393 0 0.429-0.411zM19.286 23.982l0.196-3.768-0.214-10.464q0-0.286-0.232-0.429-0.143-0.089-0.286-0.089t-0.286 0.089q-0.232 0.143-0.232 0.429l-0.018 0.107-0.179 10.339q0 0.018 0.196 4.214v0.018q0 0.179 0.107 0.304 0.161 0.196 0.411 0.196 0.196 0 0.357-0.161 0.161-0.125 0.161-0.357zM0.625 17.911l0.357 2.286-0.357 2.25q-0.036 0.161-0.161 0.161t-0.161-0.161l-0.304-2.25 0.304-2.286q0.036-0.161 0.161-0.161t0.161 0.161zM2.161 16.5l0.464 3.696-0.464 3.625q-0.036 0.161-0.179 0.161-0.161 0-0.161-0.179l-0.411-3.607 0.411-3.696q0-0.161 0.161-0.161 0.143 0 0.179 0.161zM3.804 15.821l0.446 4.375-0.446 4.232q0 0.196-0.196 0.196-0.179 0-0.214-0.196l-0.375-4.232 0.375-4.375q0.036-0.214 0.214-0.214 0.196 0 0.196 0.214zM5.482 15.696l0.411 4.5-0.411 4.357q-0.036 0.232-0.25 0.232-0.232 0-0.232-0.232l-0.375-4.357 0.375-4.5q0-0.232 0.232-0.232 0.214 0 0.25 0.232zM7.161 16.018l0.375 4.179-0.375 4.393q-0.036 0.286-0.286 0.286-0.107 0-0.188-0.080t-0.080-0.205l-0.357-4.393 0.357-4.179q0-0.107 0.080-0.188t0.188-0.080q0.25 0 0.286 0.268zM8.839 13.411l0.375 6.786-0.375 4.393q0 0.125-0.089 0.223t-0.214 0.098q-0.286 0-0.321-0.321l-0.321-4.393 0.321-6.786q0.036-0.321 0.321-0.321 0.125 0 0.214 0.098t0.089 0.223zM10.518 11.875l0.339 8.357-0.339 4.357q0 0.143-0.098 0.241t-0.241 0.098q-0.321 0-0.357-0.339l-0.286-4.357 0.286-8.357q0.036-0.339 0.357-0.339 0.143 0 0.241 0.098t0.098 0.241zM12.268 11.161l0.321 9.036-0.321 4.321q-0.036 0.375-0.393 0.375-0.339 0-0.375-0.375l-0.286-4.321 0.286-9.036q0-0.161 0.116-0.277t0.259-0.116q0.161 0 0.268 0.116t0.125 0.277zM19.268 24.411v0 0zM15.732 11.089l0.268 9.107-0.268 4.268q0 0.179-0.134 0.313t-0.313 0.134-0.304-0.125-0.143-0.321l-0.25-4.268 0.25-9.107q0-0.196 0.134-0.321t0.313-0.125 0.313 0.125 0.134 0.321zM17.5 11.429l0.25 8.786-0.25 4.214q0 0.196-0.143 0.339t-0.339 0.143-0.339-0.143-0.161-0.339l-0.214-4.214 0.214-8.786q0.018-0.214 0.161-0.357t0.339-0.143 0.33 0.143 0.152 0.357zM21.286 20.214l-0.25 4.125q0 0.232-0.161 0.393t-0.393 0.161-0.393-0.161-0.179-0.393l-0.107-2.036-0.107-2.089 0.214-11.357v-0.054q0.036-0.268 0.214-0.429 0.161-0.125 0.357-0.125 0.143 0 0.268 0.089 0.25 0.143 0.286 0.464zM41.143 19.875q0 2.089-1.482 3.563t-3.571 1.473h-14.036q-0.232-0.036-0.393-0.196t-0.161-0.393v-16.054q0-0.411 0.5-0.589 1.518-0.607 3.232-0.607 3.482 0 6.036 2.348t2.857 5.777q0.946-0.393 1.964-0.393 2.089 0 3.571 1.482t1.482 3.589z"></path>
</symbol>
<symbol id="icon-codepen" viewBox="0 0 32 32">
<path class="path1" d="M3.857 20.875l10.768 7.179v-6.411l-5.964-3.982zM2.75 18.304l3.446-2.304-3.446-2.304v4.607zM17.375 28.054l10.768-7.179-4.804-3.214-5.964 3.982v6.411zM16 19.25l4.857-3.25-4.857-3.25-4.857 3.25zM8.661 14.339l5.964-3.982v-6.411l-10.768 7.179zM25.804 16l3.446 2.304v-4.607zM23.339 14.339l4.804-3.214-10.768-7.179v6.411zM32 11.125v9.75q0 0.732-0.607 1.143l-14.625 9.75q-0.375 0.232-0.768 0.232t-0.768-0.232l-14.625-9.75q-0.607-0.411-0.607-1.143v-9.75q0-0.732 0.607-1.143l14.625-9.75q0.375-0.232 0.768-0.232t0.768 0.232l14.625 9.75q0.607 0.411 0.607 1.143z"></path>
</symbol>
<symbol id="icon-twitch" viewBox="0 0 32 32">
<path class="path1" d="M16 7.75v7.75h-2.589v-7.75h2.589zM23.107 7.75v7.75h-2.589v-7.75h2.589zM23.107 21.321l4.518-4.536v-14.196h-21.321v18.732h5.821v3.875l3.875-3.875h7.107zM30.214 0v18.089l-7.75 7.75h-5.821l-3.875 3.875h-3.875v-3.875h-7.107v-20.679l1.946-5.161h26.482z"></path>
</symbol>
<symbol id="icon-meanpath" viewBox="0 0 27 32">
<path class="path1" d="M23.411 15.036v2.036q0 0.429-0.241 0.679t-0.67 0.25h-3.607q-0.429 0-0.679-0.25t-0.25-0.679v-2.036q0-0.429 0.25-0.679t0.679-0.25h3.607q0.429 0 0.67 0.25t0.241 0.679zM14.661 19.143v-4.464q0-0.946-0.58-1.527t-1.527-0.58h-2.375q-1.214 0-1.714 0.929-0.5-0.929-1.714-0.929h-2.321q-0.946 0-1.527 0.58t-0.58 1.527v4.464q0 0.393 0.375 0.393h0.982q0.393 0 0.393-0.393v-4.107q0-0.429 0.241-0.679t0.688-0.25h1.679q0.429 0 0.679 0.25t0.25 0.679v4.107q0 0.393 0.375 0.393h0.964q0.393 0 0.393-0.393v-4.107q0-0.429 0.25-0.679t0.679-0.25h1.732q0.429 0 0.67 0.25t0.241 0.679v4.107q0 0.393 0.393 0.393h0.982q0.375 0 0.375-0.393zM25.179 17.429v-2.75q0-0.946-0.589-1.527t-1.536-0.58h-4.714q-0.946 0-1.536 0.58t-0.589 1.527v7.321q0 0.375 0.393 0.375h0.982q0.375 0 0.375-0.375v-3.214q0.554 0.75 1.679 0.75h3.411q0.946 0 1.536-0.58t0.589-1.527zM27.429 6.429v19.143q0 1.714-1.214 2.929t-2.929 1.214h-19.143q-1.714 0-2.929-1.214t-1.214-2.929v-19.143q0-1.714 1.214-2.929t2.929-1.214h19.143q1.714 0 2.929 1.214t1.214 2.929z"></path>
</symbol>
<symbol id="icon-pinterest-p" viewBox="0 0 23 32">
<path class="path1" d="M0 10.661q0-1.929 0.67-3.634t1.848-2.973 2.714-2.196 3.304-1.393 3.607-0.464q2.821 0 5.25 1.188t3.946 3.455 1.518 5.125q0 1.714-0.339 3.357t-1.071 3.161-1.786 2.67-2.589 1.839-3.375 0.688q-1.214 0-2.411-0.571t-1.714-1.571q-0.179 0.696-0.5 2.009t-0.42 1.696-0.366 1.268-0.464 1.268-0.571 1.116-0.821 1.384-1.107 1.545l-0.25 0.089-0.161-0.179q-0.268-2.804-0.268-3.357 0-1.643 0.384-3.688t1.188-5.134 0.929-3.625q-0.571-1.161-0.571-3.018 0-1.482 0.929-2.786t2.357-1.304q1.089 0 1.696 0.723t0.607 1.83q0 1.179-0.786 3.411t-0.786 3.339q0 1.125 0.804 1.866t1.946 0.741q0.982 0 1.821-0.446t1.402-1.214 1-1.696 0.679-1.973 0.357-1.982 0.116-1.777q0-3.089-1.955-4.813t-5.098-1.723q-3.571 0-5.964 2.313t-2.393 5.866q0 0.786 0.223 1.518t0.482 1.161 0.482 0.813 0.223 0.545q0 0.5-0.268 1.304t-0.661 0.804q-0.036 0-0.304-0.054-0.911-0.268-1.616-1t-1.089-1.688-0.58-1.929-0.196-1.902z"></path>
</symbol>
<symbol id="icon-periscope" viewBox="0 0 24 28">
<path class="path1" d="M12.285,1C6.696,1,2.277,5.643,2.277,11.243c0,5.851,7.77,14.578,10.007,14.578c1.959,0,9.729-8.728,9.729-14.578 C22.015,5.643,17.596,1,12.285,1z M12.317,16.551c-3.473,0-6.152-2.611-6.152-5.664c0-1.292,0.39-2.472,1.065-3.438 c0.206,1.084,1.18,1.906,2.352,1.906c1.322,0,2.393-1.043,2.393-2.333c0-0.832-0.447-1.561-1.119-1.975 c0.467-0.105,0.955-0.161,1.46-0.161c3.133,0,5.81,2.611,5.81,5.998C18.126,13.94,15.449,16.551,12.317,16.551z"></path>
</symbol>
<symbol id="icon-get-pocket" viewBox="0 0 31 32">
<path class="path1" d="M27.946 2.286q1.161 0 1.964 0.813t0.804 1.973v9.268q0 3.143-1.214 6t-3.259 4.911-4.893 3.259-5.973 1.205q-3.143 0-5.991-1.205t-4.902-3.259-3.268-4.911-1.214-6v-9.268q0-1.143 0.821-1.964t1.964-0.821h25.161zM15.375 21.286q0.839 0 1.464-0.589l7.214-6.929q0.661-0.625 0.661-1.518 0-0.875-0.616-1.491t-1.491-0.616q-0.839 0-1.464 0.589l-5.768 5.536-5.768-5.536q-0.625-0.589-1.446-0.589-0.875 0-1.491 0.616t-0.616 1.491q0 0.911 0.643 1.518l7.232 6.929q0.589 0.589 1.446 0.589z"></path>
</symbol>
<symbol id="icon-vimeo" viewBox="0 0 32 32">
<path class="path1" d="M30.518 9.25q-0.179 4.214-5.929 11.625-5.946 7.696-10.036 7.696-2.536 0-4.286-4.696-0.786-2.857-2.357-8.607-1.286-4.679-2.804-4.679-0.321 0-2.268 1.357l-1.375-1.75q0.429-0.375 1.929-1.723t2.321-2.063q2.786-2.464 4.304-2.607 1.696-0.161 2.732 0.991t1.446 3.634q0.786 5.125 1.179 6.661 0.982 4.446 2.143 4.446 0.911 0 2.75-2.875 1.804-2.875 1.946-4.393 0.232-2.482-1.946-2.482-1.018 0-2.161 0.464 2.143-7.018 8.196-6.821 4.482 0.143 4.214 5.821z"></path>
</symbol>
<symbol id="icon-reddit-alien" viewBox="0 0 32 32">
<path class="path1" d="M32 15.107q0 1.036-0.527 1.884t-1.42 1.295q0.214 0.821 0.214 1.714 0 2.768-1.902 5.125t-5.188 3.723-7.143 1.366-7.134-1.366-5.179-3.723-1.902-5.125q0-0.839 0.196-1.679-0.911-0.446-1.464-1.313t-0.554-1.902q0-1.464 1.036-2.509t2.518-1.045q1.518 0 2.589 1.125 3.893-2.714 9.196-2.893l2.071-9.304q0.054-0.232 0.268-0.375t0.464-0.089l6.589 1.446q0.321-0.661 0.964-1.063t1.411-0.402q1.107 0 1.893 0.777t0.786 1.884-0.786 1.893-1.893 0.786-1.884-0.777-0.777-1.884l-5.964-1.321-1.857 8.429q5.357 0.161 9.268 2.857 1.036-1.089 2.554-1.089 1.482 0 2.518 1.045t1.036 2.509zM7.464 18.661q0 1.107 0.777 1.893t1.884 0.786 1.893-0.786 0.786-1.893-0.786-1.884-1.893-0.777q-1.089 0-1.875 0.786t-0.786 1.875zM21.929 25q0.196-0.196 0.196-0.464t-0.196-0.464q-0.179-0.179-0.446-0.179t-0.464 0.179q-0.732 0.75-2.161 1.107t-2.857 0.357-2.857-0.357-2.161-1.107q-0.196-0.179-0.464-0.179t-0.446 0.179q-0.196 0.179-0.196 0.455t0.196 0.473q0.768 0.768 2.116 1.214t2.188 0.527 1.625 0.080 1.625-0.080 2.188-0.527 2.116-1.214zM21.875 21.339q1.107 0 1.884-0.786t0.777-1.893q0-1.089-0.786-1.875t-1.875-0.786q-1.107 0-1.893 0.777t-0.786 1.884 0.786 1.893 1.893 0.786z"></path>
</symbol>
<symbol id="icon-hashtag" viewBox="0 0 32 32">
<path class="path1" d="M17.696 18.286l1.143-4.571h-4.536l-1.143 4.571h4.536zM31.411 9.286l-1 4q-0.125 0.429-0.554 0.429h-5.839l-1.143 4.571h5.554q0.268 0 0.446 0.214 0.179 0.25 0.107 0.5l-1 4q-0.089 0.429-0.554 0.429h-5.839l-1.446 5.857q-0.125 0.429-0.554 0.429h-4q-0.286 0-0.464-0.214-0.161-0.214-0.107-0.5l1.393-5.571h-4.536l-1.446 5.857q-0.125 0.429-0.554 0.429h-4.018q-0.268 0-0.446-0.214-0.161-0.214-0.107-0.5l1.393-5.571h-5.554q-0.268 0-0.446-0.214-0.161-0.214-0.107-0.5l1-4q0.125-0.429 0.554-0.429h5.839l1.143-4.571h-5.554q-0.268 0-0.446-0.214-0.179-0.25-0.107-0.5l1-4q0.089-0.429 0.554-0.429h5.839l1.446-5.857q0.125-0.429 0.571-0.429h4q0.268 0 0.446 0.214 0.161 0.214 0.107 0.5l-1.393 5.571h4.536l1.446-5.857q0.125-0.429 0.571-0.429h4q0.268 0 0.446 0.214 0.161 0.214 0.107 0.5l-1.393 5.571h5.554q0.268 0 0.446 0.214 0.161 0.214 0.107 0.5z"></path>
</symbol>
<symbol id="icon-chain" viewBox="0 0 30 32">
<path class="path1" d="M26 21.714q0-0.714-0.5-1.214l-3.714-3.714q-0.5-0.5-1.214-0.5-0.75 0-1.286 0.571 0.054 0.054 0.339 0.33t0.384 0.384 0.268 0.339 0.232 0.455 0.063 0.491q0 0.714-0.5 1.214t-1.214 0.5q-0.268 0-0.491-0.063t-0.455-0.232-0.339-0.268-0.384-0.384-0.33-0.339q-0.589 0.554-0.589 1.304 0 0.714 0.5 1.214l3.679 3.696q0.482 0.482 1.214 0.482 0.714 0 1.214-0.464l2.625-2.607q0.5-0.5 0.5-1.196zM13.446 9.125q0-0.714-0.5-1.214l-3.679-3.696q-0.5-0.5-1.214-0.5-0.696 0-1.214 0.482l-2.625 2.607q-0.5 0.5-0.5 1.196 0 0.714 0.5 1.214l3.714 3.714q0.482 0.482 1.214 0.482 0.75 0 1.286-0.554-0.054-0.054-0.339-0.33t-0.384-0.384-0.268-0.339-0.232-0.455-0.063-0.491q0-0.714 0.5-1.214t1.214-0.5q0.268 0 0.491 0.063t0.455 0.232 0.339 0.268 0.384 0.384 0.33 0.339q0.589-0.554 0.589-1.304zM29.429 21.714q0 2.143-1.518 3.625l-2.625 2.607q-1.482 1.482-3.625 1.482-2.161 0-3.643-1.518l-3.679-3.696q-1.482-1.482-1.482-3.625 0-2.196 1.571-3.732l-1.571-1.571q-1.536 1.571-3.714 1.571-2.143 0-3.643-1.5l-3.714-3.714q-1.5-1.5-1.5-3.643t1.518-3.625l2.625-2.607q1.482-1.482 3.625-1.482 2.161 0 3.643 1.518l3.679 3.696q1.482 1.482 1.482 3.625 0 2.196-1.571 3.732l1.571 1.571q1.536-1.571 3.714-1.571 2.143 0 3.643 1.5l3.714 3.714q1.5 1.5 1.5 3.643z"></path>
</symbol>
<symbol id="icon-thumb-tack" viewBox="0 0 21 32">
<path class="path1" d="M8.571 15.429v-8q0-0.25-0.161-0.411t-0.411-0.161-0.411 0.161-0.161 0.411v8q0 0.25 0.161 0.411t0.411 0.161 0.411-0.161 0.161-0.411zM20.571 21.714q0 0.464-0.339 0.804t-0.804 0.339h-7.661l-0.911 8.625q-0.036 0.214-0.188 0.366t-0.366 0.152h-0.018q-0.482 0-0.571-0.482l-1.357-8.661h-7.214q-0.464 0-0.804-0.339t-0.339-0.804q0-2.196 1.402-3.955t3.17-1.759v-9.143q-0.929 0-1.607-0.679t-0.679-1.607 0.679-1.607 1.607-0.679h11.429q0.929 0 1.607 0.679t0.679 1.607-0.679 1.607-1.607 0.679v9.143q1.768 0 3.17 1.759t1.402 3.955z"></path>
</symbol>
<symbol id="icon-arrow-left" viewBox="0 0 43 32">
<path class="path1" d="M42.311 14.044c-0.178-0.178-0.533-0.356-0.711-0.356h-33.778l10.311-10.489c0.178-0.178 0.356-0.533 0.356-0.711 0-0.356-0.178-0.533-0.356-0.711l-1.6-1.422c-0.356-0.178-0.533-0.356-0.889-0.356s-0.533 0.178-0.711 0.356l-14.578 14.933c-0.178 0.178-0.356 0.533-0.356 0.711s0.178 0.533 0.356 0.711l14.756 14.933c0 0.178 0.356 0.356 0.533 0.356s0.533-0.178 0.711-0.356l1.6-1.6c0.178-0.178 0.356-0.533 0.356-0.711s-0.178-0.533-0.356-0.711l-10.311-10.489h33.778c0.178 0 0.533-0.178 0.711-0.356 0.356-0.178 0.533-0.356 0.533-0.711v-2.133c0-0.356-0.178-0.711-0.356-0.889z"></path>
</symbol>
<symbol id="icon-arrow-right" viewBox="0 0 43 32">
<path class="path1" d="M0.356 17.956c0.178 0.178 0.533 0.356 0.711 0.356h33.778l-10.311 10.489c-0.178 0.178-0.356 0.533-0.356 0.711 0 0.356 0.178 0.533 0.356 0.711l1.6 1.6c0.178 0.178 0.533 0.356 0.711 0.356s0.533-0.178 0.711-0.356l14.756-14.933c0.178-0.356 0.356-0.711 0.356-0.889s-0.178-0.533-0.356-0.711l-14.756-14.933c0-0.178-0.356-0.356-0.533-0.356s-0.533 0.178-0.711 0.356l-1.6 1.6c-0.178 0.178-0.356 0.533-0.356 0.711s0.178 0.533 0.356 0.711l10.311 10.489h-33.778c-0.178 0-0.533 0.178-0.711 0.356-0.356 0.178-0.533 0.356-0.533 0.711v2.311c0 0.178 0.178 0.533 0.356 0.711z"></path>
</symbol>
<symbol id="icon-play" viewBox="0 0 22 28">
<path d="M21.625 14.484l-20.75 11.531c-0.484 0.266-0.875 0.031-0.875-0.516v-23c0-0.547 0.391-0.781 0.875-0.516l20.75 11.531c0.484 0.266 0.484 0.703 0 0.969z"></path>
</symbol>
<symbol id="icon-pause" viewBox="0 0 24 28">
<path d="M24 3v22c0 0.547-0.453 1-1 1h-8c-0.547 0-1-0.453-1-1v-22c0-0.547 0.453-1 1-1h8c0.547 0 1 0.453 1 1zM10 3v22c0 0.547-0.453 1-1 1h-8c-0.547 0-1-0.453-1-1v-22c0-0.547 0.453-1 1-1h8c0.547 0 1 0.453 1 1z"></path>
</symbol>
</defs>
</svg>
</body>
</html><!-- WP Fastest Cache file was created in 0.11215090751648 seconds, on 20-04-19 0:35:44 -->
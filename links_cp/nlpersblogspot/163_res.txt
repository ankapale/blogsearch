<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xmlns:b='http://www.google.com/2005/gml/b' xmlns:data='http://www.google.com/2005/gml/data' xmlns:expr='http://www.google.com/2005/gml/expr'>
<head>
<link href='https://www.blogger.com/static/v1/widgets/2549344219-widget_css_bundle.css' rel='stylesheet' type='text/css'/>
<link href='https://www.brid.gy/webmention/blogger' rel='webmention'/>
<meta content='text/html; charset=UTF-8' http-equiv='Content-Type'/>
<meta content='blogger' name='generator'/>
<link href='https://nlpers.blogspot.com/favicon.ico' rel='icon' type='image/x-icon'/>
<link href='https://nlpers.blogspot.com/2009/' rel='canonical'/>
<link rel="alternate" type="application/atom+xml" title="natural language processing blog - Atom" href="https://nlpers.blogspot.com/feeds/posts/default" />
<link rel="alternate" type="application/rss+xml" title="natural language processing blog - RSS" href="https://nlpers.blogspot.com/feeds/posts/default?alt=rss" />
<link rel="service.post" type="application/atom+xml" title="natural language processing blog - Atom" href="https://www.blogger.com/feeds/19803222/posts/default" />
<!--Can't find substitution for tag [blog.ieCssRetrofitLinks]-->
<meta content='https://nlpers.blogspot.com/2009/' property='og:url'/>
<meta content='natural language processing blog' property='og:title'/>
<meta content='my biased thoughts on the fields of natural language processing (NLP), computational linguistics (CL) and related topics (machine learning, math, funding, etc.)' property='og:description'/>
<!--[if IE]> <script> (function() { var html5 = ("abbr,article,aside,audio,canvas,datalist,details," + "figure,footer,header,hgroup,mark,menu,meter,nav,output," + "progress,section,time,video").split(','); for (var i = 0; i < html5.length; i++) { document.createElement(html5[i]); } try { document.execCommand('BackgroundImageCache', false, true); } catch(e) {} })(); </script> <![endif]-->
<title>natural language processing blog: 2009</title>
<style id='page-skin-1' type='text/css'><!--
/*
* -----------------------------------------------------
* Blogger Template Style
* Name:     Snapshot: Madder
* Designer: Dave Shea
* URL:      mezzoblue.com / brightcreative.com
* Date:     27 Feb 2004
* Updated by: Blogger Team
* ------------------------------------------------------ */
/* Variable definitions
====================
<Variable name="textcolor" description="Text Color"
type="color" default="#474B4E">
<Variable name="pagetitlecolor" description="Blog Title Color"
type="color" default="#7B8186">
<Variable name="titlecolor" description="Post Title Color"
type="color" default="#C4663B">
<Variable name="footercolor" description="Post Footer Color"
type="color" default="#B4BABE">
<Variable name="sidebarcolor" description="Sidebar Title Color"
type="color" default="#7B8186">
<Variable name="linkcolor" description="Link Color"
type="color" default="#DD6599">
<Variable name="visitedlinkcolor" description="Visited Link Color"
type="color" default="#D6A0B6">
<Variable name="bodyfont" description="Text Font"
type="font"
default="normal normal 100% Helvetica, Arial, sans-serif">
*/
/* -- basic html elements -- */
body {
padding: 0;
margin: 0;
font-size: small;
color: #474B4E;
background: #fff;
text-align: center;
}
a {
color: #DD6599;
text-decoration: none;
}
a:visited {
color: #D6A0B6;
}
a:hover {
text-decoration: underline;
color: #FD0570;
}
h1 {
margin: 0;
color: #7B8186;
font-size: 1.5em;
text-transform: lowercase;
}
h1 a, h1 a:link, h1 a:visited {
color: #7B8186;
}
h2, #comments h4 {
font-size: 1em;
margin: 2em 0 0 0;
color: #7B8186;
background: transparent url(//www.blogblog.com/snapshot/bg-header1.gif) bottom right no-repeat;
padding-bottom: 2px;
}
h3 {
font-size: 1em;
margin: 2em 0 0 0;
background: transparent url(//www.blogblog.com/snapshot/bg-header1.gif) bottom right no-repeat;
padding-bottom: 2px;
}
h4, h5 {
font-size: 0.9em;
text-transform: lowercase;
letter-spacing: 2px;
}
h5 {
color: #7B8186;
}
h6 {
font-size: 0.8em;
text-transform: uppercase;
letter-spacing: 2px;
}
p {
margin: 0 0 1em 0;
}
img, form {
border: 0; margin: 0;
}
/* -- layout -- */
#outer-wrapper {
width: 800px;
margin: 0 auto;
text-align: left;
font: normal normal 100% Helvetica, Arial, sans-serif;
background: #fff url(//www.blogblog.com/snapshot/bg-body.gif) 0 0 repeat-y;
}
#header-wrapper {
background: #D8DADC url(//www.blogblog.com/snapshot/bg-headerdiv.gif) 0 0 repeat-y;
}
.descriptionwrapper {
background: #fff url(//www.blogblog.com/snapshot/bg-sidebar.gif) 1px 0 no-repeat;
float: right;
width: 214px;
padding: 0 0 0 8px;
margin: 1px 0 2px 0;
}
.description {
border: 1px solid #F3B89D;
background: #FFD1BC url(//www.blogblog.com/snapshot/bg-profile.gif);
padding: 10px 0 10px 7px;
margin: 4px 0 0 -6px;
color: #C4663B;
}
#header {
background: transparent url(//www.blogblog.com/snapshot/header-01.gif) bottom left no-repeat;
}
#main-wrapper {
line-height: 1.4;
float: left;
padding: 10px 12px;
border-top: solid 1px #fff;
width: 578px;
word-wrap: break-word; /* fix for long text breaking sidebar float in IE */
overflow: hidden;     /* fix for long non-text content breaking IE sidebar float */
/* Tantek hack - http://www.tantek.com/CSS/Examples/boxmodelhack.html */
voice-family: "\"}\"";
voice-family: inherit;
width: 554px;
}
/* IE5 hack */
#main {}
#sidebar {
float:right;
border-top: solid 1px #fff;
padding: 4px 0 0 7px;
background: #fff;
width: 214px;
word-wrap: break-word; /* fix for long text breaking sidebar float in IE */
overflow: hidden;     /* fix for long non-text content breaking IE sidebar float */
}
#footer {
clear: both;
background: #E9EAEB url(//www.blogblog.com/snapshot/bg-footer.gif) bottom left no-repeat;
border-top: solid 1px #fff;
min-height: 15px;
}
/* -- header style -- */
#header h1 {
padding: 12px 0 92px 4px;
width: 707px;
line-height: 1;
}
/* -- content area style -- */
#main {
line-height: 1.4;
}
.post h3 {
font-size: 1.2em;
margin-bottom: 0;
color: #C4663B;
}
.post h3 a,
.post h3 a:visited {
color: #C4663B;
}
.post {
clear: both;
margin-bottom: 4em;
}
.post-footer .post-author,
.post-footer .post-timestamp {
color: #B4BABE;
}
.uncustomized-post-template .post-author,
.uncustomized-post-template .post-timestamp {
float: left;
margin-right: 4px;
}
.uncustomized-post-template .post-footer .comment-link {
float: right;
margin-left: 4px;
}
.post img {
border: 1px solid #E3E4E4;
padding: 2px;
background: #fff;
}
.deleted-comment {
font-style:italic;
color:gray;
}
.feed-links {
clear: both;
line-height: 2.5em;
}
#blog-pager-newer-link {
float: left;
}
#blog-pager-older-link {
float: right;
}
#blog-pager {
text-align: center;
}
.comment-footer {
margin-bottom: 10px;
}
/* -- sidebar style -- */
.sidebar .widget {
margin: 1.3em 0 0.5em 0;
}
.sidebar h2 {
font-size: 1.3em;
}
.sidebar dl {
margin: 0 0 10px 0;
}
.sidebar ul {
list-style: none;
margin: 0;
padding: 0;
}
.sidebar li {
padding-bottom: 5px;
line-height: 1
}
.main .widget .clear {
clear: both;
}
/* -- sidebar style -- */
#footer p {
margin: 0;
padding: 12px 8px;
font-size: 0.9em;
}
.profile-textblock {
margin-left: 0;
clear: both;
}
.profile-img {
float: left;
margin: 0 10px 5px 0;
border: 1px solid #7C78B5;
padding: 4px;
}
/** Page structure tweaks for layout editor wireframe */
body#layout #header-wrapper {
margin-top: 0;
}
body#layout #main-wrapper {
padding:0;
}
img.latex_eq {
padding: 0;
margin: 0;
border: 0;
}

--></style>
<link href='https://www.blogger.com/dyn-css/authorization.css?targetBlogID=19803222&amp;zx=ff9afc1a-535d-4665-a907-b0b1f5ca0ff8' media='none' onload='if(media!=&#39;all&#39;)media=&#39;all&#39;' rel='stylesheet'/><noscript><link href='https://www.blogger.com/dyn-css/authorization.css?targetBlogID=19803222&amp;zx=ff9afc1a-535d-4665-a907-b0b1f5ca0ff8' rel='stylesheet'/></noscript>

</head>
<body>
<div class='navbar section' id='navbar'><div class='widget Navbar' data-version='1' id='Navbar1'><script type="text/javascript">
    function setAttributeOnload(object, attribute, val) {
      if(window.addEventListener) {
        window.addEventListener('load',
          function(){ object[attribute] = val; }, false);
      } else {
        window.attachEvent('onload', function(){ object[attribute] = val; });
      }
    }
  </script>
<div id="navbar-iframe-container"></div>
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
<script type="text/javascript">
      gapi.load("gapi.iframes:gapi.iframes.style.bubble", function() {
        if (gapi.iframes && gapi.iframes.getContext) {
          gapi.iframes.getContext().openChild({
              url: 'https://www.blogger.com/navbar.g?targetBlogID\x3d19803222\x26blogName\x3dnatural+language+processing+blog\x26publishMode\x3dPUBLISH_MODE_BLOGSPOT\x26navbarType\x3dLIGHT\x26layoutType\x3dLAYOUTS\x26searchRoot\x3dhttps://nlpers.blogspot.com/search\x26blogLocale\x3den\x26v\x3d2\x26homepageUrl\x3dhttps://nlpers.blogspot.com/\x26vt\x3d-3933451413689154563',
              where: document.getElementById("navbar-iframe-container"),
              id: "navbar-iframe"
          });
        }
      });
    </script><script type="text/javascript">
(function() {
var script = document.createElement('script');
script.type = 'text/javascript';
script.src = '//pagead2.googlesyndication.com/pagead/js/google_top_exp.js';
var head = document.getElementsByTagName('head')[0];
if (head) {
head.appendChild(script);
}})();
</script>
</div></div>
<div id='outer-wrapper'><div id='wrap2'>
<!-- skip links for text browsers -->
<span id='skiplinks' style='display:none;'>
<a href='#main'>skip to main </a> |
      <a href='#sidebar'>skip to sidebar</a>
</span>
<div id='header-wrapper'>
<div class='header section' id='header'><div class='widget Header' data-version='1' id='Header1'>
<div id='header-inner'>
<div class='titlewrapper'>
<h1 class='title'>
<a href='https://nlpers.blogspot.com/'>
natural language processing blog
</a>
</h1>
</div>
<div class='descriptionwrapper'>
<p class='description'><span>my biased thoughts on the fields of natural language processing (NLP), computational linguistics (CL) and related topics (machine learning, math, funding, etc.)</span></p>
</div>
</div>
</div></div>
</div>
<div id='content-wrapper'>
<div id='main-wrapper'>
<div class='main section' id='main'><div class='widget Blog' data-version='1' id='Blog1'>
<div class='blog-posts hfeed'>

          <div class="date-outer">
        
<h2 class='date-header'><span>30 December 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='6087417128880409526'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/12/some-random-nips-thoughts.html'>Some random NIPS thoughts...</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I missed the first two days of NIPS due to teaching.  Which is sad -- I heard there were great things on the first day.  I did end up seeing a lot that was nice.  But since I missed stuff, I'll instead post some paper suggests from one of my students, <a href="http://www.cs.utah.edu/%7Epiyush/">Piyush Rai</a>, who was there.  You can tell his biases from his selections, but that's life :).  More of my thoughts after his notes...<br /><br />Says Piyush:<br /><blockquote>There was an interesting tutorial by <a href="http://nips.cc/Conferences/2009/Program/speaker-info.php?ID=6358">Gunnar Martinsson</a> on using randomization to speed-up matrix factorization (SVD, PCA etc) of really really large matrices (by "large", I mean something like 106 x 106). People typically use Krylov subspace methods (e.g., the Lanczos algo) but these require multiple passes over the data. It turns out that with the randomized approach, you can do it in a single pass or a small number of passes (so it can be useful in a streaming setting).  The idea is quite simple. Let's assume you want the top K evals/evecs of a large matrix A. The randomized method draws K *random* vectors from a Gaussian and uses them in some way (details <a href="http://amath.colorado.edu/faculty/martinss/Talks/2009_NIPS_tutorial.pdf">here</a>) to get a "smaller version" of A on which doing SVD can be very cheap. Having got the evals/evecs of B, a simple transformation will give you the same for the original matrix A.<br />The success of many matrix factorization methods (e.g., the Lanczos) also depends on how quickly the spectrum decays (eigenvalues) and they also suggest ways of dealing with cases where the spectrum doesn't quite decay that rapidly.<br /><br />Some papers from the main conference that I found interesting:<br /><br /><a href="http://books.nips.cc/papers/files/nips22/NIPS2009_0523.pdf">Distribution Matching for Transduction</a> (Alex Smola and 2 other guys): They use maximum mean discrepancy (MMD) to do predictions in a transduction setting (i.e., when you also have the test data at training time). The idea is to use the fact that we expect the output functions f(X) and f(X') to be the same or close to each other (X are training and X' are test inputs). So instead of using the standard regularized objective used in the inductive setting, they use the distribution discrepancy (measured by say D) of f(X) and f(X') as a regularizer. D actually decomposes over pairs of training and test examples so one can use a stochastic approximation of D (D_i for the i-th pair of training and test inputs) and do something like an SGD.<br /><br /><a href="http://www.cse.ohio-state.edu/%7Embelkin/papers/SSL_SEB_NIPS_09.pdf">Semi-supervised Learning using Sparse Eigenfunction Bases</a> (Sinha and Belkin from Ohio): This paper uses the cluster assumption of semi-supervised learning. They use unlabeled data to construct a set of basis functions and then use labeled data in the LASSO framework to select a sparse combination of basis functions to learn the final classifier.<br /><br /><a href="http://books.nips.cc/papers/files/nips22/NIPS2009_1085.pdf">Streaming k-means approximation</a> (Nir Ailon et al.): This paper does an online optimization of the k-means objective function. The algo is based on the previously proposed kmeans++ algorithm.<br /><br /><a href="http://psiexp.ss.uci.edu/research/papers/RankAggregation_Distribute.pdf">The Wisdom of Crowds in the Recollection of Order Information</a>.  It's about aggregating rank information from various individuals to reconstruct the global ordering.<br /><br /><a href="http://www.cc.gatech.edu/%7Esyang46/papers/nips09.pdf">Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a> (by some folks at gatech): The problem setting is interesting here. Here the "multi-instance" is a bit of a misnomer. It means that each example in turn can consists of several sub-examples (which they call instances). E.g., a document consists of several paragraphs, or a webpage consists of text, images, videos.<br /><br /><a href="http://mlg.eng.cam.ac.uk/porbanz/reports/porbanz_NIPS09TR.pdf">Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a> (Peter Orbanz): If you care about Bayesian nonparametrics. :) It basically builds on the Kolmogorov consistency theorem to formalize and sort of gives a recipe for the construction of nonparametric Bayesian models from their parametric counterparts. Seemed to be a good step in the right direction.<br /><br /><a href="http://books.nips.cc/papers/files/nips22/NIPS2009_0464.pdf">Indian Buffet Processes with Power-law Behavior</a> (YWT and Dilan Gorur): This paper actually does the exact opposite of what I had thought of doing for IBP. The IBP (akin to the sense of the Dirichlet process) encourages the "rich-gets-richer" phenomena in the sense that a dish that has been already selected by a lot of customers is highly likely to be selected by future customers as well. This leads to the expected number of dishes (and thus the latent-features) to be something like O(alpha* log n). This paper tries to be even more aggressive and makes the relationship have a power-law behavior. What I wanted to do was a reverse behavior -- maybe more like a "socialist IBP" :) where the customers in IBP are sort of evenly distributed across the dishes.</blockquote>The rest of this post are random thoughts that occurred to me at NIPS.  Maybe some of them will get other people's wheels turning?  This was originally an email I sent to my students, but I figured I might as well post it for the world.  But forgive the lack of capitalization :):<br /><br />persi diaconis' invited talk about reinforcing random walks... that is,  you take a random walk, but every time you cross an edge, you increase  the probability that you re-cross that edge (see coppersmith + diaconis,  rolles + diaconis).... this relates to a post i had a while ago:  nlpers.blogspot.com/2007/04/multinomial-on-graph.html ... i'm thinking  that you could set up a reinforcing random walk on a graph to achieve  this.  the key problem is how to compute things -- basically want you  want is to know for two nodes i,j in a graph and some n >= 0, whether  there exists a walk from i to j that takes exactly n steps.  seems like  you could craft a clever data structure to answer this question, then  set up a graph multinomial based on this, with reinforcement (the  reinforcement basically looks like the additive counts you get from  normal multinomials)... if you force n=1 and have a fully connected  graph, you should recover a multinomial/dirichlet pair.<br /><br />also from persi's talk, persi and some guy sergei (sergey?) have a paper  on variable length markov chains that might be interesting to look at,  perhaps related to frank wood's sequence memoizer paper from icml last year.<br /><br />finally, also from persi's talk, steve mc_something from ohio has a  paper on using common gamma distributions in different rows to set  dependencies among markov chains... this is related to something i was  thinking about a while ago where you want to set up transition matrices  with stick-breaking processes, and to have a common, global, set of  sticks that you draw from... looks like this steve mc_something guy has  already done this (or something like it).<br /><br />not sure what made me think of this, but related to a talk we had here a  few weeks ago about unit tests in scheme, where they basically randomly  sample programs to "hope" to find bugs... what about setting this up as  an RL problem where your reward is high if you're able to find a bug  with a "simple" program... something like 0 if you don't find a bug, or  1/<code class="moz-txt-verticalline"><span class="moz-txt-tag">|</span>P<span class="moz-txt-tag">|</span></code> if you find a bug with program P.  (i think this came up when i  was talking to percy -- liang, the other one -- about some semantics  stuff he's been looking at.)  afaik, no one in PL land has tried  ANYTHING remotely like this... it's a little tricky because of the  infinite but discrete state space (of programs), but something like an  NN-backed Q-learning might do something reasonable :P.<br /><br />i also saw a very cool "survey of vision" talk by bill freeman... one of  the big problems they talked about was that no one has a good p(image)  prior model.  the example given was that you usually have de-noising  models like p(image)*p(noisy image|image) and you can weight p(image) by  ^alpha... as alpha goes to zero, you should just get a copy of your  noisy image... as alpha goes to infinity, you should end up getting a  good image, maybe not the one you <b class="moz-txt-star"><span class="moz-txt-tag">*</span>want<span class="moz-txt-tag">*</span></b>, but an image nonetheless.  this doesn't happen.<br /><br />one way you can see that this doesn't happen is in the following task.  take two images and overlay them.  now try to separate the two.  you  <b class="moz-txt-star"><span class="moz-txt-tag">*</span>clearly<span class="moz-txt-tag">*</span></b> need a good prior p(image) to do this, since you've lost half  your information.<br /><br />i was thinking about what this would look like in language land.  one  option would be to take two sentences and randomly interleave their  words, and try to separate them out.  i actually think that we could  solve this tasks pretty well.  you could probably formulate it as a FST  problem, backed by a big n-gram language model.  alternatively, you  could take two DOCUMENTS and randomly interleave their sentences, and  try to separate them out.  i think we would fail MISERABLY on this task,  since it requires actually knowing what discourse structure looks like.   a sentence n-gram model wouldn't work, i don't think.  (although maybe  it would?  who knows.)  anyway, i thought it was an interesting thought  experiment.  i'm trying to think if this is actually a real world  problem... it reminds me a bit of a paper a year or so ago where they  try to do something similar on IRC logs, where you try to track who is  speaking when... you could also do something similar on movie transcripts.<br /><br />hierarchical topic models with latent hierarchies  drawn from the coalescent, kind of like hdp, but not quite.  (yeah yeah  i know i'm like a parrot with the coalescent, but it's pretty freaking  awesome :P.)<br /><br /><br />That's it!   Hope you all had a great holiday season, and enjoy your New Years (I know I'm going skiing.  A lot. So there, Fernando! :)).</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/12/some-random-nips-thoughts.html' title='permanent link'>12/30/2009 04:18:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=6087417128880409526' onclick=''>40
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/12/some-random-nips-thoughts.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=6087417128880409526' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=6087417128880409526&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>16 December 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='9173611680043894328'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/12/from-kivenenwarmuth-and-eg-to-cw.html'>From Kivenen/Warmuth and EG to CW learning and Adaptive Regularization</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>This post is a bit of a historical retrospective, because it's only been recently that these things have aligned themselves in my head.<br /><br />The all goes back to <a href="http://dx.doi.org/10.1006/inco.1996.2612">Jyrki Kivenen and Manfred Warmuth's paper on exponentiated gradient descent</a> that dates back to STOC 1995.  For those who haven't read this paper, or haven't read it recently, it's a great read (although it tends to repeat itself a lot).  It's particularly interesting because they <i>derive</i> gradient descent and exponentiated gradient descent (GD and EG) as a <i>consequence</i> of other assumptions.<br /><br />In particular, suppose we have an online learning problem, where at each time step we receive an example <i>x</i>, make a linear prediction <i>(w'x)</i> and then suffer a loss.  The idea is that if we suffer no loss, then we leave <i>w</i> as is; if we do suffer a loss, then we want to balance two goals:<br /><ol><li>Change <span style="font-style: italic;">w</span> enough so that we wouldn't make this error again</li><li>Don't change <span style="font-style: italic;">w</span> too much</li></ol>The key question is how to define "too much."  Suppose that we measure changes in <span style="font-style: italic;">w</span> by looking at Euclidean distance between the updated <span style="font-style: italic;">w</span> and the old <span style="font-style: italic;">w</span>.  If we work through the math for enforcing 1 while minimizing 2, we derive the gradient descent update rule that's been used for optimizing, eg., perceptrons for squared loss for ages.<br /><br />The magic is what happens if we use something <span style="font-style: italic;">other than</span> Euclidean distance.  If, instead, we assume that the <span style="font-style: italic;">w</span>s are all positive, we can use an (unnormalized) KL divergence to measure differences between weight vectors.  Doing this leads to multiplicative updates, or the exponentiated gradient algorithm.<br /><br />(Obvious (maybe?) open question: what happens if you replace the distance with some other divergence, say a Bregman, or alpha or phi-divergence?)<br /><br />This line of thinking leads naturally to <a href="http://jmlr.csail.mit.edu/papers/v7/crammer06a.html">Crammer et al.'s work on Online Passive Aggressive</a> algorithms, from JMLR 2006.  Here, the idea remains the same, but instead of simply ensuring that we make a correct classification, ala rule (1) above, we ensure that we make a correct classification <span style="font-style: italic;">with a margin of at least 1</span>.  They use Euclidean distance to measure the difference in weight vectors, and, for many cases, can get closed-form updates that look GD-like, but not exactly GD.  (Aside: what happens if you use, eg., KL instead of Euclidean?)<br /><br />Two years later, <a href="http://webee.technion.ac.il/people/koby/publications/icml08_variance.pdf">Mark Dredze, Koby Crammer and Fernando Pereira presented Confidence-Weighted Linear Classification</a>.  The idea here is the same: don't change the weight vectors too much, but achieve good classification.  The insight here is to represent weight vectors by <span style="font-style: italic;">distributions</span> over weight vectors, and the goal is to change these <span style="font-style: italic;">distributions</span> enough, but not too much.  Here, we go back to KL, because KL makes more sense for distributions, and make a Gaussian assumption on the weight vector distribution.  (This has close connections both to PAC-Bayes and, if I'm wearing my Bayesian hat, Kalman filtering when you make a Gaussian assumption on the posterior, even though it's not really Gaussian... it would be interesting to see how these similarities play out.)<br /><br />The cool thing here is that you effectively get variable learning rates on different parameters, where confident parameters get moved less.  (In practice, one really awesome effect is that you tend to only need one pass over your training data to do a good job!)  If you're interested in the Bayesian connection, you can get a very similar style algorithm if you do <a href="http://research.microsoft.com/apps/pubs/default.aspx?id=79460">EP on a Bayesian classification algorithm (by Stern, Herbrich and Graepel)</a>, which is what Microsoft Bing uses for online ads.<br /><br />This finally bring us to NIPS this year, where <a href="http://books.nips.cc/papers/files/nips22/NIPS2009_0611.pdf">Koby Crammer, Alex Kulesza and Mark Dredze presented work on Adaptive Regularization of Weight Vectors</a>.  Here, they take Confidence Weighted classification and turn the constraints into pieces of the regularizer (somewhat akin to doing a Lagrangian trick).  Doing so allows them to derive a representer theorem.  But again, the intuition is exactly the same: don't change the classifier too much, but enough.<br /><br />All in all, this is a very interesting line of work.  The reason I'm posting about it is because I think seeing the connections makes it easier to sort these different ideas into bins in my head, depending on what your loss is (squared versus hinge), what your classifier looks like (linear versus distribution over linear) and what your notion of "similar classifiers" is (Euclidean or KL).<br /><br />(Aside: <a href="http://www.stat.rutgers.edu/%7Etzhang/papers/nips00-rwinnow.pdf">Tong Zhang has a paper on regularized winnow methods</a>, which fits in here somewhere, but not quite as cleanly.)</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/12/from-kivenenwarmuth-and-eg-to-cw.html' title='permanent link'>12/16/2009 09:19:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=9173611680043894328' onclick=''>35
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/12/from-kivenenwarmuth-and-eg-to-cw.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=9173611680043894328' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=9173611680043894328&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/classification' rel='tag'>classification</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>,
<a href='https://nlpers.blogspot.com/search/label/online%20learning' rel='tag'>online learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>17 November 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='8565049129965680682'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/11/k-means-vs-gmm-sum-product-vs-max.html'>K-means vs GMM, sum-product vs max-product</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I finished K-means and Gaussian mixture models in class last week or maybe the week before.  I've <a href="http://nlpers.blogspot.com/2009/03/mixture-models-clustering-or-density.html">previously discussed</a> the fact that these two are really solving different problems (despite being structurally so similar), but today's post is about something different.<br /><br />There are two primary differences between the typical presentation of K-means and the typical presentation of GMMs.  (I say "typical" because you can modify these algorithms fairly extensively as you see fit.)  <span style="font-weight: bold;">The first difference</span> is that GMMs just have more parameters.  The parameters of K-means are typically the cluster assignments ("z") and the means ("mu").  The parameters of a GMM are typically these (z and mu) as well as the class prior probabilities ("pi") and cluster covariances ("Sigma").  The GMM model is just richer.  Of course, you can restrict it so all clusters are isotropic and all prior probabilities are even, in which case you've effectively removed this difference (or you can add these things into K-means).  <span style="font-weight: bold;">The second difference</span> is that GMMs operate under the regime of "soft assignments," meaning that points aren't wed to clusters: they only prefer (in a probabilistic sense) some clusters to others.  This falls out naturally from the EM formalization, where the soft assignments are simply the expectations of the assignments under the current model.<br /><br />One can get rid of the second difference by running "hard EM" (also called "Viterbi EM" in NLP land), where the expectations are clamped at their most likely value.  This leads to something that has much more of a K-means feel.<br /><br />This "real EM" versus "hard EM" distinction comes up a lot in NLP, where computing exact expectations is often really difficult.  (Sometimes you get complex variants, like the "pegging" approaches in the IBM machine translation models, but my understanding from people who run in this circle is that pegging is much ado about nothing.)  My general feeling has always been "if you don't have much data, do real EM; if you have tons of data, hard EM is probably okay."  (This is purely from a practical perspective.)  The idea is that when you have tons and tons of data, you can approximate expectations reasonably well by averaging over many data points.  (Yes, this is hand-wavy and it's easy to construct examples where it fails.  But it seems to work many times.)  Of course, you can get pedantic and say "hard EM sucks: it's maximizing p(x,z) but I really want to maximize p(x)" to which I say: ho hum, who cares, you don't actually care about p(x), you care about some extrinsic evaluation metric which, crossing your fingers, you hope correlates with p(x), but for all I know it correlates better with p(x,z).<br /><br />Nevertheless, a particular trusted friend has told me he's always remiss when he can't do full EM and has to do hard EM: he's never seen a case where it doesn't help.  (Or maybe "rarely" would be more fair.)  Of course, this comes at a price: for many models, maximization (search) can be done in polynomial time, but computing expectations can be #P-hard (basically because you have to enumerate -- or count -- over every possible assignment).<br /><br />Now let's think about approximate inference in graphical models.  Let's say I have a graphical model with some nodes I want to maximize over (call them "X") and some nodes I want to marginalize out (call them "Z").  For instance, in GMMs, the X nodes would be the means, covariances and cluster priors; the Z nodes would be the assignments.  (Note that this is departing slightly from typical notation for EM.)  Suppose I want to do inference in such a model.  Here are three things I can do:<br /><ol><li>Just run max-product.  That is, maximize p(X,Z) rather than p(X).</li><li>Just run sum-product.  That is, compute expectations over X and Z, rather than just over Z.</li><li>Run EM, by alternating something like sum-product on Z and something like max-product onX.</li></ol>Of these, only (3) is really doing the "right thing."  Further, let's get away from the notion of p(X) not correlating with some extrinsic evaluation by just measuring ourselves against <span style="font-style: italic;">exact inference.</span>  (Yes, this limits us to relatively small models with 10 or 20 binary nodes.)<br /><br />What do you think happens?  Well, first, things vary as a function of the number of X nodes versus Z nodes in the graph.<br /><br />When most of the nodes are X (maximization) nodes, then max-product does best and EM basically does the same.<br /><br />Whe most of the nodes are Z (marginalization) nodes, then EM does best and sum-product does almost the same.  <span style="font-weight: bold;">But max product also does almost the same.</span><br /><br />This is an effect that we've been seeing regularly, regardless of what the models look like (chains or lattices), what the potentials look like (high temperature or low temperature) and how you initialize these models (eg., in the chain case, EM converges to different places depending on initialization, while sum- and max-product do not).  <span style="font-weight: bold;">Max product is just unbeatable.</span><br /><br />In a sense, from a practical perspective, this is nice.  It says: if you have a mixed model, just run max product and you'll do just as well as if you had done something more complicated (like EM).  But it's also frustrating: we <span style="font-style: italic;">should</span> be getting some leverage out of marginalizing over the nodes that we should marginalize over.  Especially in the high temperature case, where there is lots of uncertainty in the model, max product should start doing worse and worse (note that when we evaluate, we only measure performance on the "X" nodes -- the "Z" nodes are ignored).<br /><br />Likening this back to K-means versus GMM, for the case where the models are the same (GMM restricted to not have priors or covariances), the analogy is that <span style="font-style: italic;">as far as the means go, it doesn't matter which one you use.</span>  Even if there's lots of uncertainty in the data.  Of course, you may get much better <span style="font-style: italic;">assignments</span> from GMM (or you may not, I don't really know).  But if all you really care about at the end of the day are the Xs (the means), then our experience with max-product suggests that it just won't matter.  At all.  Ever.<br /><br />Part of me finds this hard to believe, and note that I haven't actually run experiments with K-means and GMM, but the results in the graphical model cases are sufficiently strong and reproducible that I'm beginning to trust them.  Shouldn't someone have noticed this before, though?  For all the effort that's gone into various inference algorithms for graphical models, why haven't we ever noticed that you just can't beat max-product?<br /><br />(Yes, I'm aware of some theoretical results, eg., the Wainwright result that sum-product + randomized rounding is a provably good approximation to the MAP assignment, but this result actually goes the other way, and contradicts many of our experimental studies where sum-product + rounding just flat out sucks.  Maybe there are other results out there that we just haven't been able to dig up.)</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/11/k-means-vs-gmm-sum-product-vs-max.html' title='permanent link'>11/17/2009 08:30:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=8565049129965680682' onclick=''>60
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/11/k-means-vs-gmm-sum-product-vs-max.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=8565049129965680682' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=8565049129965680682&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/clustering' rel='tag'>clustering</a>,
<a href='https://nlpers.blogspot.com/search/label/graphical%20models' rel='tag'>graphical models</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>07 November 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='5649405799149328916'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/03/nlp-as-study-of-representations.html'>NLP as a study of representations</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p><a href="http://www.cs.utah.edu/%7Eriloff/">Ellen Riloff</a> and I run an NLP reading group pretty much every semester.  Last semester we covered "old school NLP."  We independently came up with lists of what we consider some of the most important ideas (idea = paper) from pre-1990 (most are much earlier) and let students select which to present.  There was a lot of overlap between Ellen's list and mine (not surprisingly).  <span style="color: rgb(255, 0, 0);"><del>If people are interested, I can provide the whole list (just post a comment and I'll dig it up)</del></span>.  <span style="color: rgb(51, 51, 255);">The whole list of topics is posted as a comment.</span>  The topics that were actually selected are <a href="http://www.cs.utah.edu/nlp/nlpmtg-spring09.html">here</a>.<br /><br />I hope the students have found this exercise useful.  It gets you thinking about language in a way that papers from the 2000s typically do not.  It brings up a bunch of issues that we no longer think about frequently.  Like language.  (Joking.)  (Sort of.)<br /><br />One thing that's really stuck out for me is how much "old school" NLP comes across essentially as a study of <i>representations</i>.  Perhaps this is a result of the fact that AI -- as a field -- was (and, to some degree, still is) enamored with knowledge representation problems.  To be more concrete, let's look at a few examples.  It's already been a while since I read these last (I had meant to write this post during the spring when things were fresh in my head), so please forgive me if I goof a few things up.<br /><br />I'll start with one I know well: Mann and Thompson's rhetorical structure theory paper from 1988.  This is basically "the" RST paper.  I think that when a many people think of RST, they think of it as a list of ways that sentences can be organized into hierarchies.  Eg., this sentence provides background for that one, and together they argue in favor of yet a third.  But this isn't really where RST begins.  It begins by trying to understand the communicative role of text structure.  That is, when I write, I am trying to communicate something.  Everything that I write (if I'm writing "well") is toward that end.  For instance, in this post, I'm trying to communicate that old school NLP views representation as the heart of the issue.  This current paragraph is supporting that claim by providing a concrete example, which I am using to try to <span style="font-style: italic;">convince</span> you of my claim.<br /><br />As a more detailed example, take the "Evidence" relation from RST.  M+T have the following characterization of "Evidence."  Herein, "N" is the nucleus of the relation, "S" is the satellite (think of these as sentences), "R" is the reader and "W" is the writer:<br /><span style="font-style: italic;"></span><blockquote><span style="font-style: italic;">relation name:</span>           Evidence<br /><span style="font-style: italic;">constraints on N:  </span>     R might not believe N to a degree satisfactory to W<br /><span style="font-style: italic;">constraints on S:</span>        R believes S or will find it credible<br /><span style="font-style: italic;">constraints on N+S:</span>  R's comprehending S increases R's belief of N<br /><span style="font-style: italic;">the effect:</span>                  R's belief of N is increased<br /><span style="font-style: italic;">locus of effect:</span>           N<br /></blockquote>This is a totally different way from thinking about things than I think we see nowadays.  I kind of liken it to how I tell students <span style="font-style: italic;">not</span> to program.  If you're implementing something moderately complex (say, forward/backward algorithm), first write down all the math, then start implementing.  Don't start implementing first.  I think nowadays (and sure, I'm guilty!) we see a lot of implementing without the math.  Or rather, with plenty of math, but without a representational model of what it is that we're studying.<br /><br />The central claim of the RST paper is that one can think of texts as being organized into elementary discourse units, and these are connected into a tree structure by relations like the one above.  (Or at least this is my reading of it.)  That is, they have <span style="font-style: italic;">laid out a representation of text</span> and claimed that this is how texts get put together.<br /><br />As a second example (this will be sorter), take Wendy Lehnert's 1982 paper, "Plot units and narrative summarization."  Here, the story is about how stories get put together.  The most interesting thing about the plot units model to me is that it breaks from how one might naturally think about stories.  That is, I would naively think of a story as a series of events.  The claim that Lehnert makes is that this is not the right way to think about it.  Rather, we should think about stories as sequences of <span style="font-style: italic;">affect states</span>.  Effectively, an affect state is how a character is feeling at any time.  (This isn't quite right, but it's close enough.)  For example, Lehnert presents the following story:<br /><blockquote>When John tried to start his care this morning, it wouldn't turn over.  He asked his neighbor Paul for help.  Paul did something to the carburetor and got it going.  John thanked Paul and drove to work.</blockquote>The representation put forward for this story is something like: (1) negative-for-John (the car won't start), which leads to (2) motivation-for-John (to get it started, which leads to (3) positive-for-John (it's started), when then links back and <span style="font-style: italic;">resolves</span> (1).  You can also analyze the story from Paul's perspective, and then add links that go between the two characters showing how things interact.  The rest of the paper describes how these relations work, and how they can be put together into more complex event sequences (such as "promised request bungled").  Again, a high level representation of how stories work <span style="font-style: italic;">from the perspective of the characters.</span><br /><br />So now I, W, hope that you, R, have an increased belief in the title of the post.<br /><br />Why do I think this is interesting?  Because at this point, we know <span style="font-style: italic;">a lot</span> about how to deal with structure in language.  From a machine learning perspective, if you give me a structure and some data (and some features!), I will learn something.  It can even be unsupervised if it makes you feel better.  So in a sense, I think we're getting to a point where we can go back, look at some really hard problems, use the deep linguistic insights from two decades (or more) ago, and start taking a crack at things that are really deep.  Of course, features are a big problem; as a very wise man once said to me: "Language is hard.  The fact that statistical association mining at  the word level made it appear easy for the past decade doesn't alter  the basic truth.  <span class="moz-smiley-s1"><span> :-)."  We've got many of the ingredients to start making progress, but it's not going to be easy!<br /></span></span></p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/03/nlp-as-study-of-representations.html' title='permanent link'>11/07/2009 10:38:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=5649405799149328916' onclick=''>61
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/03/nlp-as-study-of-representations.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=5649405799149328916' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=5649405799149328916&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/discourse' rel='tag'>discourse</a>,
<a href='https://nlpers.blogspot.com/search/label/linguistics' rel='tag'>linguistics</a>,
<a href='https://nlpers.blogspot.com/search/label/problems' rel='tag'>problems</a>,
<a href='https://nlpers.blogspot.com/search/label/structured%20prediction' rel='tag'>structured prediction</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>06 November 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='1816082572594767533'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/11/getting-started-in-bayesian-nlp.html'>Getting Started In: Bayesian NLP</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>This isn't so much a post in the "GSI" series, but just two links that recently came out.  <a href="http://www.isi.edu/%7Eknight/">Kevin Knight</a> and <a href="http://www.umiacs.umd.edu/%7Eresnik/">Philip Resnik</a> both just came out with tutorials for Bayesian NLP.  They're both excellent, and almost entirely non-redundant.  I highly recommend reading both.  And I thank Kevin and Philip from the bottom of my heart, since I'd been toying with the idea of writing such a thing (for a few years!) and they've saved me the effort.  I'd probably start with Kevin's and then move on to Philip's (which is more technically meaty), but either order is really fine.<br /><ul><li><a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">Bayesian Inference with Tears</a> by Kevin</li><li><a href="http://www.umiacs.umd.edu/%7Eresnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</a> by Philip</li></ul>Thanks again to both of them.  (And if you haven't read <a href="http://www.isi.edu/natural-language/mt/wkbk.pdf">Kevin's previous workbook on SMT</a> -- which promises free beer! -- I highly recommend that, too.)</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/11/getting-started-in-bayesian-nlp.html' title='permanent link'>11/06/2009 10:53:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=1816082572594767533' onclick=''>55
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/11/getting-started-in-bayesian-nlp.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=1816082572594767533' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=1816082572594767533&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/bayesian' rel='tag'>bayesian</a>,
<a href='https://nlpers.blogspot.com/search/label/news' rel='tag'>news</a>,
<a href='https://nlpers.blogspot.com/search/label/papers' rel='tag'>papers</a>,
<a href='https://nlpers.blogspot.com/search/label/survey' rel='tag'>survey</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>21 October 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='9119507162108309109'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/10/convex-or-not-my-schizophrenic.html'>Convex or not, my schizophrenic personality</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>Machine learning as a field has been very convex-happy for the past decade or so.  So much so that when I saw a tutorial on submodular optimization in ML (one of the best tutorials I've seen), they said something along the lines of "submodularity will be for this decade what convexity was for the last decade."  (Submodularity is cool and I'll post about it more in the future, but it's kind of a discrete analog of convexity.  There's a NIPS workshop on the topic coming up.)  This gives a sense of how important convexity has been.<br /><br />There's also a bit of an undercurrent of "convexity isn't so great" from other sides of the ML community (roughly from the neural nets folks); see, for instance, Yann LeCun's talk <a href="http://videolectures.net/eml07_lecun_wia/">Who's Afraid of Non-convex Loss Functions</a>, a great and entertaining talk.<br /><br />There's a part of me that loves convexity.  Not having to do random restarts, being assured of global convergence, etc., all sounds very nice.  I use logistic regression/maxent for almost all of my classification needs, have never run a neural network, and have only occasionally used svms (though of course they are convex, too).  When I teach ML (as I'm doing now), I make a bit deal about convexity: it makes life easy in many ways.<br /><br />That said, almost none of my recent papers reflect this.  In fact, in the <a href="http://hal3.name/docs/daume08flat.pdf">structure compilation paper</a>, we flat out say that non-linearity in the model (which leads to a non-convex loss function) is the major reason why CRFs outperform independent classifiers in structured prediction tasks!  Moreover, whenever I start doing Bayesian stuff, usually solved with some form of <a href="http://hal3.name/HBC">MCMC</a>, I've completely punted on everything convex.  In a "voting with my feet" world, I could care less about convexity!  For the most part, if you're using EM or sampling or whatever, you don't care much about it either.  Somehow we (fairly easily!) tolerate whatever negative effects there are of non-convex optimization.<br /><br />I think one reason why such things don't both us, as NLPers, as much as they bother the average machine learning person is that we are willing to invest some energy in intelligent initialization.  This already puts us in a good part of the optimization landscape, and doing local hillclimbing from there is not such a big deal.  A classic example is the "Klein and Manning" smart initializer for unsupervised parsing, where a small amount of human knowledge goes a long way above a random initializer.<br /><br />Another style of initialization is the IBM alignment model style.  IBM model 4 is, of course, highly non-convex and ridiculously difficult to optimize (the E step is intractable).  So they do a smart initialization, using the output of model 3.  Model 3, too, is highly non-convex (but not quite so much so), so they initialize with model 2.  And so on, down to model 1, which is actually convex and fairly easy to optimize.  This sequencing of simple models to complex models also happens in some statistical analysis, where you first fit first order effects and then later fit higher order effects.  The danger, of course, is that you got to a bad hill to climb, but this overall generally appears to be a bigger win than starting somewhere in the middle of a random swamp.  (Of course, later, Bob Moore had this cute argument that even though model 1 is convex, we don't actually ever optimize it to the global optimum, so doing clever initialization for model 1 is also a good idea!)<br /><br />These two ideas: clever initialization, and sequential initialization, seem like powerful ideas that I would like to see make their way into more complex models.  For instance, in the original LDA paper, Dave Blei used an initialization where they pick some random documents as seeds for topics.  As far as I know, no one really does this anymore (does anyone know why: does it really not matter?), but as we keep building more and more complex models, and lose hope that our off the shelf optimizer (or sampler) is going to do anything reasonable, we're probably going to need to get back to this habit, perhaps trying to formalize it in the meantime.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/10/convex-or-not-my-schizophrenic.html' title='permanent link'>10/21/2009 09:18:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=9119507162108309109' onclick=''>30
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/10/convex-or-not-my-schizophrenic.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=9119507162108309109' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=9119507162108309109&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/bayesian' rel='tag'>bayesian</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>,
<a href='https://nlpers.blogspot.com/search/label/structured%20prediction' rel='tag'>structured prediction</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>25 September 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='7690788007844261334'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/09/some-notes-on-job-search.html'>Some notes on job search</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>There are tons of "how to apply for academic jobs" write-ups out there; this is not one of them.  It's been four years (egads!) since I began my job search and there are lots of things I think I did well and lots of things I wish I had done differently.<br /><br />When I entered grad school, I was fairly sure that I eventually wanted a university job.  During high school, my career goal was to be a high school math teacher.  Then I went to college and realized that, no, I wanted to teach math to undergraduates.  Then I was an advanced undergraduate and realized that I wanted to teach grads and do research.  Teaching was always very important to me, though of course I fell in love with research later.  It was unfortunate that it took so long for me to actually get involved in research, but my excuse was that I wasn't in CS, where REU-style positions are plentiful and relatively easy to come by (system development, anyone?).<br /><br />However, the more time I spend in grad school, including an internship at MSR with Eric Brill (but during which I befriended many in the NLP group at MSR, a group that I still love), I realized that industry labs were a totally great place to go, too.<br /><br />I ended up applying to basically everything under the sun, provided they had a non-zero number of faculty in either NLP or ML.  I talked (mostly off the record) with a few people about post-doc positions (I heard later than simultaneously exploring post-docs and academic positions is not a good idea: hiring committees don't like to "reconsider" people; I don't know how true this is, but I heard it too late myself to make any decisions based on it), applied for some (okay, many) tenure-track positions, some research-track positions (okay, few) and to the big three industry labs.  I wrote three cover letters, one more tailored to NLP, one more to ML and one more combined, three research statements (ditto) and one teaching statement.  In retrospect, they were pretty reasonable, I think, though not fantastic.  I don't think I did enough to make my future research plans <span style="font-style:italic;">not</span> sound like "more of the same."<br /><br />I suppose my biggest piece of advice for applying is (to the extent possible) find someone you know and trust at the institution and try to figure out exactly what they're looking for.  Obviously you can't change who you are and the work you've done, but you definitely can sell it in slightly different ways.  This is why I essentially had three application packages -- the material was the same, the focus was different.  But, importantly, they were all <span style="font-style:italic;">true</span>.  The more this person trusts you, the more of the inside scoop they can give you.  For instance, we had a robotics/ML position open (which, sadly, we had to close due to budget issues), but in talking to several ML people, they felt that they weren't sufficiently "robotics" enough; I think I was able to dissuade them of this opinion and we ended up getting a lot of excellent applicants before we shut down the slot.<br /><br />Related, it's hard to sell yourself across two fields.  At the time I graduated, I saw myself as basically straddling NLP and ML.  This can be a hard sell to make.  I feel in retrospect that you're often better off picking something and really selling that aspect.  From the other side of the curtain, what often happens is that you need an advocate (or two) in the department to which you're applying.  If you sell yourself as an X person, you can get faculty in X behind you; if you sell yourself as a Y person, you can get faculty in Y behind you.  However, if you sell yourself as a mix, the X faculty might prefer a pure X and the Y faculty might prefer a pure Y.  Of course, this isn't always true: <a href="http://www.cs.umd.edu">Maryland</a> is basically looking for a combined NLP/ML person this year to compliment their existing strengths.  Of course, this doesn't always hold: this is something that you should try to find out from friends at the places to which you're applying.<br /><br />For the application process itself, my experience here and what I've heard from <span style="font-style:italic;">most</span> (but not all) universities is that interview decisions (who to call in) get made by a topic-specific hiring committee.  This means that to get in the door, you have to appeal to the hiring committee, which is typically people in your area, if it's an area-specific call for applications.  Typically your application will go to an admin, first, who will filter based on your cover letter to put you in the right basket (if there are multiple open slots) or the waste basket (for instance, if you don't have a PhD).  It then goes to the hiring committee.  Again, if you have a friend in the department, it's not a bad idea to let them know by email that you've applied after everything has been submitted (including letters) to make sure that you don't end up in the waste bin.<br /><br />Once your application gets to the hiring committee, the hope is that they've already heard of you.  But if they haven't, hopefully they've heard of at least one of your letter writers.  When we get applications, I typically first sort by whether I've heard of the applicant, then by the number of letter writers they have that I've heard of, then loosely by the reputation of their university.  And I make my way down the list, not always all the way to the bottom.  (Okay, I've only done this once, and I think I got about 90% of the way through.)<br /><br />In my experience, what we've looked for in applications is (a) a good research statement, including where you're going so as to distinguish yourself from your advisor, (b) a not-bad teaching statement (it's hard to get a job at a research university on a great teaching statement, but it's easy to lose an offer on a bad one... my feeling here is just to be concrete and not to pad it with BS -- if you don't have much to say, don't say much), (c) great letters, and (d) an impressive CV.  You should expect that the hiring committee <span style="font-style:italic;">will</span> read some of your papers before interviewing you.  This means that if you have dozens, you should highlight somewhere (probably the research statement) what are they best ones that they should read.  Otherwise they'll choose essentially randomly, and (depending on your publishing style) this could hurt.  As always, put your best foot forward and make it <span style="font-style:italic;">easy</span> for the hiring committee to find out what's so great about you.<br /><br />Anyway, that's basically it.  There's lots more at interview stage, but these are my feelings for application stage.  I'd be interested to hear if my characterization of the hiring process is vastly different than at other universities; plus, if there are other openings that might be relevant to NLP/ML folks, I'm sure people would be very pleased to seem them in the comments section.<br /><br />Good luck, all your graduating folks!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/09/some-notes-on-job-search.html' title='permanent link'>9/25/2009 08:29:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=7690788007844261334' onclick=''>53
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/09/some-notes-on-job-search.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=7690788007844261334' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=7690788007844261334&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/hiring' rel='tag'>hiring</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>09 September 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='1399106941578360738'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/09/where-did-you-apply-to-grad-school.html'>Where did you Apply to Grad School?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p><a href="http://www.cs.utah.edu/%7Eriloff">Ellen</a> and I are interested (for obvious reasons) in how people choose what schools to apply to for grad school.  Note that this is <i>not</i> the question of how you chose where to go.  This is about what made the list of where you actually applied.  We'd really appreciate if you'd fill out our 10-15 minute survey and pass it along to your friends (and enemies).  If you're willing, please go <a href="http://www.surveymonkey.com/s.aspx?sm=oyWkBK96uKCxs2_2fIGBnyBw_3d_3d">here</a>.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/09/where-did-you-apply-to-grad-school.html' title='permanent link'>9/09/2009 03:15:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=1399106941578360738' onclick=''>34
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/09/where-did-you-apply-to-grad-school.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=1399106941578360738' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=1399106941578360738&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/survey' rel='tag'>survey</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>07 September 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='3100762567405425285'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/09/acl-and-emnlp-retrospective-many-days.html'>ACL and EMNLP retrospective, many days late</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>Well, ACL and EMNLP are long gone.  And sadly I missed one day of each due either to travel or illness, so most of my comments are limited to Mon/Tue/Fri.  C'est la vie.  At any rate, here are the papers I saw or read that I really liked.<br /><ul><li><p> </p><p><a href="http://aclweb.org/anthology-new/P/P09/P09-1010.pdf">P09-1010</a> [<a href="http://aclweb.org/anthology-new/P/P09/P09-1010.bib">bib</a>]: <b>S.R.K. Branavan; Harr Chen; Luke Zettlemoyer; Regina Barzilay</b><br /><i>Reinforcement Learning for Mapping Instructions to Actions<br /><br /></i>and<br /></p><p><a href="http://aclweb.org/anthology-new/P/P09/P09-1011.pdf">P09-1011</a> [<a href="http://aclweb.org/anthology-new/P/P09/P09-1011.bib">bib</a>]: <b>Percy Liang; Michael Jordan; Dan Klein</b><br /><i>Learning Semantic Correspondences with Less Supervision<br /><br /></i>these papers both address what might roughly be called the grounding problem, or at least trying to learn something about semantics by looking at data.  I really really like this direction of research, and both of these papers were really interesting.  Since I really liked both, and since I think the directions are great, I'll take this opportunity to say what I felt was a bit lacking in each.  In the Branavan paper, the particular choice of reward was both clever and a bit of a kludge.  I can easily imagine that it wouldn't generalize to other domains: thank goodness those Microsoft UI designers happened to call the Start Button something like UI_STARTBUTTON.  In the Liang paper, I worry that it relies too heavily on things like lexical match and other very domain specific properties.  They also should have cited Fleischman and Roy, which Branavan et al did, but which many people in this area seem to miss out on -- in fact, I feel like the Liang paper is in many ways a cleaner and more sophisticated version of the Fleischman paper.<br /></p></li><li><p><a href="http://aclweb.org/anthology-new/P/P09/P09-1054.pdf">P09-1054</a> [<a href="http://aclweb.org/anthology-new/P/P09/P09-1054.bib">bib</a>]: <b>Yoshimasa Tsuruoka; Jun&#8217;ichi Tsujii; Sophia Ananiadou</b><br /><i>Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty<br /><br /></i>This paper is kind of an extension of the truncated gradient approach to learning l1-regularized models that John, Lihong and Tong had last year at NIPS.  The paper did a great job at motivated why L1 penalties is hard.  The first observation is that L1 regularizes optimized by gradient steps like to "step over zero."  This is essentially<i> </i>the observation in truncated gradient and frankly kind of an obvious one (I always thought this is how <span style="font-style: italic;">everyone</span> optimized these models, though of course John, Lihong and Tong actually proved something about it).  The second observation, which goes into this current paper, is that you often end up with a lot of non-zeros simply because you haven't run enough gradient steps since the last increase.  They have a clever way to accumulating these penalties lazily and applying them at the end.  It seems to do very well, is easy to implement, etc.  But they can't (or haven't) proved anything about it.</p></li><li><p><a href="http://aclweb.org/anthology-new/P/P09/P09-1057.pdf">P09-1057</a> [<a href="http://aclweb.org/anthology-new/P/P09/P09-1057.bib">bib</a>]: <b>Sujith Ravi; Kevin Knight</b><br /><i>Minimized Models for Unsupervised Part-of-Speech Tagging<br /><br /></i>I didn't actually see this paper (I think I was chairing a session at the time), but I know about it from talking to Sujith.  Anyone who considers themselves a Bayesian in the sense of "let me put a prior on that and it will solve all your ills" should read this paper.  Basically they show that sparse priors don't give you things that are sparse enough, and that by doing some ILP stuff to minimize dictionary size, you can get tiny POS tagger models that do very well.<br /></p></li><li><a href="http://aclweb.org/anthology-new/D/D09/D09-1006.pdf">D09-1006</a>: [<a href="http://aclweb.org/anthology-new/D/D09/D09-1006.bib">bib</a>] <b><author><first>Omar F.</first> <last>Zaidan</last></author>; <author><first>Chris</first> <last>Callison-Burch</last></author></b><br /><i>Feasibility of Human-in-the-loop Minimum Error Rate Training<br /><br /></i>Chris told me about this stuff back in March when I visited JHU and I have to say I was totally intrigued.  Adam already discussed this paper in an earlier post, so I won't go into more details, but it's definitely a fun paper.<br /><i><br /></i></li><li><a href="http://aclweb.org/anthology-new/D/D09/D09-1011.pdf">D09-1011</a>: [<a href="http://aclweb.org/anthology-new/D/D09/D09-1011.bib">bib</a>] <b><author><first>Markus</first> <last>Dreyer</last></author>; <author><first>Jason</first> <last>Eisner</last></author></b><br /><i>Graphical Models over Multiple Strings<br /><br /></i>This paper is just fun from a technological perspective.  The idea is to have graphical models, but where nodes are distributions over strings represented as finite state automata.  You do message passing, where your messages are now automata and you get to do all your favorite operations (or at least all of Jason's favorite operations) like intersection, composition, etc. to compute beliefs.  Very cool results.<br /><i><br /></i></li><li><a href="http://aclweb.org/anthology-new/D/D09/D09-1024.pdf">D09-1024</a>: [<a href="http://aclweb.org/anthology-new/D/D09/D09-1024.bib">bib</a>] <b><author><first>Ulf</first> <last>Hermjakob</last></author></b><br /><i>Improved Word Alignment with Statistics and Linguistic Heuristics<br /><br /></i>Like the Haghighi coreference paper below, here we see how to do word alignment without fancy math!<i><br /><br /></i></li><li><a href="http://aclweb.org/anthology-new/D/D09/D09-1120.pdf">D09-1120</a>: [<a href="http://aclweb.org/anthology-new/D/D09/D09-1120.bib">bib</a>] <b><author><first>Aria</first> <last>Haghighi</last></author>; <author><first>Dan</first> <last>Klein</last></author></b><br /><i>Simple Coreference Resolution with Rich Syntactic and Semantic Features<br /><br /></i>How to do coreference without math!<i> </i> I didn't know you could still get papers accepted if they didn't have equations in them!</li></ul>In general, here's a trend I've seen in both ACL and EMNLP this year.  It's the "I find a new data source and write a paper about it" trend.  I don't think this trend is either good or bad: it simply is.  A lot of these data sources are essentially Web 2.0 sources, though some are not.  Some are Mechanical Turk'd sources.  Some are the Penn Discourse Treebank (about which there were a ridiculous number of papers: it's totally unclear to me why everyone all of a sudden thinks discourse is cool just because there's a new data set -- what was wrong with the RST treebank that it turned everyone off from discourse for ten years?!  Okay, that's being judgmental and I don't totally feel that way.  But I partially feel that way.)<br /><br /><ul><p></p></ul></p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/09/acl-and-emnlp-retrospective-many-days.html' title='permanent link'>9/07/2009 01:34:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=3100762567405425285' onclick=''>57
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/09/acl-and-emnlp-retrospective-many-days.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=3100762567405425285' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=3100762567405425285&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>,
<a href='https://nlpers.blogspot.com/search/label/papers' rel='tag'>papers</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>14 August 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='6973000274164228070'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/08/classifier-performance-alternative.html'>Classifier performance: alternative metrics of success</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I really enjoyed Mark Dredze's talk at EMNLP on <a href="http://www.cs.jhu.edu/%7Emdredze/publications/emlnp09_mccw.pdf">multiclass confidence weighted algorithms</a>, where they take their CW binary predictors and extend them in two (basically equivalent) ways to a multiclass/structured setting (warning: I haven't read the paper!).  Mark did a great job presenting, as always, and dryly suggested that we should all throw away our perceptrons and our MIRAs and SVMs and just switch to CW permanently.  It was a pretty compelling case.<br /><br />Now, I'm going to pick on basically every "yet another classifier" paper I've read in the past ten years (read: ever).  I'm not trying to point fingers, but just try to better understand why I, personally, haven't yet switched to using these things and continue to use either logistic regression or averaged perceptron for all of my classification needs (aside from the fact that I am rather fond of a particular <a href="http://hal3.name/megam">software package</a> for doing these things -- note, though, that it does support PA and maybe soon CW if I decide to spend 10 minutes implementing it!).<br /><br />Here's the deal.  Let's look at SVM versus logreg.  Whether this is <i>actually</i> true or not, I have this gut feeling that logreg is much less sensitive to hyperparameter selection than are SVMs.  This is not at all based on any science, and the experience that it's based on it somewhat unfair (comparing megam to libSVM, for instance, which use very different optimization methods, and libSVM doesn't do early stopping while megam does).  However, I've heard from at least two other people that they have the same internal feelings.  In other words, here's a caricature of how I believe logreg and SVM behave:<br /><br /><img src="https://lh3.googleusercontent.com/proxy/9bTSz053U9cONcBzJGdtjgQ7HtY1BqKjrKuIJ7PZbuOnH95J-PJsaKQc2HwUTABIGP3B42A=s0-d"><br /><br />That is, if you really tune the regularizer (lambda) well, then SVMs will win out.  But for the majority of the settings, they're either the same or logreg is a bit better.<br /><br />As a result, what do I do?  I use logreg with lambda=1.  That's it.  No tuning, no nothing.<br /><br />(Note that, as I said before, I haven't ever run experiments to verify this.  I think it would be a moderately interesting thing to try to see if it really holds up when all else -- eg., the optimization algorithm, early stopping, implementation, choice of regularizer (L1, L2, KL, etc.), and so on -- are held constant... maybe it's not true.  But if it is, then it's an interesting theoretical question: hinge loss and log loss don't look <i>that</i> different, despite the fact that <a href="http://hunch.net/?p=85">John seems to not like how log loss diverges</a>: why should this be true?)<br /><br />This is also why I use averaged perceptron: there aren't <i>any</i> hyperparameters to select.  It just runs.<br /><br />What I'd really like to see in future "yet another classifier" papers is an analysis of sensitivity to hyperparameter selection.  You could provide graphs and stuff, but these get hard to read.  I like numbers.  I'd like a single number that I can look at.  Here are two concrete proposals for what such a number could be (note: I'm assuming you're also going to provide performance numbers at the best possible selection of hyperparameters from development data or cross validation... I'm talking about something in addition):<br /><ol><li>Performance at a <span style="font-style: italic;">default setting</span> of the hyperparameter.  For instance, SVM-light uses something like average inverse norm of the data vectors as the <span style="font-style: italic;">C</span> parameter. Or you could just us <span style="font-style: italic;">1</span>, like I do for logreg.  In particular, suppose you're testing your algorithm on 20 data sets from UCI.  Pick a single regularization parameter (or parameter selection scheme, ala SVM-light) to use for <span style="font-style: italic;">all</span> of them and report results using that value.  If this is about the same as the "I carefully tuned" setting, I'm happy.  If it's way worse, I'm not so happy.</li><li>Performance within a range.  Let's say that if I do careful hyperparameter selection then I get an accuracy of <span style="font-style: italic;">X.</span>  How large is the range of hyperparameters for which my accuracy is at least <span style="font-style: italic;">X*0.95</span>?  I.e., if I'm willing to suffer 5% multiplicative loss, how lazy can I be about hp selection?  For this, you'll probably need to grid out your performance and then do empirical integration to approximate this.  Of course, you'll need to choose a bounded range for your hp (usually zero will be a lower bound, but you'll have to pick an upper bound, too -- but this is fine: as a practitioner, if you don't give me an upper bound, I'm going to be somewhat unhappy).<br /></li></ol>Neither of these is totally ideal, but I think they'd be a lot better than the current situation of really having no idea!  Maybe there are other proposals out there that I don't know about, or maybe other readers have good ideas.  But for me, if you're going to convince me to switch to your algorithm, this is something that I really really want to know.<br /><br />(As an aside, Mark, if you're reading this, I can imagine the whole CW thing getting a bit confused if you're using feature hashing: have you tried this?  Or has someone else?)</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/08/classifier-performance-alternative.html' title='permanent link'>8/14/2009 09:40:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=6973000274164228070' onclick=''>82
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/08/classifier-performance-alternative.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=6973000274164228070' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=6973000274164228070&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/evaluation' rel='tag'>evaluation</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>03 August 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='5864752401833622021'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/08/acs-machine-translation-papers-at-emnlp.html'>ACS: Machine Translation Papers at EMNLP</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p><span style="color: rgb(255, 0, 0);font-family:arial;" >[Guest Post by <a href="http://homepages.inf.ed.ac.uk/alopez/">Adam Lopez</a>... thanks, Adam!  Hal's comment: you may remember that a while ago I proposed the idea of <a href="http://nlpers.blogspot.com/search/label/ACS">conference area chairs posting summaries of their areas</a>; well, Adam is the first to take me up on this idea... I still think it's a good idea, so anyone else who wants to do so in the future, let me know!]</span><br /><br />Conferences can be exhausting, and back-to-back conferences can be <i>really</i> exhausting, so I want to convince you to pace yourself and save some energy for EMNLP at the end of the week, because we have some really interesting MT papers.  I'll focus mainly on oral presentations, because unlike poster sessions, the parallel format of the oral sessions entails a hard choice between mutually exclusive options, and part of my motivation is to help you make that choice. That being said, there are many interesting papers at the poster session, so do take a look at them!<br /><br />MT is a busy research area, and we have a really diverse set of papers covering the whole spectrum of ideas: from blue sky research on novel models, formalisms, and algorithms, to the hard engineering problems of wringing higher accuracy and speed out of a mature, top-scoring NIST system.  I <i>occasionally</i> feel that my colleagues on far reaches of either side of this spectrum are too dismissive of work on the other side; we need both if we're going to improve translation.<br /><br /><b>Outside the Box</b><br /><br />Before giving you a guided tour through that spectrum, I want to highlight one paper that I found thought-provoking, but hard to classify.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1006.pdf">Zaidan &amp; Callison-Burch</a> question a basic assumption underlying most machine learning approaches to NLP: that we must optimize on an easily computable approximation to the true loss function.  They ask: why not optimize for human judgement?  They design a metric that uses judgements on small snippets of a target sentence (defined by a spanning nonterminal in a parse tree of the aligned source sentence) and figure how many judgements they would need to collect (using Amazon Mechanical Turk) to cover an iteration of MERT, exploiting the fact that these snippets reoccur repeatedly during optimization.  How hard is this exactly?  I would say, in terms of <a href="http://nlpers.blogspot.com/2006/02/art-of-loss-functions.html">this scale of loss functions</a>, that their metric is a 2.  Yet, it turns out to be cheap and fast to compute.  The paper doesn't report results of an actual optimization run, but it's in the works... hopefully you'll learn more at the conference.<br /><br /><b>Connecting Theory and Practice</b><br /><br />A few papers combine deep theoretical insight with convincing empirical results.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1007.pdf">Hopkins &amp; Langmead</a> improve on <a href="http://aclweb.org/anthology-new/J/J07/J07-2003.pdf">cube pruning</a>, a popular approximate search technique for structured models with non-local features (i.e. translation with an integrated language model).  They move cube pruning from its ad hoc roots to a firm theoretical basis by constructing a reduction to A* search, connecting it to classical AI search literature.  This informs the derivation of new heuristics for a syntax-based translation model, including an admissible heuristic to perform <i>exact</i> cube pruning.  It's still globally approximate, but exact for the local prediction problem that cube pruning solves (i.e., what are the n-best state splits of an item, given the n-best input states from previous deductions?).  Amazingly, this is only slightly slower than the inexact version and improves the accuracy of a strong baseline on a large-scale Arabic-English task.<br /><br /><a href="http://aclweb.org/anthology-new/D/D09/D09-1005.pdf">Li &amp; Eisner</a> show how to compute a huge number of statistics efficiently over a combinatorially large number of hypotheses represented in a hypergraph.  The statistics include expected hypothesis length, feature expectation, entropy, cross-entropy, KL divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix.  It's beautifully simple: they recast the quantities of interest as semirings and run the inside (or inside-outside) algorithm.  As an example application, they perform minimum risk training on a small Chinese-English task,  reporting gains in accuracy.  For a related paper on minimum risk techniques, see the poster by <a href="http://aclweb.org/anthology-new/D/D09/D09-1147.pdf">Pauls et al.</a><br /><br /><b>Novel Modeling and Learning Approaches</b><br /><br /><a href="http://aclweb.org/anthology-new/D/D09/D09-1105.pdf">Tromble &amp; Eisner</a> also connect translation to theory by way of a novel model, framing reordering as an instance of the <i>linear ordering problem</i>: given a matrix of pairwise ordering preferences between all words in a sentence, can we find a permutation that optimizes the global score?  This is NP-hard, but they give a reasonable approximation based on ITG, with some clever dynamic programming tricks to make it work.  Then they show how to learn the matrix and use it to reorder test sentences prior to translation, improving over the lexicalized reordering model of Moses on German-English.<br /><br />However, most of the new models at EMNLP are syntax-based.  In the last few years, syntax-based modeling has focused primarily on variants of synchronous context-free grammar (SCFG).  This year there's a lot of work investigating more expressive formalisms.<br /><br />Two papers model translation with restricted variants of synchronous tree-adjoining grammar (STAG).  <a href="http://aclweb.org/anthology-new/D/D09/D09-1021.pdf">Carreras &amp; Collins</a> model syntax atop phrase pairs with a parser using sister adjunction (as in their <a href="http://aclweb.org/anthology-new/W/W08/W08-2102.pdf">2008 parser</a>).  The model resembles a synchronous version of Markov grammar, which also connects it to recent dependency models of translation (e.g. <a href="http://aclweb.org/anthology-new/P/P08/P08-1066.pdf">Shen et al. 2008</a>, <a href="http://aclweb.org/anthology-new/P/P09/P09-1087.pdf">Galley et al. 2009</a>, Gimpel &amp; Smith below, and  <a href="http://aclweb.org/anthology-new/D/D09/D09-1123.pdf">Hassan et al.</a> in the poster session).  Decoding is NP-complete, and devising efficient beam search is a key point in the paper.  The resulting system outperforms Pharaoh on German-English.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1076.pdf">DeNeefe &amp; Knight</a> model target-side syntax via synchronous tree insertion grammar (STIG).  It's similar to synchronous tree substitution grammar (STSG; previously realized in MT as <a href="http://aclweb.org/anthology-new/N/N04/N04-1035.pdf">GHKM</a>) with added left- and right-adjunction operations to model optional arguments.  They show how to reuse a lot of the STSG machinery via a grammar transformation from STIG to STSG, and the results improve on a strong Arabic-English baseline.<br /><br /><a href="http://aclweb.org/anthology-new/D/D09/D09-1023.pdf">Gimpel & Smith</a> use a relatively new formalism: <a href="http://aclweb.org/anthology-new/W/W06/W06-3104.pdf">quasi-synchronous</a> dependency grammar (QDG).  In quasi-synchronous grammar, the generation of a target syntax tree is conditioned on (but not necessarily isomorphic to) a source syntax tree.  Formally, each target node can be annotated with <i>any</i> source node.  Since in dependency grammar the nodes are words, their QDG  model resembles a word-to-word model.  Decoding with QDG was not obvious given past work, and is one of several novel contributions of the paper.  Another is the idea that all possible biphrases can fire an associated feature, regardless of overlap.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1107.pdf">Kriinen</a> makes this idea central.  Instead of reasoning over the latent derivations of a generative model, his model directly optimizes a feature-based representation of the target sentence, where the features consist of any biphrase in the training set (per standard heuristics).  This raises some new problems -- such as how to find the target sentence given the optimal feature vector -- which are solved with dynamic programming.  The decoder doesn't quite beat Moses when used with a language model, but it's an order of magnitude faster!<br /><br />Three other papers operate on STSG models, with an emphasis on learning techniques. <a href="http://aclweb.org/anthology-new/D/D09/D09-1037.pdf">Cohn &amp; Blunsom</a> reformulate tree-to-string STSG induction as a problem in non-parametric Bayesian inference, extending <a href="http://aclweb.org/anthology-new/N/N09/N09-1062.pdf">their TSG model</a> for monolingual parsing, and removing the dependence on heuristics over noisy GIZA++ word alignments.   The model produces more compact rules, and outperforms GHKM on a Chinese-English task. This is a hot topic: check out <a href="http://aclweb.org/anthology-new/D/D09/D09-1136.pdf">Liu &amp; Gildea</a>'s poster for an alternative Bayesian formulation of the same problem and language pair.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1039.pdf">Galron et al.</a> look at <i>tree-to-tree</i> STSG (from a Data-Oriented Parsing perspective), with an eye towards discriminatively learning STSG rules to optimize for translation accuracy.<br /><br />Bayesian inference also figures in the model of <a href="http://aclweb.org/anthology-new/D/D09/D09-1075.pdf">Chung & Gildea</a>, who aim at bilingually-informed segmentation of a source language.  The model is like IBM Model 1, except that the source positions are actually substrings of the source instead of single positions.  Reasoning over the substring boundaries makes it resemble an HMM, and they use a sparse prior to avoid overfitting.  Tokenizing new text uses the marginal distribution on source language segmentations, and this performs almost as well as a supervised segmenter on Chinese, and better on Korean, in end-to-end translation.<br /><br />SCFG models aren't completely forgotten: <a href="http://aclweb.org/anthology-new/D/D09/D09-1073.pdf">Zhang & Li</a> offer a new twist on reordering in binary-branching SCFG. Given a source parse, we could <a href="http://aclweb.org/anthology-new/C/C08/C08-1127.pdf">train a maximum entropy classifier</a> to decide whether any binary production should be inverted; this requires a lot of computation over sparse vectors.  They instead represent the features implicitly using a <a href="http://books.nips.cc/papers/files/nips14/AA58.pdf">tree convolution kernel</a>, showing nice gains in Chinese-English.<br /><br />On the algorithmic side, <a href="http://aclweb.org/anthology-new/D/D09/D09-1079.pdf">Levenberg & Osborne</a> look at language modeling under the condition that we have unbounded data streams in both source and target language, bounded computation, and the desire to bias our language model towards more recent language use without constantly retraining it.  They accomplish this with online perfect hashing (extending <a href="http://aclweb.org/anthology-new/P/P08/P08-1058.pdf">previous</a> <a href="http://aclweb.org/anthology-new/P/P07/P07-1065.pdf">work</a>) in a succinct data structure that supports deletions, showing that they can draw on recent information in both the source and the target to incrementally update the model while keeping a bounded memory footprint.<br /><br /><a href="http://aclweb.org/anthology-new/D/D09/D09-1050.pdf">Bai et al.</a> focus on the problem of acquiring multiword expressions (i.e. idioms), showing why typical word alignment methods fail, and using a combination of statistical association measures and heuristics to fix the problem, with small gains in Chinese-English.<br /><br /><b>Decoding</b><br /><br />Since SCFG models have become mainstream, there's been a greater emphasis on decoding.  Following a <a href="http://aclweb.org/anthology-new/N/N06/N06-1033.pdf">recent strand</a> of <a href="http://aclweb.org/anthology-new/N/N09/N09-1026.pdf">research</a> on grammar transformations for SCFG, <a href="http://aclweb.org/anthology-new/D/D09/D09-1038.pdf">Xiao et al.</a> observe that, in the space of possible transformations, many will pair source yields with huge numbers of target yields, which compete during decoding and thus result in more search errors.  The trick is to select a transform that distributes target yields more evenly across source yields.  They pose this as an optimization problem and give a greedy algorithm; the resulting grammar is reliably better under a variety of conditions on a Chinese-English task. Meanwhile, <a href="http://aclweb.org/anthology-new/D/D09/D09-1108.pdf">Zhang et al.</a> engineer more efficient STSG decoding for the case in which the source is a parse forest and source units are tree fragments.  The trick is to encode translation rules in the tree path equivalent of a prefix tree. On Chinese-English this improves decoding speed and ultimately translation accuracy, because the decoder can consider larger fragments much more efficiently.  Finally, see <a href="http://aclweb.org/anthology-new/D/D09/D09-1117.pdf">Finch & Sumita</a>'s comprehensive poster on bidirectional phrase-based decoding for a huge number of language pairs.<br /><br /><b>Onwards and Upwards</b><br /><br />The align/extract/MERT pipeline popularized by Moses and other NIST-style systems is incredibly hard to improve, but several papers manage just that.<br /><br /><a href="http://aclweb.org/anthology-new/D/D09/D09-1024.pdf">Hermjakob</a>'s word aligner starts from lexical translation parameters learned by a statistical alignment model. Then, following some fairly general observations on different linguistic classes of words, it uses some well-motivated heuristics to fix a whole bunch of little things that many more principled models ignore: the different behavior of content words (improved via careful manipulation of pointwise mutual information) and function words (improved via constraints from parse structure) is treated along with careful handling of numbers, transliterations, and morphology to give strong improvements in Arabic-English.<br /><br /><a href="http://aclweb.org/anthology-new/D/D09/D09-1106.pdf">Liu et al.</a> then extract phrases by relaxing <a href="http://aclweb.org/anthology-new/N/N03/N03-1017.pdf">standard heuristic constraints</a>.  Given a posterior probability for every alignment point, they simply calculate the probability that a phrase would be extracted, and use this as their count in the typical frequency-based estimate.  It's efficient and improves Chinese-English.<br /><br />Three papers incorporate new feature types into strong baseline translation models, following a <a href="http://aclweb.org/anthology-new/D/D08/D08-1024.pdf">recent</a> <a href="http://aclweb.org/anthology-new/N/N09/N09-1025.pdf">trend</a>. <a href="http://aclweb.org/anthology-new/D/D09/D09-1008.pdf">Shen et al.</a> devise some clever local features using source-side context, derivation span length, and dependency modeling to make impressive improvements on an already <a href="http://aclweb.org/anthology-new/P/P08/P08-1066.pdf">impressive baseline system</a> in both Chinese-English and Arabic-English.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1074.pdf">Matsoukas et al.</a> then show how a mixed-genre system can effectively be adapted for a particular target domain, by using a small amount data to tune weights tied to genre and collection types in the training corpus, again with strong results in Arabic-English.  <a href="http://aclweb.org/anthology-new/D/D09/D09-1022.pdf">Mauser et al.</a> take their previous <a href="http://aclweb.org/anthology-new/D/D08/D08-1039.pdf">triplet lexicon model</a> (a probabilistic feature using an outside source word as additional conditioning context) and move it from a reranking step into the decoding step, with a nice experimental treatment showing improvements in large-scale Chinese-English and Arabic-English.<br /><br />If you've seen the latest NIST results, you know that system combination gives <i>huge</i> improvements.   Check out posters by <a href="http://aclweb.org/anthology-new/D/D09/D09-1125.pdf">He & Toutanova</a>, <a href="http://aclweb.org/anthology-new/D/D09/D09-1114.pdf">Duan et al.</a>, and <a href="http://aclweb.org/anthology-new/D/D09/D09-1115.pdf">Feng et al.</a> to learn the latest techniques.  Last but not least, if you need a strategy for language pairs with very little parallel data, the poster by <a href="http://aclweb.org/anthology-new/D/D09/D09-1141.pdf">Nakov & Ng</a> will interest you.<br /><br /><b>Thanks</b><br /><br />EMNLP was the first time I've been area chair for a conference, and it was really rewarding to work with such great volunteers and< to see the great papers that were selected (I should note here that I included two papers <i>not</i> on my track that I'm quite familiar with -- the ones from Edinburgh).  It was also very enlightening, but that's another story.  Many thanks to Hal for <a href="http://nlpers.blogspot.com/2008/06/old-school-conference-blogging.html">offering this forum</a> to share the results!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/08/acs-machine-translation-papers-at-emnlp.html' title='permanent link'>8/03/2009 05:38:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=5864752401833622021' onclick=''>151
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/08/acs-machine-translation-papers-at-emnlp.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=5864752401833622021' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=5864752401833622021&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/ACS' rel='tag'>ACS</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20translation' rel='tag'>machine translation</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>01 August 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4852035910034519912'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/08/destination-singapore.html'>Destination: Singapore</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>Welcome to everyone to <a href="http://www.acl-ijcnlp-2009.org/">ACL</a>!  It's pretty rare for me to end up conferencing in a country I've been before, largely because I try to avoid it.  When I was here last time, I stayed with <a href="//www.blogger.com/www.gatsby.ucl.ac.uk/%7Eywteh">Yee Whye</a>, who was here at the time as a postdoc at NUS, and lived here previously in his youth.  As a result, he was an excellent "tour guide."  With his help, here's a list of mostly food related stuff that you should definitely try while here (see also the <a href="http://www.colips.org/blog/acl-ijcnlp-2009/">ACL blog</a>):<br /><ul><li>Pepper crab.  The easiest to find are the "No Signboard" restaurant chain.  Don't wear a nice shirt unless you plan on doing laundry.<br /></li><li>Chicken rice.  This sounds lame.  Sure, chicken is kind of tasty.  Rice is kind of tasty.  But the key is that the rice is cooked in or with melted chicken fat.  It's probably the most amazingly simple and delicious dish I've ever had.  "Yet Kun" (or something like that) is along Purvis street.</li><li>Especially for dessert, there's Ah Chew, a Chinese place around Liang Seah street in the Bugis area (lots of other stuff there too).</li><li>Hotpot is another local specialty: there is very good  spicy Szechuan hotpot around Liang Seah street.</li><li>For real Chinese tea, <a href="http://www.tea-chapter.com.sg/">here</a>.  (Funny aside: when I did this, they first asked "have you had tea before?"  Clearly the meaning is "have you had real Chinese tea prepared traditionally and tasted akin to a wine tasting?"  But I don't think I would ever ask someone "have you had wine before?"  But I also can't really think of a better way to ask this!)</li><li>Good late night snacks can be found at Prata stalls (eg., indian roti with curry).</li><li>The food court at Vivocity, despite being a food court, is very good.  You should have some hand-pressed sugar cane juice -- very sweet, but very tasty (goes well with some spicy hotpot).</li><li>Chinatown has good Chinese dessert (eg., bean stuff) and frog leg porridge.</li></ul>Okay, so this list is <span style="font-style: italic;">all</span> food.  But frankly, what else are you going to do here?  Go to malls?  :).  There's definitely nice architecture to be seen; I would recommend the Mosque off of Arab street; of course you have to go to the Esplanade (the durian-looking building); etc.  You can see a <a href="http://www.cs.utah.edu/%7Ehal/photos_singapore.html">few photos</a> from my last trip here.<br /><br />Now, I realize that most of the above list is not particularly friendly to my happy cow friends.  Here's a <a href="http://www.happycow.net/gmaps/searchaddmaps.php?distance=20&amp;address=Central%20Singapore,%20Singapore">list of restaurants</a> that happy cow provides.  There are quite a few vegetarian options, probably partially because of the large Muslim population here.  There aren't as many vegan places, but certainly enough.  For the vegan minded, there is a <a href="http://living-vegan.blogspot.com/">good blog about being vegan in Singapore</a> (first post is about a recent local talk by Campbell, the author of <a href="http://www.amazon.com/China-Study-Comprehensive-Nutrition-Implications/dp/1932100660/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1249169838&amp;sr=8-1">The China Study</a>, which I recommend everyone at least reads).  I can't vouch for the quality of these places, but here's a short list drawn from Living Vegan:<br /><ul><li>Mushroom hotpot at <a href="http://living-vegan.blogspot.com/2009/07/mushroom-hotpot-and-dimsum-buffet-ling.html">Ling Zhi</a></li><li>Fried fake meat <a href="http://living-vegan.blogspot.com/2009/06/delivege-udon-noodles.html">udon noodles</a> (though frankly I'm not a big fan of fake meat)</li><li><a href="http://living-vegan.blogspot.com/2009/06/new-green-pasture-cafe.html">Green Pasture cafe</a>; looks like you probably have to be a bit careful here in terms of what you order</li><li><a href="http://living-vegan.blogspot.com/2009/01/yes-natural-and-lotus-heart.html">Yes Natural</a>; seems like it has a bunch of good options</li><li><a href="http://living-vegan.blogspot.com/2008/08/set-meals-at-lotus-veg-restaurant.html">Lotus Veg restaurant</a>, seems to have a bunch of dim sum (see <a href="http://living-vegan.blogspot.com/2008/04/dim-sum-lotus-vegetarian-restaurant.html">here</a>, too)<br /></li><li>If you must, there's <a href="http://living-vegan.blogspot.com/2008/08/vegan-pizzas-in-singapore.html">pizza</a></li><li>And oh-my-gosh, there's actually <a href="http://living-vegan.blogspot.com/2008/03/vegetarian-chicken-rice-at-fork-and.html">veggie chicken rice</a>, though it doesn't seem like it holds up to the same standards as real chicken rice (if it did, that would be impressive)</li></ul>Okay, you can find more for yourself if you go through their links :).<br /><br />Enjoy your time here!<br /><br /><i>Quick update:</i> Totally forgot about coffee.  If you need your espresso kick, Highlander coffee (49 Kampong Bahru Road) comes the most recommended, but is a bit of a hike from the conference area.  Of course, you could also try the local specialty: burnt crap with condensed milk (lots and lots of discussion especially on page two <a href="http://www.coffeegeek.com/forums/worldregional/australasia/310510?Page=2">here</a>).</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/08/destination-singapore.html' title='permanent link'>8/01/2009 05:17:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4852035910034519912' onclick=''>142
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/08/destination-singapore.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4852035910034519912' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4852035910034519912&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>21 July 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='7278192582658457328'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/07/non-parametric-as-memorizing-in-exactly.html'>Non-parametric as memorizing, in exactly the wrong way?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>There is a cool view of the whole non-parametric Bayes thing that I think is very instructive.  It's easiest to see in the case of the Pitman-Yor language modeling work by Frank Wood and Yee Whye Teh.  The view is "memorize what you can, and back off (to a parametric model) when you can't."  This is basically the "backoff" view... where NP Bayes comes in is to control the whole "what you can" aspect.  In other words, if you're doing language modeling, try to memorize two grams; but if you haven't seen enough to be confident in your memorization, back off to one grams; and if you're not confident there, back off to a uniform distribution (which is our parametric model -- the base distribution).<br /><br />Or, if you think about the state-splitting PCFG work (done both at Berkeley and at Stanford), basically what's going on is that we're memorizing as many differences of tree labels as we can, and then backing off to the "generic" label when memorization fails. Or if we look at Trevor Cohn's NP-Bayes answer to DOP, we see a similar thing: memorize big tree chunks, but if you can't, fall back to a simple CFG (our parametric model).<br /><br />Now, the weird thing is that this mode of memorization is kind of backwards from how I (as an outsider) typically interpret cognitive models (any cogsci people out there, feel free to correct me!).  If you take, for instance, morphology, there's evidence that this is exactly <span style="font-style: italic;">not</span> what humans do.  We (at least from a popular science) perspective, basically memorize simple rules and then remember exceptions.  That is, we remember that to make the past tense of a verb, we add "-ed" (the sound, not the characters) but for certain verbs, we don't: go/went, do/did, etc.  You do little studies where you ask people to inflect fake words and they generally follow the rule, not the exceptions (but see * below).<br /><br />If NP Bayes had its way on this problem (or at least if the standard models I'm familiar with had their way), they would memorize "talk" -> "talked" and "look" -> "looked" and so on because they're so familiar.  Sure, it would still memorize the exceptions, but it would also memorize the really common "rule cases too... why?  Because of course it <span style="font-style:italic;">could</span> fall back to the parametric model, but these are so common that the standard models would really like to take advantage of the rich-get-richer phenomenon on things like DPs, thus saving themselves cost by memorizing a new "cluster" for each common word.  (Okay, this is just my gut feeling about what such models would do, but I think it's at least defensible.)  Yes, you could turn the DP "alpha" parameter down, but I guess I'm just not convinced this would do the right thing.  Maybe I should just implement such a beast but, well, this is a blog post, not a *ACL paper :P.<br /><br />Take as an alternative example the language modeling stuff.  Basically what it says is "if you have enough data to substantiate memorizing a 5 gram, you should probably memorize a 5 gram."  But why?  If you could get the same effect with a 2 or 3 gram, why waste the storage/time?!<br /><br />I guess you could argue "your prior is wrong," which is probably true for most of these models.  In which case I guess the question is "what prior does what I want?"  I don't have a problem with rich get richer -- in fact, I think it's good in this case.  I also don't have a problem with a logarithmic growth rate in the number of exceptions (though I'd be curious how this holds up empirically -- in general, I'm a big fan of checking if your prior makes sense; for instance, Figure 3 (p16) of <a href="http://hal3.name/docs#daume05dpscm">my supervised clustering paper</a>).  I just don't like the notion of memorizing when you don't have to.<br /><br />(*) I remember back in grad school a linguist from Yale came and gave a talk at USC.  Sadly, I can't remember who it was: if anyone wants to help me out, I'd really appreciate it!  The basic claim of the talk is that humans actually memorize a lot more than we give them credit for.  The argument was in favor of humans basically memorizing all morphology and <span style="font-style:italic;">not</span> backing off to rules like "add -ed."  One piece of evidence in favor of this was timing information for asking people to inflect words: the timing seemed to indicate a <span style="font-style:italic;">linear search</span> through a long list of words that could possibly be inflected.  I won't say much more about this because I'm probably already misrepresenting it, but it's an interesting idea.  And, if true, maybe the NP models are doing exactly what they should be doing!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/07/non-parametric-as-memorizing-in-exactly.html' title='permanent link'>7/21/2009 07:41:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=7278192582658457328' onclick=''>147
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/07/non-parametric-as-memorizing-in-exactly.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=7278192582658457328' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=7278192582658457328&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/bayesian' rel='tag'>bayesian</a>,
<a href='https://nlpers.blogspot.com/search/label/linguistics' rel='tag'>linguistics</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>05 July 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='7392957434501063185'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/07/small-changes-beget-good-or-bad.html'>Small changes beget good or bad examples?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>If you compare vision research with NLP research, there are a lot of interesting parallels.  Like we both like linear models.  And conditional random fields.  And our problems are a lot harder than binary classification.  And there are standard data sets that we've been evaluating on for decades and continue to evaluate on (I'm channeling Bob here :P).<br /><br />But there's one thing that happens, the difference of which is so striking, that I'd like to call it to center stage.  It has to do with "messing with our inputs."<br /><br />I'll spend a bit more time describing the vision approach, since it's probably less familiar to the average reader.  Suppose I'm trying to handwriting recognition to identify digits from zero to nine (aka MNIST).  I get, say, 100 labeled zeros, 100 labeled ones, 100 labeled twos and so on.  So a total of 1000 data points.  I can train any off the shelf classifier based on pixel level features and get some reasonable performance (maybe 80s-90s, depending).<br /><br />Now, I want to insert knowledge.  The knowledge that I want to insert is some notion of invariance.  I.e., if I take an image of a zero and translate it left a little bit, it's still a zero.  Or up a little bit.  Of if I scale it up 10%, it's still a zero.  Or down 10%.  Or if I rotate it five degrees.  Or negative five.  All zeros.  Same hold for all the other digits.<br /><br />One way to insert this knowledge is to muck with the learning algorithm.  That's too complicated for me: I want something simpler.  So what I'll do is take my 100 zeros and 100 ones and so on and just manipulate them a bit.  That is, I'll sample a random zero, and apply some small random transformations to it, and call it another labeled example, also a zero.  Now I have 100,000 training points.  I train my off the shelf classifier based on pixel level features and get 99% accuracy or more.  The same trick works for other vision problem (eg., recognizing animals).  (This process is so common that it's actually described in Chris Bishop's new-ish PRML book!)<br /><br />This is what I mean by small changes (to the input) begetting good example.  A slightly transformed zero is still a zero.<br /><br />Of course, you have to be careful.  If you rotate a six by 180 degrees, you get a nine.  If you rotate a cat by 180 degrees, you get an unhappy cat.  More seriously, if you're brave, you might start looking at a class of transformations called diffeomorphisms, which are fairly popular around here.  These are nice because of their nice mathematical properties, but un-nice because they can be slightly too flexible for certain problems.<br /><br />Now, let's go over to NLP land.  Do we ever futz with our inputs?<br /><br />Sure!<br /><br />In language modeling, we'll sometimes permute words or replace one word with another to get a negative example.  Noah Smith futzed with his inputs in contrastive estimation to produce negative examples by swapping adjacent words, or deleting words.<br /><br />In fact, try as I might, I cannot think of a single case in NLP where we make small changes to an input to get another good input: we always do it to get a <span style="font-style: italic;">bad</span> input!<br /><br />In a sense, this means that one thing that vision people have that we don't have is a notion of semantics preserving transformations.  Sure, linguists (especially those from that C-guy) study transformations.  And there's a vague sense that work in paraphrasing leads to transformations that maintain semantic equivalence.  But the thing is that we really don't know any transformations that preserve semantics.  Moreover, some transformations that seem benign (eg., passivization) actually are not: one of my favorite papers at NAACL this year by <a href="http://umiacs.umd.edu/~resnik/pubs/greene_resnik_naacl2009.pdf">Greene and Resnik showed that syntactic structure affects sentiment</a> (well, them, drawing on a lot of psycholinguistics work)!<br /><br />I don't have a significant point to this story other than it's kind of weird.  I mentioned this to some people at ICML and got a reaction that replacing words with synonyms should be fine.  I remember doing this in high school, when word processors first started coming with thesauri packed in.  The result seemed to be that if I actually knew the word I was plugging in, life was fine... but if not, it was usually a bad replacement.  So this seems like something of a mixed bag: depending on how liberal you are with defining "synonym" you might be okay do this, but you might also not be.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/07/small-changes-beget-good-or-bad.html' title='permanent link'>7/05/2009 07:55:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=7392957434501063185' onclick=''>174
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/07/small-changes-beget-good-or-bad.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=7392957434501063185' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=7392957434501063185&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/linguistics' rel='tag'>linguistics</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>30 June 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='5007776794380796144'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/06/icmlcoltuai-2009-retrospective.html'>ICML/COLT/UAI 2009 retrospective</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>This will probably be a bit briefer than my corresponding NAACL post because even by day two of ICML, I was a bit burnt out; I was also constantly swapping in other tasks (grants, etc.).  Note that John has already posted his <a href="http://hunch.net/?p=813">list of papers</a>.<br /><ol><li><a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#317">#317</a>: Multi-View Clustering via Canonical Correlation Analysis (<i>Chaudhuri, Kakade, Livescu, Sridharan</i>).  This paper shows a new application of CCA to clustering across multiple views.  They use some wikipedia data in experiments and actually prove something about the fact that (under certain multi-view-like assumptions), CCA does the "right thing."<br /></li><li><a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#295">#295</a>: Learning Nonlinear Dynamic Models (<i>Langford, Salakhutdinov,, Zhang</i>).  The cool idea here is to cut a deterministic classifier in half and use its internal state as a sort of sufficient statistic.  Think about what happens if you represent your classifier as a circuit (DAG); then anywhere you cut along the circuit gives you a sufficient representation to predict.  To avoid making circuits, they use neural nets, which have an obvious "place to cut" -- namely, the internal nodes.</li><li><a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#364">#364</a>: Online Dictionary Learning for Sparse Coding (<i>Mairal, Bach, Ponce, Sapiro</i>).  A new approach to sparse coding; the big take-away is that it's online and fast.</li><li><a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#394">394</a>: MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification (<i>Zhu, Ahmed, Xing</i>).  This is a very cute idea for combining objectives across topic models (namely, the variational objective) and classification (the SVM objective) to learn topics that are good for performing a classification task.</li><li><a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#393">#393</a>: Learning from Measurements in Exponential Families (<i>Liang, Jordan, Klein</i>).  Suppose instead of seeing (x,y) pairs, you just see some statistics on (x,y) pairs -- well, you can still learn.  (In a sense, this formalizes some work out of the UMass group; see also the Bellare, Druck and McCallum paper at UAI this year.)</li><li><a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#119">#119</a>: Curriculum Learning (<i>Bengio, Louradour, Collobert, Weston</i>).  The idea is to present examples in a well thought-out order rather than randomly.  It's a cool idea; I've tried it in the context of unsupervised parsing (the unsearn paper at ICML) and it never helped and often hurt (sadly).  I curriculum-ified by sentence length, though, which is maybe not a good model, especially when working with WSJ10 -- maybe using vocabulary would help.</li><li> <a href="http://www.cs.utah.edu/%7Ehal/tmp/icml/abstracts.html#319">#319</a>: A Stochastic Memoizer for Sequence Data (<i>Wood, Archambeau, Gasthaus, James, Whye Teh</i>).  If you do anything with Markov models, you should read this paper.  The take away is: how can I learn a Markov model with (potentially) infinite memory in a linear amount of time and space, and with good "backoff" properties.  Plus, there's some cool new technology in there.</li><li> A Uniqueness Theorem for Clustering     <i>Reza Bosagh Zadeh, Shai Ben-David.  </i>I already talked about this issue a bit, but the idea here is that if you fix k, then the clustering axioms become satisfiable, and are satisfied by two well known algorithms.  Fixing k is a bit unsatisfactory, but I think this is a good step in the right direction.</li><li>Convex Coding<i> David Bradley, J. Andrew Bagnell.  </i>The idea is to make coding convex by making it infinite!  And then do something like boosting.</li><li>On Smoothing and Inference for Topic Models<i>  Arthur Asuncion, Max Welling, Padhraic Smyth, Yee Whye Teh.  </i>If you do topic models, read this paper: basically, none of the different inference algorithms do any better than the others (perplexity-wise) if you estimate hyperparameters well.  Come are, of course, faster though.</li><li>Correlated Non-Parametric Latent Feature Models <i>Finale Doshi-Velez, Zoubin Ghahramani.</i>  This is an indian-buffet-process-like model that allows factors to be correlated.  It's somewhat in line with our <a href="http://www.cs.utah.edu/%7Ehal/docs/#daume08ihfrm">own paper from NIPS</a> last year.  There's still something a bit unsatisfactory in both our approach and their approach that we can't do this "directly."</li><li>Domain Adaptation: Learning Bounds and Algorithms.      <i>Yishay Mansour, Mehryar Mohri and Afshin Rostamizadeh</i>.  Very good work on some learning theory for domain adaptation based on the idea of stability.</li></ol>Okay, that's it.  Well, not really: there's lots more good stuff, but those were the things that caught my eye.  Feel free to tout your own favorites in the comments.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/06/icmlcoltuai-2009-retrospective.html' title='permanent link'>6/30/2009 06:50:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=5007776794380796144' onclick=''>45
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/06/icmlcoltuai-2009-retrospective.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=5007776794380796144' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=5007776794380796144&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>25 June 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='952319119169860390'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/06/should-there-be-shared-task-for-semi.html'>Should there be a shared task for semi-supervised learning in NLP?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p><span style="color: rgb(255, 0, 0);">(Guest post by </span><a style="color: rgb(255, 0, 0);" href="http://ssli.ee.washington.edu/people/duh/">Kevin Duh</a><span style="color: rgb(255, 0, 0);">.  Thanks, Kevin!)</span><br /><br />At the <a href="http://sites.google.com/site/sslnlp/">NAACL SSL-NLP Workshop</a> recently, we discussed whether there ought to be a "shared task" for semi-supervised learning in NLP. The panel discussion consisted of <a href="http://www.cs.utah.edu/%7Ehal/">Hal Daume</a>, <a href="http://www.cs.brown.edu/%7Edmcc">David McClosky</a>, and <a href="http://pages.cs.wisc.edu/%7Egoldberg/">Andrew Goldberg</a> as panelists and audience input from Jason Eisner, Tom Mitchell, and many others. Here we will briefly summarize the points raised and hopefully solicit some feedback from blog readers.<br /><br /><b>Three motivations for a shared task</b><br /><br />A1. Fair comparison of methods: A common dataset will allow us to compare different methods in an insightful way. Currently, different research papers use different datasets or data-splits, making it difficult to draw general intuitions from the combined body of research.<br /><br />A2. Establish a methodology for evaluating SSLNLP results: How exactly should a semi-supervised learning method be evaluated? Should we would evaluate the same method for both low-resource scenarios (few labeled points, many unlabeled points) and high-resource scenarios (many labeled points, even more unlabeled points)? Should we evaluate the same method under different ratios of labeled/unlabeled data? Currently there is no standard methodology for evaluating SSLNLP results, which means that the completeness/quality of experimental sections in research papers varies considerably.<br /><br />A3. Encourage more research in the area: A shared task can potentially lower the barrier of entry to SSLNLP, especially if it involves pre-processed data and  community support network. This will make it easier for researchers in outside fields, or researchers with smaller budgets to contribute their expertise to the field. Furthermore, a shared task can potentially direct the community research efforts in a particular subfield. For example, "online/lifelong learning for SSL" and "SSL as joint inference of multiple tasks and heterogeneous labels" (a la Jason Eisner's keynote) were identified as new, promising areas to focus on in the panel discussion. A shared task along those lines may help us rally the community behind these efforts.<br /><br /><b>Arguments against the above points</b><br /><br />B1. Fair comparison: Nobody really argues against fair comparison of methods. The bigger question, however, is whether there exist a *common* dataset or task where everyone is interested in. At the SSLNLP Workshop, for example, we had papers in a wide range of areas ranging from information extraction to parsing to text classification to speech. We also had papers where the need for unlabeled data is intimately tied in to particular components of a larger system. So, a common dataset is good, but what dataset can we all agree upon?<br /><br />B2. Evaluation methodology: A consistent standard for evaluating SSLNLP results is nice to have, but this can be done independently from a shared task through, e.g. an influential paper or gradual recognition of its importance by reviewers. Further, one may argue: what makes you think that your particular evaluation methodology is the best? What makes you think people will adopt it generally, both inside and outside of the shared task?<br /><br />B3. Encourage more research: It is nice to lower the barriers to entry, especially if we have pre-processed data and scripts. However, it has been observed in other shared tasks that often it is the pre-processing and features that matter most (more than the actual training algorithm). This presents a dilemma: If the shared task pre-processes the data to make it easy for anyone to join, will we lose the insights that may be gained via domain knowledge? On the other hand, if we present the data in raw form, will this actually encourage outside researchers to join the field?<br /><br /><b>Rejoinder</b><br /><br />A straw poll at the panel discussion showed that people are generally in favor of looking into the idea of a shared task. The important question is how to make it work, and especially how to address counterpoints B1 (what task to choose) and B3 (how to prepare the data). We did not have enough time during the panel discussion to go through the details, but here are some suggestions:<br /><ul><li> We can view NLP problems as several big "classes" of problems: sequence prediction, tree prediction, multi-class classification, etc. In choosing a task, we can pick a representative task in each class, such as name-entity recognition for sequence prediction, dependency parsing for tree prediction, etc. This common dataset won't attract everyone in NLP, but at least it will be relevant for a large subset of researchers.<br /><br /></li><li> If participants are allowed to pre-process their own data, the evaluation might require participant to submit a supervised system along with their semi-supervised system, using the same feature set and setup, if possible. This may make it easier to learn from results if there are differences in pre-processing.<br /><br /></li><li> There should also be a standard supervised and semi-supervised baseline (software) provided by the shared task organizer. This may lower the barrier of entry for new participants, as well as establish a common baseline result.<br /></li></ul>Any suggestions? Thoughts?</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/06/should-there-be-shared-task-for-semi.html' title='permanent link'>6/25/2009 02:40:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=952319119169860390' onclick=''>40
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/06/should-there-be-shared-task-for-semi.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=952319119169860390' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=952319119169860390&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>20 June 2009</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='3983224320399781458'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2009/06/why-i-dont-buy-clustering-axioms.html'>Why I Don't Buy Clustering Axioms</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>In NIPS 15, Jon Kleinberg presented some <a href="https://www.blogger.com/www.cs.cornell.edu/home/kleinber/nips15.pdf">impossibility results for clustering</a>.  The idea is to specify three axioms that all clustering functions should obey and examine those axioms.<br />
<br />
Let (X,d) be a metric space (so X is a discrete set of points and d is a metric over the points of X).  A clustering function F takes d as input and produces a clustering of the data.  The three axioms Jon states that all clustering functions should satisfy are:<br />
<ol>
<li><span style="font-weight: bold;">Scale invariance:</span> For all d, for all a&gt;0, F(ad) = F(d).  In other words, if I transform all my distances by scaling them uniformly, then the output of my clustering function should stay the same.</li>
<li><span style="font-weight: bold;">Richness:</span> The range of F is the set of all partitions.  In other words, there isn't any bias that prohibits us from producing some particular partition.</li>
<li><span style="font-weight: bold;">Consistency:</span> Suppose F(d) gives some clustering, C.  Now, modify d by shrinking distances within clusters of C and expanding distances between clusters in C.  Call the new metric d'.  Then F(d') = C.</li>
</ol>
Kleinberg's result is that there is no function <span style="font-style: italic;">F</span> that simultaneously satisfies all these requirements.  Functions can satisfy two, but never all three.  There have been a bunch of follow on papers, including one at <a href="http://books.nips.cc/papers/files/nips21/NIPS2008_0383.pdf">NIPS</a> last year and one that I just saw at <a href="http://www.andrew.cmu.edu/user/rbosaghz/papers/slunique.pdf">UAI</a>.<br />
<br />
If you think about these axioms a little bit, they kind of make sense.  My problem is that if you think about them a lot of bit, you (or at least I) realize that they're broken.  The biggest broken one is consistency, which becomes even more broken when combined with scale invariance.<br />
<br />
What I'm going to do to convince you that consistency is broken is start with some data in which there is (what I consider) a natural clustering into two clusters.  I'll then apply consistency a few times to get something that (I think) should yield a different clustering.<br />
<br />
Let's start with some data.  The colors are my opinion as to how the data should be clustered:<br />
<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/data1.png" style="cursor: pointer; display: block; height: 196px; margin: 0px auto 10px; text-align: center; width: 256px;">I hope you agree with my color coding.  Now, let's apply consistency.  In particular, let's move some of the red points, only reducing inter-clustering distances.  Formally, we find the closest pair of points and move things toward those.<br />
<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/data2.png" style="cursor: pointer; display: block; height: 196px; margin: 0px auto 10px; text-align: center; width: 256px;">The arrows denote the directions these points will be moved.    To make the situation more visually appealing, let's move things into lines:<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/data3.png" style="cursor: pointer; display: block; height: 196px; margin: 0px auto 10px; text-align: center; width: 256px;">Okay, this is already looking funky.  Let's make it even worse.  Let's apply consistency again and start moving some blue points:<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/data4.png" style="cursor: pointer; display: block; height: 196px; margin: 0px auto 10px; text-align: center; width: 256px;">Again, let's organize these into a line:<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/data5.png" style="cursor: pointer; display: block; height: 196px; margin: 0px auto 10px; text-align: center; width: 256px;">And if I had given you this data to start with, my guess is the clustering you'd have come up with is more like:<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/data6.png" style="cursor: pointer; display: block; height: 196px; margin: 0px auto 10px; text-align: center; width: 256px;">This is a violation of consistency.<br />
<br />
So, what I'd like someone to do is to argue to my <span style="font-style: italic;">why</span> consistency is actually a desirable property.<br />
<br />
I can come up with lots of other examples.  One reason why this invariance is bad is because it renders the notion of "reference sizes" irrelevant.  This is of course a problem if you have prior knowledge (eg., one thing measured in millimeters, the other in kilometers).  But even in the case where you don't know knowledge, what you can do is take the following.  Take data generated by thousands of well separated Gaussians, so that the clearly right thing to do is have one cluster per Gaussian.  Now, for each of these clusters except for one, shrink them down to single points.  This is possible by consistency.  Now, your data basically looks like thousands-1 of clusters with zero inter-cluster distances and then one cluster that's spread out.  But now it seems that the reasonable thing is to put each data point that was in this old cluster into its own cluster, essentially because I feel like the other data shows you what clusters should look like.  If you're not happy with this, apply scaling and push these points out super far from each other.  (I don't think this example is as compelling as the one I drew in pictures, but I still think it's reasonable.<br />
<br />
Now, in the UAI paper this year, they show that if you fix the number of clusters, these axioms are now consistent.  (Perhaps this has to do with the fact that all of my "weird" examples change the number of clusters -- though frankly I don't think this is necessary... I probably could have arranged it so that the resulting green and blue clusters look like a single line that maybe should just be one cluster by itself.)  But I still feel like consistency isn't even something we want.<br />
<br />
(Thanks to the <a href="http://apollonius.cs.utah.edu/mediawiki/index.php/Algorithms_Seminar">algorithms group</a> at Utah for discussions related to some of these issues.)<br />
<br />
<b>UPDATE 20 JUNE 2009, 3:49PM EST</b><br />
<br />
Here's some data to justify the "bad things happen even when the number of clusters stays the same" claim.<br />
<br />
Start with this data:<br />
<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/clusters1.png" style="cursor: pointer; display: block; margin: 0px auto 10px; text-align: center;"><br />
Now, move some points toward the middle (note they have to spread to the side a bit so as not to decrease intra-cluster distances).<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/clusters2.png" style="cursor: pointer; display: block; margin: 0px auto 10px; text-align: center;"><br />
Yielding data like the following:<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/clusters3.png" style="cursor: pointer; display: block; margin: 0px auto 10px; text-align: center;"><br />
Now, I feel like two horizontal clusters are most natural here.  But you may disagree.  What if I add some more data (ie., this is data that would have been in the original data set too, where it clearly would have been a third cluster):<br />
<img alt="" border="0" src="https://www.umiacs.umd.edu/~hal/nlpers/clusters4.png" style="cursor: pointer; display: block; margin: 0px auto 10px; text-align: center;"><br />
And if you still disagree, well then I guess that's fine.  But what if there were hundreds of other clusters like that.<br />
<br />
I guess the thing that bugs me is that I seem to like clusters that have similar structures.  Even if some of these bars were rotated arbitrarily (or, preferably, in an axis-aligned manner), I would still feel like there's some information getting shared across the clusters.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2009/06/why-i-dont-buy-clustering-axioms.html' title='permanent link'>6/20/2009 09:43:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=3983224320399781458' onclick=''>61
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2009/06/why-i-dont-buy-clustering-axioms.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=3983224320399781458' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=3983224320399781458&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

        </div></div>
      
</div>
<div class='blog-pager' id='blog-pager'>
<span id='blog-pager-newer-link'>
<a class='blog-pager-newer-link' href='https://nlpers.blogspot.com/search?updated-max=2010-08-31T19:09:00-06:00&amp;max-results=20&amp;reverse-paginate=true' id='Blog1_blog-pager-newer-link' title='Newer Posts'>Newer Posts</a>
</span>
<span id='blog-pager-older-link'>
<a class='blog-pager-older-link' href='https://nlpers.blogspot.com/search?updated-max=2009-06-20T09:43:00-06:00&amp;max-results=20' id='Blog1_blog-pager-older-link' title='Older Posts'>Older Posts</a>
</span>
<a class='home-link' href='https://nlpers.blogspot.com/'>Home</a>
</div>
<div class='clear'></div>
<div class='blog-feeds'>
<div class='feed-links'>
Subscribe to:
<a class='feed-link' href='https://nlpers.blogspot.com/feeds/posts/default' target='_blank' type='application/atom+xml'>Posts (Atom)</a>
</div>
</div>
</div></div>
</div>
<div id='sidebar-wrapper'>
<div class='sidebar section' id='sidebar'><div class='widget Profile' data-version='1' id='Profile1'>
<h2>About Me</h2>
<div class='widget-content'>
<a href='https://www.blogger.com/profile/02162908373916390369'><img alt='My photo' class='profile-img' height='80' src='//4.bp.blogspot.com/-S8xlo8qPXpU/WL7hxHzH1uI/AAAAAAAAAe8/4fKySmJ6GjgrPj3tFRsoZGtO5ddT4dAigCK4B/s80/menyc-250x300.png' width='65'/></a>
<dl class='profile-datablock'>
<dt class='profile-data'>
<a class='profile-name-link g-profile' href='https://www.blogger.com/profile/02162908373916390369' rel='author' style='background-image: url(//www.blogger.com/img/logo-16.png);'>
hal
</a>
</dt>
</dl>
<a class='profile-link' href='https://www.blogger.com/profile/02162908373916390369' rel='author'>View my complete profile</a>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=Profile&widgetId=Profile1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("Profile1"));' rel='nofollow' target='configProfile1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div><div class='widget Label' data-version='1' id='Label1'>
<h2>Labels</h2>
<div class='widget-content list-label-widget-content'>
<ul>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/acl'>acl</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/ACS'>ACS</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/advising'>advising</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/algorithms'>algorithms</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/bayesian'>bayesian</a>
<span dir='ltr'>(10)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/chunking'>chunking</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/classification'>classification</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/clustering'>clustering</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/community'>community</a>
<span dir='ltr'>(26)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/conferences'>conferences</a>
<span dir='ltr'>(45)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/coreference'>coreference</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/data'>data</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/discourse'>discourse</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/domain%20adaptation'>domain adaptation</a>
<span dir='ltr'>(5)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/evaluation'>evaluation</a>
<span dir='ltr'>(9)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/finite%20state%20methods'>finite state methods</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/graphical%20models'>graphical models</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/hiring'>hiring</a>
<span dir='ltr'>(7)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/information%20retrieval'>information retrieval</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/journals'>journals</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/language%20modeling'>language modeling</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/linguistics'>linguistics</a>
<span dir='ltr'>(7)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/loss%20functions'>loss functions</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/machine%20learning'>machine learning</a>
<span dir='ltr'>(45)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/machine%20translation'>machine translation</a>
<span dir='ltr'>(6)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/mcmc'>mcmc</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/news'>news</a>
<span dir='ltr'>(4)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/online%20learning'>online learning</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/papers'>papers</a>
<span dir='ltr'>(17)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/parsing'>parsing</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/PL'>PL</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/poll'>poll</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/problems'>problems</a>
<span dir='ltr'>(12)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/questions'>questions</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/random'>random</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/research'>research</a>
<span dir='ltr'>(12)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/reviewing'>reviewing</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/sentiment'>sentiment</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/software'>software</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/speech'>speech</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/statistics'>statistics</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/structured%20prediction'>structured prediction</a>
<span dir='ltr'>(5)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/summarization'>summarization</a>
<span dir='ltr'>(4)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/survey'>survey</a>
<span dir='ltr'>(6)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/teaching'>teaching</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/theory'>theory</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/topic%20models'>topic models</a>
<span dir='ltr'>(1)</span>
</li>
</ul>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=Label&widgetId=Label1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("Label1"));' rel='nofollow' target='configLabel1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div><div class='widget BlogList' data-version='1' id='BlogList1'>
<h2 class='title'>My Blog List</h2>
<div class='widget-content'>
<div class='blog-list-container' id='BlogList1_container'>
<ul id='BlogList1_blogs'>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/gRmMbXYxsKaakAnDOlu22pn73PfLHqJy0TyxmGvXE-ML8dhnWFBNJqSvxj-F3dL2PwdVpIFMEuhSg9TVTHET5mLLbaXNHw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://statmodeling.stat.columbia.edu' target='_blank'>
Statistical Modeling, Causal Inference, and Social Science</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~3/FfbqmTx5vIo/' target='_blank'>
What&#8217;s the upshot?
</a>
</span>
<div class='item-time'>
10 hours ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/KFVTjwSPBHxu5yTo_QpLSb-6NWXuxf2iDLG36Od0kKaCT5noGO4IyTITQC-PI7Ndf_ixNQFYuDRikb_FAI1KdA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://nuit-blanche.blogspot.com/' target='_blank'>
Nuit Blanche</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/blogspot/wCeDd/~3/ii2Vklni8BM/neumann-networks-for-inverse-problems.html' target='_blank'>
Neumann Networks for Inverse Problems in Imaging
</a>
</span>
<div class='item-time'>
22 hours ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/BnQftQM3jOX5aZOFpbgfSbqiTDnjOdA3RAaJu6RpPDh5yYUC1nVfGB4wgFODh2WwDNjgUrOJSaS0e2Rc1yttzDRC=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://lucatrevisan.wordpress.com' target='_blank'>
in   theory</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/' target='_blank'>
Online Optimization Post 2: Constructing Pseudorandom Sets
</a>
</span>
<div class='item-time'>
1 day ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/0p3jSguSoH_b3zcUal959JlweUeJsUYgG7_ZDjsq9rPDH7hWb2xQIslqAloeEhhnSWRi-3tNicza8RuACF2qNFJNWjjbXZjZ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://blog.computationalcomplexity.org/' target='_blank'>
Computational Complexity</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html' target='_blank'>
Geo-Centric Complexity
</a>
</span>
<div class='item-time'>
1 day ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/AXKkogToc1zzaQzQ7gQUlMlDOT6yNpXYfoH-u-EN96orKMNShipc4B2-3w16-oxNSA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://lemire.me/blog' target='_blank'>
Daniel Lemire's blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/daniel-lemire/atom/~3/S2s4xkcZJVA/' target='_blank'>
The shopper&#8217;s dilemma: wait for new technology or buy now?
</a>
</span>
<div class='item-time'>
2 days ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/TArGCb8h_qeNfbn8xnZy2RHwgKsytt8SWh5ZNtfgwhAZt4bkHuFLHrIAGwOtIsSzmmXmCNQ0AShaBFwuggP4GZTC=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://mysliceofpizza.blogspot.com/' target='_blank'>
my slice of pizza</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://mysliceofpizza.blogspot.com/2019/04/conference-acm-siam-algorithmic.html' target='_blank'>
Conference: ACM-SIAM Algorithmic Principles of Computer Systems (APoCS20)
</a>
</span>
<div class='item-time'>
1 week ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/ZKegNZzwsozNHjZpHzqSB0RB3LH1XsP1IQCNkIRdzmSfXTs02gCPddS2GpBW-5uN9-xevClYJ7t5i_aie0o=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://terrytao.wordpress.com' target='_blank'>
What's new</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://terrytao.wordpress.com/2019/04/12/nominations-for-2020-doob-prize-now-open/' target='_blank'>
Nominations for 2020 Doob Prize now open
</a>
</span>
<div class='item-time'>
2 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/i986p1ASLFBHizPu7L-Izuc0qmUrwf8P1BUod8VTD041x9HGwTveSdoIAuT4Qayitd-54yMh8ag=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://blog.geomblog.org/' target='_blank'>
The Geomblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/TheGeomblog/~3/f-Go67FP6JE/new-conference-announcement.html' target='_blank'>
New conference announcement
</a>
</span>
<div class='item-time'>
2 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/sby2Wfl6cEqBMMS7feUM8QArNbm7_7tFmV5YGMt_MTCTQFGBFCuWGxbjuGlaTRrPRgqZ5y8oetOMrw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://wadler.blogspot.com/' target='_blank'>
Wadler's Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://wadler.blogspot.com/2019/03/pinkers-thirteen-rules-for-better.html' target='_blank'>
Pinker's Thirteen Rules for Better Writing
</a>
</span>
<div class='item-time'>
4 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/yxui1sZaOfhURaKfvaicNoXIzX5vD4DblwTuFJuQkMNTTYUt70-jS6QCK4L14MlSplmF6WSYDW6Lpk_L=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.talkingbrains.org/' target='_blank'>
Talking Brains</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/TalkingBrains/~3/spYQGDVYZ_k/postdoc-positions-in-cognitive.html' target='_blank'>
Postdoc Positions in Cognitive Neuroscience of Communication at the University of Connecticut
</a>
</span>
<div class='item-time'>
4 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/3mp7ahsogluZg1OlYi96t2lnp14FEJ4ZzPCbXZj-R7eVlc31hW1QQH6aeaJGEDIERIvcAoIXxOLd=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://www.jstatsoft.org/index.php/jss' target='_blank'>
Journal of Statistical Software</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://www.jstatsoft.org/index.php/jss/article/view/v088i07' target='_blank'>
CoClust: A Python Package for Co-Clustering
</a>
</span>
<div class='item-time'>
4 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/FjV3tq-0b3CpIwoo6e7Q0SOyQzgPvWNlD6xsQ0JVOK0FRoaifpI9GxPRZR2FA66r=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://hunch.net' target='_blank'>
Machine Learning (Theory)</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://hunch.net/?p=11377237' target='_blank'>
Code submission should be encouraged but not compulsory
</a>
</span>
<div class='item-time'>
2 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/KfOhaf0BkgQuxaz6OmtKkdMPZRS5wqyGaEaqCg0bmjAt7O2rCfYaYEcCp-BLMQ9wJlXznI9vUNoLY4FElPaEEQ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://mybiasedcoin.blogspot.com/' target='_blank'>
My Biased Coin</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://mybiasedcoin.blogspot.com/2019/01/analco-sosa-soda-post.html' target='_blank'>
ANALCO, SOSA, SODA post
</a>
</span>
<div class='item-time'>
3 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/QGeVyt5AkoZIpxWjBk_NL4HujYlryY5uNlkOWV3esZCj_zhLTAnX5EPH7F6g2omSs2kvidfzA25gO366=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://gowers.wordpress.com' target='_blank'>
Gowers's Weblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://gowers.wordpress.com/2018/12/22/how-craig-barton-wishes-hed-taught-maths/' target='_blank'>
How Craig Barton wishes he&#8217;d taught maths
</a>
</span>
<div class='item-time'>
4 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/vB_dUUOTiWKpwmQA3wQs0CbQvYmScWDWUELl0QmKNEBHiHgfmKIPefMnSE1S9DImirNZWWXy9VnQ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.scala-lang.org/news/' target='_blank'>
The Scala Programming Language</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://www.scala-lang.org/news/2018/12/20/programming-reactive-systems-course.html' target='_blank'>
New Course: &#8220;Programming Reactive Systems&#8221;
</a>
</span>
<div class='item-time'>
4 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/etR8ZpeKnVIJT2AedMQO5lqZAahaieyYw_JtjgTr--mRepSFK4iTaWba9AGRoqiNqdx7jW2-E2jCZ3ULwKw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://www.earningmyturns.org/' target='_blank'>
Earning My Turns</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://www.earningmyturns.org/2018/09/august-september-music.html' target='_blank'>
August-September music
</a>
</span>
<div class='item-time'>
7 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/FbIQW-FqR87X7HuQMennc2Ij_4FGVD44paEILlJTi6fQrteu6yFSXQ41CCjGHREZAMLQLzqE=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://math.andrej.com/' target='_blank'>
Mathematics and Computation</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://math.andrej.com/2018/08/25/how-to-implement-type-theory-in-an-hour/' target='_blank'>
How to implement type theory in an hour
</a>
</span>
<div class='item-time'>
8 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/SoLaIWMkhytJYtynE-8jWUTJKicd7Hih3dq7bSgDvrRsZmfOfTzqSVtUwICMMcpjmqpI-Y4N4uQNmU748YQLZ0Q=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.computervisionblog.com/' target='_blank'>
tombone's blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html' target='_blank'>
DeepFakes: AI-powered deception machines
</a>
</span>
<div class='item-time'>
11 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/yt8h1ZYHFMDQW_9PR6rNo1Dw2K3FUSn3fbGA6Fy5WOCLPRuxX055MHLXHwxADHzWnig0XH9lDwPdKaKkrA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://tcsmath.wordpress.com' target='_blank'>
tcs math - some mathematics of theoretical computer science</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://tcsmath.wordpress.com/2018/04/12/two-pages-of-the-book-stuck-together/' target='_blank'>
Two pages of the book, stuck together
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/noV6FJXy-GpUHYDaX4Jz08874ToASMix2ktY42-gnXHyTk7MAdICtwgaDvt-Z7zwTPOfnFwHaasPZGY=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://glinden.blogspot.com/' target='_blank'>
Geeking with Greg</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/GeekingWithGreg/~3/8hdwr6FYRNs/two-decades-of-amazoncom-recommendations.html' target='_blank'>
Two decades of Amazon.com recommendations
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/GpKKryW83zrC7eMxsACQryipwCxw37p_XUy-gHToXE8vtDcIeAYgSYOW6J6cF62aXSJsWursKdGRgjWZ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://logicomp.blogspot.com/' target='_blank'>
Logicomp</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://logicomp.blogspot.com/2017/06/correctness-by-design-vs-formal.html' target='_blank'>
Correctness by design vs. formal verification
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/-SCE5lhNjiVahN050Lb1_OtBsRZNaZAPXOXMvHwhVUauJzbLVJ_TwcE4lhH_aO0djhZDj6-f=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://xorshammer.com' target='_blank'>
XOR's Hammer</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://xorshammer.com/2017/05/29/how-is-it-even-possible-for-a-sailboat-to-sail-into-the-wind/' target='_blank'>
How is it even possible for a sailboat to sail into the wind?
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/GDBy96Tf59cU2vfXiwchvcA9KjhB6tN0w3DqzTp7ddHhBLCXWRwAEj0sW565UPv800n-pAF7TWti=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://michaelnielsen.org/blog' target='_blank'>
Michael Nielsen</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/michaelnielsen/wmna/~3/lKQMYJNDCbs/' target='_blank'>
Is there a tension between creativity and accuracy?
</a>
</span>
<div class='item-time'>
2 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/78EWePFP1AJYPtSm9a_hjfzYgAE8s2PjjF6scLE0c_AEI0epeXqlAWuI1HHxko8OG7E7YNfeqA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://blog.oddhead.com/' target='_blank'>
Oddhead Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://blog.oddhead.com/2015/10/25/algorithmic-economics-postdoc-msr-nyc/' target='_blank'>
Algorithmic economics postdoc position at Microsoft Research, NYC
</a>
</span>
<div class='item-time'>
3 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/Bldaa0ZH6pLuY6nJFpI0g2JBZuuqvE4iPnXgesmCMKKZH43KMjWJIuk6WsEruDwYjSHl8o8quz9A4Xnar6cLwLg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://andysresearch.blogspot.com/' target='_blank'>
Andy's Math/CS page</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://andysresearch.blogspot.com/2014/10/making-academic-contacts-some-thoughts.html' target='_blank'>
Making academic contacts (some thoughts for new researchers)
</a>
</span>
<div class='item-time'>
4 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/LZsZyd-3XVF0IO2rrd5Jpd_n1Z2W6neGxuoRv5w35le1VaGs9Lz_qyhVH4VTayrtpJsY3FAy3FGDmw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://statmt.blogspot.com/' target='_blank'>
The StatMT Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://statmt.blogspot.com/2014/09/easy-parallel-corpora-from-wikipedia.html' target='_blank'>
Easy parallel corpora from Wikipedia
</a>
</span>
<div class='item-time'>
4 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/qhUJAYiAXvnFUlSdEzeOuYoBnsake1NYn9ZwXSxs_7YdZw8JKc_9T1D6IsHMvnlVrR5Qj8RZmVteWxg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://vimsu99.blogspot.com/' target='_blank'>
Learning in Vision</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://vimsu99.blogspot.com/2014/03/dual-submissions-busted.html' target='_blank'>
Dual submissions -- busted!
</a>
</span>
<div class='item-time'>
5 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/GGKgv2BmcHGmvUAIfX7sUhK57Dc9d6KSQ8DVJMI71doXga1P7UuFrsFBK8DFUGaw4Uc8qWDIV3rRCvUhz_I=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://infoweekly.blogspot.com/' target='_blank'>
WebDiarios de Motocicleta</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://infoweekly.blogspot.com/2012/05/presburger-award.html' target='_blank'>
Presburger Award
</a>
</span>
<div class='item-time'>
6 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/NbftHuWwXwRm0DaBzWWdl-v1Mc9g0HRSI_iWCKDuPINT9Kk659MvF15X3YIXAuvyCqA7FbY3pFXT=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://lingpipe-blog.com' target='_blank'>
LingPipe Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://lingpipe-blog.com/2010/05/04/upgrading-java-classes-backward-compatible-serialization/' target='_blank'>
Upgrading Java Classes with Backward-Compatible Serialization
</a>
</span>
<div class='item-time'>
8 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/f6P41xXANB9H0q36WCoBVSpW9pOS_vy-kZuJBs9330tEelQCHLfWLAN9opWP6cnffVGwsf_Vacckj_rCbH0iTg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://qualgorithms.blogspot.com/' target='_blank'>
Quantum Algorithms</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://qualgorithms.blogspot.com/2008/12/polynomial-time-quantum-algorithm-for.html' target='_blank'>
Polynomial-time quantum algorithm for the simulation of chemical dynamics
</a>
</span>
<div class='item-time'>
10 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/JizXqqROCXx8pzJDKXwmbRKZYGxbxWSENCQR8s8xlGXYHeNXOBV6qBTtTRMQmwtpGf4wfrHfT67b=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.sixthform.info/maths' target='_blank'>
Mathematics Weblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://www.sixthform.info/maths/?p=166' target='_blank'>
A Levels
</a>
</span>
<div class='item-time'>
11 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/pOtzqKmyiuTgPPj0x7w-GnTQwM6XUNQ6ETzm22wWCKOJw2qJiO9GT_bhN2-WEgGr0CSovWmQZbzZu4Dou14s=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://structlearn.blogspot.com/' target='_blank'>
Structured Learning</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://structlearn.blogspot.com/2007/07/corrections-to-acl-anthology-urls.html' target='_blank'>
Corrections to ACL Anthology URLs
</a>
</span>
<div class='item-time'>
11 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/HrcFRkeXWL8DHGyWNKVC4n3rrz7VkVt2_fOnOcd6z7mNlFKx1kdF19QChXdXiOLDHFQ6uib2K8FUqp7rg0VFP1A=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://apperceptual.wordpress.com/feed/' target='_blank'>
Apperceptual</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/2XvQqtbpDV0xsPTDKoGYXAMTMSEsE97PvyMzhsNBBL47_FcjCOx0NBAve0qjidMoCmfhJAg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://sixthform.info/maths/b2rss2.php' target='_blank'>
Mathematics Weblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/ZPe0wqG0xJWmj6H4oD1pOfw7IPjgERounhx4FY8Wdn_QgFGkkAUwOWujgdqug_lS=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://mehve.org/ywml/atom.xml' target='_blank'>
yw's machine learning blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/cWKQwFJ3NTJZtcZQin2RkroItRhwJX0wR2-uEshtM7URWaTrgTwxk6bLQYp3YFoSqV4xmg0NoleX5lB2iBwEk2Tl=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://undirectedgrad.blogspot.com/feeds/posts/default' target='_blank'>
Undirected Grad</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/vJRjUacZ51faqmiZWjpVOmGzEJkwnGm0RzdSQvPRd3ZOWkraGAzzGClKvQ7htgW9PoQ0G6yPdY_5zXz6=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.datawrangling.com/feed/' target='_blank'>
Data Wrangling</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/c2zXSkDdDJAXffRUYfEemwqH78a-oGktig4Kiz_mxkTXvApTM_ZABOe-y9LjggcQlOgx2ZNhTq-Z6Od9-QQP=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://ergodicity.iamganesh.com/feed/atom/' target='_blank'>
Ganesh Swami</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/w4aA3hr7EXbhjjUyVhJtI2EIVGifPwOB9_fTxO8SsAR_z8raYW9iq4V_Uco_bmBhtqcRJGVPXz2QCfQroP8uKZO4K1PPnMkjqy0=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://www.lists.utah.edu/wws/rss/latest_d_read/mstatbiostat?count=20&for=10' target='_blank'>
mstatbiostat :</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/qXnoxci4mhuckJyGBIyTYsRQDNoiuL2KdUaWYog8x0kKXNl9wjzFgl9AArHka6tgC7-qOuBvkw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://groundtruth.info/AstroStat/slog/feed/' target='_blank'>
The AstroStat Slog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/UY6Ny1sQrfL6ayfx-wsyDsh4sekpUEWG8gO158YZrXDDlZ-hTux5J1byz3Plv_yQAGJwWZd97FrhnHQ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://feeds.feedburner.com/MainlyData' target='_blank'>
Mainly Data</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/m_j2k_iy9FaVdLNKRfRNdBs8OrO-qn6gqG571o0DnIoJyjG6egaEuApJ_3yhpPu2UWNe=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://conflate.net/inductio/feed/' target='_blank'>
Inductio Ex Machina</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/fifP-pSmqTcZADNbIY9nO3WHTJEyBTE6t9AsMgnbh5ZWHukAJzpJ7xLQtjLfljH-wh_ldKiFb28AzbBjwvNv=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://magic.aladdin.cs.cmu.edu/feed/atom/' target='_blank'>
[Lowerbounds, Upperbounds]</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/RML3Rwk2SwWmmZjNQVAk8eA5DWqj0rIh44aXVluOXhziCiULvrgajKJjTVbOWNXoNoGKv_S97chvpKttSqgaL5Z8ZIIlNAk_=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://clair.si.umich.edu:8080/wordpress/?feed=atom' target='_blank'>
Information Engineering</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/qXnoxci4mhuckJyGBIyTYsRQDNoiuL2KdUaWYog8x0kKXNl9wjzFgl9AArHka6tgC7-qOuBvkw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://groundtruth.info/AstroStat/slog/' target='_blank'>
http://groundtruth.info/AstroStat/slog/</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/7qHgz80_XsH_8TUY9dNHcdWkry8tQ26AsKIrGtDxAYNcS6Uxd8VEvKxh80KRhPIfryVq-qnvnFs0ZqAIUq9FgOCO=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://ciir.cs.umass.edu/~fdiaz/irblog/?feed=atom' target='_blank'>
Information Retrieval</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/voorwkyCRbH1oCYI0tgn66M0QcGgc1XopARmKFsoCAynNrJmoBEdBTQ-EWNnrgOr1P7vsESI=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://ba.stat.cmu.edu/forthcoming.xml' target='_blank'>
Bayesian Analysis Journal :: Forthcoming Articles</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
</ul>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=BlogList&widgetId=BlogList1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("BlogList1"));' rel='nofollow' target='configBlogList1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div>
</div><div class='widget BlogArchive' data-version='1' id='BlogArchive1'>
<h2>Blog Archive</h2>
<div class='widget-content'>
<div id='ArchiveList'>
<div id='BlogArchive1_ArchiveList'>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2018/'>
2018
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2018/07/'>
July
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2018/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/'>
2017
</a>
<span class='post-count' dir='ltr'>(10)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/08/'>
August
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/04/'>
April
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/03/'>
March
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/'>
2016
</a>
<span class='post-count' dir='ltr'>(17)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/12/'>
December
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/11/'>
November
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/08/'>
August
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/07/'>
July
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/06/'>
June
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/05/'>
May
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/03/'>
March
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/'>
2015
</a>
<span class='post-count' dir='ltr'>(7)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/12/'>
December
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/10/'>
October
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/'>
2014
</a>
<span class='post-count' dir='ltr'>(14)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/11/'>
November
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/10/'>
October
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/09/'>
September
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/07/'>
July
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/06/'>
June
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/05/'>
May
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/04/'>
April
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/'>
2013
</a>
<span class='post-count' dir='ltr'>(4)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/09/'>
September
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/07/'>
July
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/04/'>
April
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/'>
2012
</a>
<span class='post-count' dir='ltr'>(7)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/12/'>
December
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/02/'>
February
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/'>
2011
</a>
<span class='post-count' dir='ltr'>(16)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/12/'>
December
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/10/'>
October
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/07/'>
July
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/05/'>
May
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/04/'>
April
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/03/'>
March
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/02/'>
February
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/01/'>
January
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/'>
2010
</a>
<span class='post-count' dir='ltr'>(29)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/11/'>
November
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/10/'>
October
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/09/'>
September
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/08/'>
August
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/07/'>
July
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/06/'>
June
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/04/'>
April
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/02/'>
February
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/01/'>
January
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate expanded'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy toggle-open'>

        &#9660;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/'>
2009
</a>
<span class='post-count' dir='ltr'>(34)</span>
<ul class='hierarchy'>
<li class='archivedate expanded'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy toggle-open'>

        &#9660;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/12/'>
December
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/12/some-random-nips-thoughts.html'>Some random NIPS thoughts...</a></li>
<li><a href='https://nlpers.blogspot.com/2009/12/from-kivenenwarmuth-and-eg-to-cw.html'>From Kivenen/Warmuth and EG to CW learning and Ada...</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/11/'>
November
</a>
<span class='post-count' dir='ltr'>(3)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/11/k-means-vs-gmm-sum-product-vs-max.html'>K-means vs GMM, sum-product vs max-product</a></li>
<li><a href='https://nlpers.blogspot.com/2009/03/nlp-as-study-of-representations.html'>NLP as a study of representations</a></li>
<li><a href='https://nlpers.blogspot.com/2009/11/getting-started-in-bayesian-nlp.html'>Getting Started In: Bayesian NLP</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/10/'>
October
</a>
<span class='post-count' dir='ltr'>(1)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/10/convex-or-not-my-schizophrenic.html'>Convex or not, my schizophrenic personality</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/09/'>
September
</a>
<span class='post-count' dir='ltr'>(3)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/09/some-notes-on-job-search.html'>Some notes on job search</a></li>
<li><a href='https://nlpers.blogspot.com/2009/09/where-did-you-apply-to-grad-school.html'>Where did you Apply to Grad School?</a></li>
<li><a href='https://nlpers.blogspot.com/2009/09/acl-and-emnlp-retrospective-many-days.html'>ACL and EMNLP retrospective, many days late</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/08/'>
August
</a>
<span class='post-count' dir='ltr'>(3)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/08/classifier-performance-alternative.html'>Classifier performance: alternative metrics of suc...</a></li>
<li><a href='https://nlpers.blogspot.com/2009/08/acs-machine-translation-papers-at-emnlp.html'>ACS: Machine Translation Papers at EMNLP</a></li>
<li><a href='https://nlpers.blogspot.com/2009/08/destination-singapore.html'>Destination: Singapore</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/07/'>
July
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/07/non-parametric-as-memorizing-in-exactly.html'>Non-parametric as memorizing, in exactly the wrong...</a></li>
<li><a href='https://nlpers.blogspot.com/2009/07/small-changes-beget-good-or-bad.html'>Small changes beget good or bad examples?</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/06/'>
June
</a>
<span class='post-count' dir='ltr'>(8)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2009/06/icmlcoltuai-2009-retrospective.html'>ICML/COLT/UAI 2009 retrospective</a></li>
<li><a href='https://nlpers.blogspot.com/2009/06/should-there-be-shared-task-for-semi.html'>Should there be a shared task for semi-supervised ...</a></li>
<li><a href='https://nlpers.blogspot.com/2009/06/why-i-dont-buy-clustering-axioms.html'>Why I Don&#39;t Buy Clustering Axioms</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/05/'>
May
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/04/'>
April
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/03/'>
March
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/02/'>
February
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/01/'>
January
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/'>
2008
</a>
<span class='post-count' dir='ltr'>(37)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/12/'>
December
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/11/'>
November
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/08/'>
August
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/07/'>
July
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/06/'>
June
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/05/'>
May
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/04/'>
April
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/03/'>
March
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/02/'>
February
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/01/'>
January
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/'>
2007
</a>
<span class='post-count' dir='ltr'>(58)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/12/'>
December
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/11/'>
November
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/10/'>
October
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/09/'>
September
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/08/'>
August
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/07/'>
July
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/06/'>
June
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/05/'>
May
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/04/'>
April
</a>
<span class='post-count' dir='ltr'>(8)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/03/'>
March
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/02/'>
February
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/01/'>
January
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/'>
2006
</a>
<span class='post-count' dir='ltr'>(78)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/12/'>
December
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/11/'>
November
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/10/'>
October
</a>
<span class='post-count' dir='ltr'>(10)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/09/'>
September
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/08/'>
August
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/07/'>
July
</a>
<span class='post-count' dir='ltr'>(8)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/06/'>
June
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/05/'>
May
</a>
<span class='post-count' dir='ltr'>(11)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/04/'>
April
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/03/'>
March
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/02/'>
February
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/01/'>
January
</a>
<span class='post-count' dir='ltr'>(9)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2005/'>
2005
</a>
<span class='post-count' dir='ltr'>(6)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2005/12/'>
December
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=BlogArchive&widgetId=BlogArchive1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("BlogArchive1"));' rel='nofollow' target='configBlogArchive1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div></div>
</div>
<!-- spacer for skins that want sidebar and main to be the same height-->
<div class='clear'>&#160;</div>
</div>
<!-- end content-wrapper -->
<div id='footer-wrapper'>
<div class='footer section' id='footer'><div class='widget HTML' data-version='1' id='HTML1'>
<div class='widget-content'>
<script src="//s17.sitemeter.com/js/counter.js?site=s17nlers" type="text/javascript">
</script>
</div>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=HTML&widgetId=HTML1&action=editWidget&sectionId=footer' onclick='return _WidgetManager._PopupConfig(document.getElementById("HTML1"));' rel='nofollow' target='configHTML1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div></div>
</div>
</div></div>
<!-- end outer-wrapper -->
<script src='https://apis.google.com/js/plusone.js' type='text/javascript'></script>

<script type="text/javascript" src="https://www.blogger.com/static/v1/widgets/640298382-widgets.js"></script>
<script type='text/javascript'>
window['__wavt'] = 'AOuZoY5LJPji0K1W73uE3vxl99fzBXXfBw:1556334913298';_WidgetManager._Init('//www.blogger.com/rearrange?blogID\x3d19803222','//nlpers.blogspot.com/2009/','19803222');
_WidgetManager._SetDataContext([{'name': 'blog', 'data': {'blogId': '19803222', 'title': 'natural language processing blog', 'url': 'https://nlpers.blogspot.com/2009/', 'canonicalUrl': 'https://nlpers.blogspot.com/2009/', 'homepageUrl': 'https://nlpers.blogspot.com/', 'searchUrl': 'https://nlpers.blogspot.com/search', 'canonicalHomepageUrl': 'https://nlpers.blogspot.com/', 'blogspotFaviconUrl': 'https://nlpers.blogspot.com/favicon.ico', 'bloggerUrl': 'https://www.blogger.com', 'hasCustomDomain': false, 'httpsEnabled': true, 'enabledCommentProfileImages': true, 'gPlusViewType': 'FILTERED_POSTMOD', 'adultContent': false, 'analyticsAccountNumber': '', 'encoding': 'UTF-8', 'locale': 'en', 'localeUnderscoreDelimited': 'en', 'languageDirection': 'ltr', 'isPrivate': false, 'isMobile': false, 'isMobileRequest': false, 'mobileClass': '', 'isPrivateBlog': false, 'feedLinks': '\x3clink rel\x3d\x22alternate\x22 type\x3d\x22application/atom+xml\x22 title\x3d\x22natural language processing blog - Atom\x22 href\x3d\x22https://nlpers.blogspot.com/feeds/posts/default\x22 /\x3e\n\x3clink rel\x3d\x22alternate\x22 type\x3d\x22application/rss+xml\x22 title\x3d\x22natural language processing blog - RSS\x22 href\x3d\x22https://nlpers.blogspot.com/feeds/posts/default?alt\x3drss\x22 /\x3e\n\x3clink rel\x3d\x22service.post\x22 type\x3d\x22application/atom+xml\x22 title\x3d\x22natural language processing blog - Atom\x22 href\x3d\x22https://www.blogger.com/feeds/19803222/posts/default\x22 /\x3e\n', 'meTag': '', 'adsenseHostId': 'ca-host-pub-1556223355139109', 'adsenseHasAds': false, 'view': '', 'dynamicViewsCommentsSrc': '//www.blogblog.com/dynamicviews/4224c15c4e7c9321/js/comments.js', 'dynamicViewsScriptSrc': '//www.blogblog.com/dynamicviews/b1e9f650b2e5f9f3', 'plusOneApiSrc': 'https://apis.google.com/js/plusone.js', 'disableGComments': true, 'sharing': {'platforms': [{'name': 'Get link', 'key': 'link', 'shareMessage': 'Get link', 'target': ''}, {'name': 'Facebook', 'key': 'facebook', 'shareMessage': 'Share to Facebook', 'target': 'facebook'}, {'name': 'BlogThis!', 'key': 'blogThis', 'shareMessage': 'BlogThis!', 'target': 'blog'}, {'name': 'Twitter', 'key': 'twitter', 'shareMessage': 'Share to Twitter', 'target': 'twitter'}, {'name': 'Pinterest', 'key': 'pinterest', 'shareMessage': 'Share to Pinterest', 'target': 'pinterest'}, {'name': 'Email', 'key': 'email', 'shareMessage': 'Email', 'target': 'email'}], 'disableGooglePlus': true, 'googlePlusShareButtonWidth': 300, 'googlePlusBootstrap': '\x3cscript type\x3d\x22text/javascript\x22\x3ewindow.___gcfg \x3d {\x27lang\x27: \x27en\x27};\x3c/script\x3e'}, 'hasCustomJumpLinkMessage': false, 'jumpLinkMessage': 'Read more', 'pageType': 'archive', 'pageName': '2009', 'pageTitle': 'natural language processing blog: 2009'}}, {'name': 'features', 'data': {'sharing_get_link_dialog': 'true', 'sharing_native': 'false'}}, {'name': 'messages', 'data': {'edit': 'Edit', 'linkCopiedToClipboard': 'Link copied to clipboard!', 'ok': 'Ok', 'postLink': 'Post Link'}}, {'name': 'template', 'data': {'name': 'custom', 'localizedName': 'Custom', 'isResponsive': false, 'isAlternateRendering': false, 'isCustom': true}}, {'name': 'view', 'data': {'classic': {'name': 'classic', 'url': '?view\x3dclassic'}, 'flipcard': {'name': 'flipcard', 'url': '?view\x3dflipcard'}, 'magazine': {'name': 'magazine', 'url': '?view\x3dmagazine'}, 'mosaic': {'name': 'mosaic', 'url': '?view\x3dmosaic'}, 'sidebar': {'name': 'sidebar', 'url': '?view\x3dsidebar'}, 'snapshot': {'name': 'snapshot', 'url': '?view\x3dsnapshot'}, 'timeslide': {'name': 'timeslide', 'url': '?view\x3dtimeslide'}, 'isMobile': false, 'title': 'natural language processing blog', 'description': 'my biased thoughts on the fields of natural language processing (NLP), computational linguistics (CL) and related topics (machine learning, math, funding, etc.)', 'url': 'https://nlpers.blogspot.com/2009/', 'type': 'feed', 'isSingleItem': false, 'isMultipleItems': true, 'isError': false, 'isPage': false, 'isPost': false, 'isHomepage': false, 'isArchive': true, 'isLabelSearch': false, 'archive': {'year': 2009, 'rangeMessage': 'Showing posts from 2009'}}}]);
_WidgetManager._RegisterWidget('_NavbarView', new _WidgetInfo('Navbar1', 'navbar', document.getElementById('Navbar1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HeaderView', new _WidgetInfo('Header1', 'header', document.getElementById('Header1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogView', new _WidgetInfo('Blog1', 'main', document.getElementById('Blog1'), {'cmtInteractionsEnabled': false, 'lightboxEnabled': true, 'lightboxModuleUrl': 'https://www.blogger.com/static/v1/jsbin/1977841774-lbx.js', 'lightboxCssUrl': 'https://www.blogger.com/static/v1/v-css/368954415-lightbox_bundle.css'}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_ProfileView', new _WidgetInfo('Profile1', 'sidebar', document.getElementById('Profile1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_LabelView', new _WidgetInfo('Label1', 'sidebar', document.getElementById('Label1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogListView', new _WidgetInfo('BlogList1', 'sidebar', document.getElementById('BlogList1'), {'numItemsToShow': 0, 'totalItems': 47}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogArchiveView', new _WidgetInfo('BlogArchive1', 'sidebar', document.getElementById('BlogArchive1'), {'languageDirection': 'ltr', 'loadingMessage': 'Loading\x26hellip;'}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HTMLView', new _WidgetInfo('HTML1', 'footer', document.getElementById('HTML1'), {}, 'displayModeFull'));
</script>
</body>
</html>
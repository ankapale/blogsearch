<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xmlns:b='http://www.google.com/2005/gml/b' xmlns:data='http://www.google.com/2005/gml/data' xmlns:expr='http://www.google.com/2005/gml/expr'>
<head>
<link href='https://www.blogger.com/static/v1/widgets/2549344219-widget_css_bundle.css' rel='stylesheet' type='text/css'/>
<link href='https://www.brid.gy/webmention/blogger' rel='webmention'/>
<meta content='text/html; charset=UTF-8' http-equiv='Content-Type'/>
<meta content='blogger' name='generator'/>
<link href='https://nlpers.blogspot.com/favicon.ico' rel='icon' type='image/x-icon'/>
<link href='https://nlpers.blogspot.com/2010/' rel='canonical'/>
<link rel="alternate" type="application/atom+xml" title="natural language processing blog - Atom" href="https://nlpers.blogspot.com/feeds/posts/default" />
<link rel="alternate" type="application/rss+xml" title="natural language processing blog - RSS" href="https://nlpers.blogspot.com/feeds/posts/default?alt=rss" />
<link rel="service.post" type="application/atom+xml" title="natural language processing blog - Atom" href="https://www.blogger.com/feeds/19803222/posts/default" />
<!--Can't find substitution for tag [blog.ieCssRetrofitLinks]-->
<meta content='https://nlpers.blogspot.com/2010/' property='og:url'/>
<meta content='natural language processing blog' property='og:title'/>
<meta content='my biased thoughts on the fields of natural language processing (NLP), computational linguistics (CL) and related topics (machine learning, math, funding, etc.)' property='og:description'/>
<!--[if IE]> <script> (function() { var html5 = ("abbr,article,aside,audio,canvas,datalist,details," + "figure,footer,header,hgroup,mark,menu,meter,nav,output," + "progress,section,time,video").split(','); for (var i = 0; i < html5.length; i++) { document.createElement(html5[i]); } try { document.execCommand('BackgroundImageCache', false, true); } catch(e) {} })(); </script> <![endif]-->
<title>natural language processing blog: 2010</title>
<style id='page-skin-1' type='text/css'><!--
/*
* -----------------------------------------------------
* Blogger Template Style
* Name:     Snapshot: Madder
* Designer: Dave Shea
* URL:      mezzoblue.com / brightcreative.com
* Date:     27 Feb 2004
* Updated by: Blogger Team
* ------------------------------------------------------ */
/* Variable definitions
====================
<Variable name="textcolor" description="Text Color"
type="color" default="#474B4E">
<Variable name="pagetitlecolor" description="Blog Title Color"
type="color" default="#7B8186">
<Variable name="titlecolor" description="Post Title Color"
type="color" default="#C4663B">
<Variable name="footercolor" description="Post Footer Color"
type="color" default="#B4BABE">
<Variable name="sidebarcolor" description="Sidebar Title Color"
type="color" default="#7B8186">
<Variable name="linkcolor" description="Link Color"
type="color" default="#DD6599">
<Variable name="visitedlinkcolor" description="Visited Link Color"
type="color" default="#D6A0B6">
<Variable name="bodyfont" description="Text Font"
type="font"
default="normal normal 100% Helvetica, Arial, sans-serif">
*/
/* -- basic html elements -- */
body {
padding: 0;
margin: 0;
font-size: small;
color: #474B4E;
background: #fff;
text-align: center;
}
a {
color: #DD6599;
text-decoration: none;
}
a:visited {
color: #D6A0B6;
}
a:hover {
text-decoration: underline;
color: #FD0570;
}
h1 {
margin: 0;
color: #7B8186;
font-size: 1.5em;
text-transform: lowercase;
}
h1 a, h1 a:link, h1 a:visited {
color: #7B8186;
}
h2, #comments h4 {
font-size: 1em;
margin: 2em 0 0 0;
color: #7B8186;
background: transparent url(//www.blogblog.com/snapshot/bg-header1.gif) bottom right no-repeat;
padding-bottom: 2px;
}
h3 {
font-size: 1em;
margin: 2em 0 0 0;
background: transparent url(//www.blogblog.com/snapshot/bg-header1.gif) bottom right no-repeat;
padding-bottom: 2px;
}
h4, h5 {
font-size: 0.9em;
text-transform: lowercase;
letter-spacing: 2px;
}
h5 {
color: #7B8186;
}
h6 {
font-size: 0.8em;
text-transform: uppercase;
letter-spacing: 2px;
}
p {
margin: 0 0 1em 0;
}
img, form {
border: 0; margin: 0;
}
/* -- layout -- */
#outer-wrapper {
width: 800px;
margin: 0 auto;
text-align: left;
font: normal normal 100% Helvetica, Arial, sans-serif;
background: #fff url(//www.blogblog.com/snapshot/bg-body.gif) 0 0 repeat-y;
}
#header-wrapper {
background: #D8DADC url(//www.blogblog.com/snapshot/bg-headerdiv.gif) 0 0 repeat-y;
}
.descriptionwrapper {
background: #fff url(//www.blogblog.com/snapshot/bg-sidebar.gif) 1px 0 no-repeat;
float: right;
width: 214px;
padding: 0 0 0 8px;
margin: 1px 0 2px 0;
}
.description {
border: 1px solid #F3B89D;
background: #FFD1BC url(//www.blogblog.com/snapshot/bg-profile.gif);
padding: 10px 0 10px 7px;
margin: 4px 0 0 -6px;
color: #C4663B;
}
#header {
background: transparent url(//www.blogblog.com/snapshot/header-01.gif) bottom left no-repeat;
}
#main-wrapper {
line-height: 1.4;
float: left;
padding: 10px 12px;
border-top: solid 1px #fff;
width: 578px;
word-wrap: break-word; /* fix for long text breaking sidebar float in IE */
overflow: hidden;     /* fix for long non-text content breaking IE sidebar float */
/* Tantek hack - http://www.tantek.com/CSS/Examples/boxmodelhack.html */
voice-family: "\"}\"";
voice-family: inherit;
width: 554px;
}
/* IE5 hack */
#main {}
#sidebar {
float:right;
border-top: solid 1px #fff;
padding: 4px 0 0 7px;
background: #fff;
width: 214px;
word-wrap: break-word; /* fix for long text breaking sidebar float in IE */
overflow: hidden;     /* fix for long non-text content breaking IE sidebar float */
}
#footer {
clear: both;
background: #E9EAEB url(//www.blogblog.com/snapshot/bg-footer.gif) bottom left no-repeat;
border-top: solid 1px #fff;
min-height: 15px;
}
/* -- header style -- */
#header h1 {
padding: 12px 0 92px 4px;
width: 707px;
line-height: 1;
}
/* -- content area style -- */
#main {
line-height: 1.4;
}
.post h3 {
font-size: 1.2em;
margin-bottom: 0;
color: #C4663B;
}
.post h3 a,
.post h3 a:visited {
color: #C4663B;
}
.post {
clear: both;
margin-bottom: 4em;
}
.post-footer .post-author,
.post-footer .post-timestamp {
color: #B4BABE;
}
.uncustomized-post-template .post-author,
.uncustomized-post-template .post-timestamp {
float: left;
margin-right: 4px;
}
.uncustomized-post-template .post-footer .comment-link {
float: right;
margin-left: 4px;
}
.post img {
border: 1px solid #E3E4E4;
padding: 2px;
background: #fff;
}
.deleted-comment {
font-style:italic;
color:gray;
}
.feed-links {
clear: both;
line-height: 2.5em;
}
#blog-pager-newer-link {
float: left;
}
#blog-pager-older-link {
float: right;
}
#blog-pager {
text-align: center;
}
.comment-footer {
margin-bottom: 10px;
}
/* -- sidebar style -- */
.sidebar .widget {
margin: 1.3em 0 0.5em 0;
}
.sidebar h2 {
font-size: 1.3em;
}
.sidebar dl {
margin: 0 0 10px 0;
}
.sidebar ul {
list-style: none;
margin: 0;
padding: 0;
}
.sidebar li {
padding-bottom: 5px;
line-height: 1
}
.main .widget .clear {
clear: both;
}
/* -- sidebar style -- */
#footer p {
margin: 0;
padding: 12px 8px;
font-size: 0.9em;
}
.profile-textblock {
margin-left: 0;
clear: both;
}
.profile-img {
float: left;
margin: 0 10px 5px 0;
border: 1px solid #7C78B5;
padding: 4px;
}
/** Page structure tweaks for layout editor wireframe */
body#layout #header-wrapper {
margin-top: 0;
}
body#layout #main-wrapper {
padding:0;
}
img.latex_eq {
padding: 0;
margin: 0;
border: 0;
}

--></style>
<link href='https://www.blogger.com/dyn-css/authorization.css?targetBlogID=19803222&amp;zx=ff9afc1a-535d-4665-a907-b0b1f5ca0ff8' media='none' onload='if(media!=&#39;all&#39;)media=&#39;all&#39;' rel='stylesheet'/><noscript><link href='https://www.blogger.com/dyn-css/authorization.css?targetBlogID=19803222&amp;zx=ff9afc1a-535d-4665-a907-b0b1f5ca0ff8' rel='stylesheet'/></noscript>

</head>
<body>
<div class='navbar section' id='navbar'><div class='widget Navbar' data-version='1' id='Navbar1'><script type="text/javascript">
    function setAttributeOnload(object, attribute, val) {
      if(window.addEventListener) {
        window.addEventListener('load',
          function(){ object[attribute] = val; }, false);
      } else {
        window.attachEvent('onload', function(){ object[attribute] = val; });
      }
    }
  </script>
<div id="navbar-iframe-container"></div>
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
<script type="text/javascript">
      gapi.load("gapi.iframes:gapi.iframes.style.bubble", function() {
        if (gapi.iframes && gapi.iframes.getContext) {
          gapi.iframes.getContext().openChild({
              url: 'https://www.blogger.com/navbar.g?targetBlogID\x3d19803222\x26blogName\x3dnatural+language+processing+blog\x26publishMode\x3dPUBLISH_MODE_BLOGSPOT\x26navbarType\x3dLIGHT\x26layoutType\x3dLAYOUTS\x26searchRoot\x3dhttps://nlpers.blogspot.com/search\x26blogLocale\x3den\x26v\x3d2\x26homepageUrl\x3dhttps://nlpers.blogspot.com/\x26vt\x3d-3933451413689154563',
              where: document.getElementById("navbar-iframe-container"),
              id: "navbar-iframe"
          });
        }
      });
    </script><script type="text/javascript">
(function() {
var script = document.createElement('script');
script.type = 'text/javascript';
script.src = '//pagead2.googlesyndication.com/pagead/js/google_top_exp.js';
var head = document.getElementsByTagName('head')[0];
if (head) {
head.appendChild(script);
}})();
</script>
</div></div>
<div id='outer-wrapper'><div id='wrap2'>
<!-- skip links for text browsers -->
<span id='skiplinks' style='display:none;'>
<a href='#main'>skip to main </a> |
      <a href='#sidebar'>skip to sidebar</a>
</span>
<div id='header-wrapper'>
<div class='header section' id='header'><div class='widget Header' data-version='1' id='Header1'>
<div id='header-inner'>
<div class='titlewrapper'>
<h1 class='title'>
<a href='https://nlpers.blogspot.com/'>
natural language processing blog
</a>
</h1>
</div>
<div class='descriptionwrapper'>
<p class='description'><span>my biased thoughts on the fields of natural language processing (NLP), computational linguistics (CL) and related topics (machine learning, math, funding, etc.)</span></p>
</div>
</div>
</div></div>
</div>
<div id='content-wrapper'>
<div id='main-wrapper'>
<div class='main section' id='main'><div class='widget Blog' data-version='1' id='Blog1'>
<div class='blog-posts hfeed'>

          <div class="date-outer">
        
<h2 class='date-header'><span>18 November 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='7142612846170713721'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/11/crowdsourcing-workshop-tutorial.html'>Crowdsourcing workshop (/tutorial) decisions</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>Everyone at conferences (with multiple tracks) always complains that there are time slots with nothing interesting, and other time slots with too many interesting papers.&nbsp; People have suggested crowdsourcing this, enabling parcipants to say -- well ahead of the conference -- which papers they'd go to... then let an algorithm schedule.<br />
<br />
I think there are various issues with this model, but don't want to talk about it.&nbsp; What I do want to talk about is <b>applying the same ideas to workshop acceptance decisions.</b>&nbsp; This comes up because I'm one of the two workshop chairs for ACL this year, and because John Langford just pointed to the ICML call for tutorials.&nbsp; (I think what I have to say applies equally to tutorials as to workshops.)<br />
<br />
I feel like a workshop (or tutorial) is <i>successful</i> if it is well attended.&nbsp; This applies both from a monetary perspective, as well as a scientific perspective.&nbsp; (Note, though, that I think that small workshops can also be successful, especially if they are either fostering a small community, bring people in, or serving other purposes.&nbsp; That is to say, size is not <i>all </i>that matters.&nbsp; But it is a big part of what matters.)<br />
<br />
We have 30-odd workshop proposals for three of us to sort through (John Carroll and I are the two workshop chairs for ACL, and Marie Candito is the workshop chair for EMNLP; workshops are being reviewed jointly -- which actually makes the allocation process <i>more</i> difficult).&nbsp; The idea would be that I could create a poll, like the following:<br />
<ol><li>Are you going to ACL?&nbsp; Yes, maybe, no</li>
<li>Are you going to EMNLP?&nbsp; Yes, maybe, no</li>
<li>If workshop A were offered at a conference you were going to, would you go to workshop A?</li>
<li>If workshop B...</li>
<li>And so on</li>
</ol>This gives you two forms of information.&nbsp; First it can help estimate expected attendance (though we ask proposers to estimate that, too, and I think they do a reasonable job if you skew their estimates down by about 10%).&nbsp; But more importantly, <i>it gives correlations between workshops</i>.&nbsp; This lets you be sure that you're not scheduling things on top of each other that people might want to go to.&nbsp; Some of these are obvious (for instance, if we got 10 MT workshop proposals... which didn't actually happen but is moderately conceivable :P), but some are not.&nbsp; For instance, maybe people who care about annotation also care about ML, but maybe not?&nbsp; I actually have no idea.<br />
<br />
Of course we're not going to do this this year.&nbsp; It's too late already, and it would be unfair to publicise all the proposals, given that we didn't tell proposers in advance that we would do so.&nbsp; And of course <b>I don't think this should exclusively be a popularity contest</b>.&nbsp; But I do beleive that <b>popularity should be a factor.</b>&nbsp; And it should probably be a reasonably big factor.&nbsp; Workshop chairs could then use the output of an optimization algorithm as a starting point, and use this as additional data for making decisions.&nbsp; Especially since two or three people are being asked to make decisions that cover--essentially--all areas of NLP, this actually seems like a good idea to me.<br />
<br />
I actually think something like this is more likely to actually happen at a conference like ICML than ACL, since ICML seems (much?) more willing to try new things than ACL (for better or for worse).<br />
<br />
But I do think it would be interesting to try to see what sort of response you get.&nbsp; Of course, just polling on this blog wouldn't be sufficient: you'd want to spam, perhaps all of last year's attendees.&nbsp; But this isn't particularly difficult.<br />
<br />
Is there anything I'm not thinking of that would make this obviously not work?&nbsp; I could imagine someone saying that maybe people won't propose workshops/tutorials if the proposals will be made public?&nbsp; I find that a bit hard to swallow.&nbsp; Perhaps there's a small embarassment factor if you're public and then don't get accepted.&nbsp; But I <b>wouldn't advocate making the voting results public</b> -- they would be private to the organizers / workshop chairs.<br />
<br />
I guess -- I feel like I'm channeling Fernando here? -- that another possible issue is that <b>you might not be able to decide which workshops you'd go to without seeing what papers are there and who is presenting</b>.&nbsp; This is probably true.&nbsp; But this is also the same problem that the workshop chairs face anyway: we have to guess that good enough papers/people will be there to make it worthwhile.&nbsp; I doubt I'm any better at guessing this than any other random NLP person...<br />
<br />
So what am I forgetting?</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/11/crowdsourcing-workshop-tutorial.html' title='permanent link'>11/18/2010 03:52:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=7142612846170713721' onclick=''>8
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/11/crowdsourcing-workshop-tutorial.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=7142612846170713721' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=7142612846170713721&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>09 November 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='8682169235051598883'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/11/managing-group-papers.html'>Managing group papers</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>Every time a major conference deadline (ACL, NIPS, EMNLP, ICML, etc...) comes around, we usually have a slew of papers (&gt;=3, typically) that are getting prepared.&nbsp; I would say on average 1 doesn't make it, but that's par for the course.<br />
<br />
For AI-Stats, whose deadline just passed, I circulated student paper drafts to all of my folks to solicit comments at any level that they desired.&nbsp; Anywhere from not understanding the problem/motivation to typos or errors in equations.&nbsp; My experience was that it was useful, both from the perspective of distributing some of my workload and getting an alternative perspective, to keeping everyone abreast of what everyone else is working on.<br />
<br />
In fact, it was so successful that two students suggested to me that I <i>require</i> more-or-less complete drafts of papers at least one week in advance so that this can take place.&nbsp; How you require something like this is another issue, but the suggestion they came up with was that I'll only cover conference travel if this occurs.&nbsp; It's actually not a bad idea, but I don't know if I'm enough of a hard-ass (or perceived as enough of a hard-ass) to really pull it off.&nbsp; Maybe I'll try it though.<br />
<br />
The bigger question is how to manage such a thing.&nbsp; I was thinking of installing some conference management software locally (eg., <a href="http://www.cs.ucla.edu/%7Ekohler/hotcrp/">HotCRP</a>, which I really like) and giving students "reviewer" access.&nbsp; Then, they could upload their drafts, perhaps with an email circulated when a new draft is available, and other students (and me!) could "review" them.&nbsp; (Again, perhaps with an email circulated -- I'm a big fan of "push" technology: I don't have time to "pull" anymore!)<br />
<br />
The only concern I have is that it would be really nice to be able to track updates, or to have the ability for authors to "check off" things that reviewers suggested.&nbsp; Or to allow discussion.&nbsp; Or something like that.<br />
<br />
I'm curious if anyone has ever tried anything like this and whether it was successful or not.&nbsp; It seems like if you can get a culture of this established, it could actually be quite useful.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/11/managing-group-papers.html' title='permanent link'>11/09/2010 07:41:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=8682169235051598883' onclick=''>13
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/11/managing-group-papers.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=8682169235051598883' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=8682169235051598883&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/advising' rel='tag'>advising</a>,
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>21 October 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='6296520072559484676'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/10/comparing-bounds.html'>Comparing Bounds</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>This is something that's bothered me for quite a while, and I don't know of a good answer.&nbsp; I used to think it was something that theory people didn't worry about, but then this exact issue was brought up by a reviewer of a theory-heavy paper that we have at NIPS this year (with Avishek Saha and <a href="http://www.umiacs.umd.edu/%7Eabhishek/">Abhishek Kumar</a>). There are (at least?) two issues with comparing bounds, the first is the obvious "these are both upper bounds, what does it mean to compare them?"&nbsp; The second is the slightly less obvious "but your empirical losses may be totally different" issue.&nbsp; It's actually the <i>second</i> one that I want to talk about, but I have much less of a good internal feel about it.<br />
<br />
Let's say that I'm considering two learning approaches.&nbsp; Say it's SVMs versus logistic regression.&nbsp; Both regularized.&nbsp; Or something.&nbsp; Doesn't really matter.&nbsp; At the end of the day, I'll have a bound that looks roughly like:<br />
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; expected test error &lt;= empirical training error + f( complexity / N)<br />
<br />
Here, f is often "sqrt", but could really be any function.&nbsp; And N is the number of data points.<br />
<br />
Between two algorithms, both "f" and "complexity" can vary.&nbsp; For instance, one might have a linear dependence on the dimensionality of the data (i.e., complexity looks like O(D), where D is dimensionality) and the other might have a superlinear dependence (eg., O(D log D)).&nbsp; Or one might have a square root.&nbsp; Who knows.&nbsp; Sometimes there's an inf or sup hiding in there, too, for instance in a lot of the margin bounds.<br />
<br />
At the end of the day, we of course want to say "my algorithm is better than your algorithm."&nbsp; (What else is there in life?)&nbsp; The standard way to say this is that "my f(complexity / N) looks better than your f'(complexity' / N)."<br />
<br />
Here's where two issues crop up.&nbsp; The first is that our bound is just an upper bound.&nbsp; For instance, Alice could come up to me and say "I'm thinking of a number between 1 and 10" and Bob could say "I'm thinking of a number between 1 and 100."&nbsp; Even though the bound is lower for Alice, it doesn't mean that Alice is actually thinking of a smaller number -- maybe Alice is thinking of 9 and Bob of 5.&nbsp; In this way, the bounds can be misleading.<br />
<br />
My general approach with this issue is to squint, as I do for experimental results.&nbsp; I don't actually care about constant factors: I just care about things like "what does the dependence on D look like."&nbsp; Since D is usually huge for problems I care about, a linear or sublinear dependence on D looks really good to me.&nbsp; Beyond that I don't really care.&nbsp; I especially don't care if the proof techniques are quite similar.&nbsp; For instance, if they both use Rademacher complexities, then I'm more willing to compare them than if one uses Rademacher complexities and the other uses covering numbers.&nbsp; They somehow feel more comparable: I'm less likely to believe that the differences are due to the method of analysis.<br />
<br />
(You can also get around this issue with some techniques, like Rademacher complexities, which give you both upper and lower bounds, but I don't think anyone really does that...)<br />
<br />
The other issue I don't have as good a feeling for.&nbsp; The issue is that we're entirely ignoring the "empirical training error" question.&nbsp; In fact, this is often measured differently between different algorithms!&nbsp; For instance, for SVMs, the formal statement is more like "expected <i>0/1 loss</i> on test &lt;= empirical <i>hinge loss</i> on training + ..."&nbsp; Whereas for logistic regression, you might be comparing expected 0/1 loss with empirical <i>log loss.</i><br />
<br />
Now I really don't know what to do.<br />
<br />
We ran into this issue because we were trying to compare some bounds between EasyAdapt and a simple model trained just on source data.&nbsp; The problem is that the source training error might be totally incomparable to the (source + target) training error.<br />
<br />
But the issue is for sure more general.&nbsp; For instance, what if your training error is measured in squared error?&nbsp; Now this can be <i>huge</i> when hinge loss is still rather small.&nbsp; In fact, your squared error could be quadratically large in your hinge loss.&nbsp; Actually it could be arbitrarily larger, since hinge goes to zero for any sufficiently correct classification, but squared error does not.&nbsp; (Neither does log loss.)<br />
<br />
This worries me greatly, much more than the issue of comparing upper bounds.<br />
<br />
Does this bother everyone, or is it just me?&nbsp; Is there a good way to think about this that gets your out of this conundrum?</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/10/comparing-bounds.html' title='permanent link'>10/21/2010 10:13:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=6296520072559484676' onclick=''>3
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/10/comparing-bounds.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=6296520072559484676' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=6296520072559484676&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>,
<a href='https://nlpers.blogspot.com/search/label/theory' rel='tag'>theory</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>05 October 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4790628870864598174'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/10/my-giant-reviewing-error.html'>My Giant Reviewing Error</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I try to be a good reviewer, but like everything, reviewing is a learning process.&nbsp; About five years ago, I was reviewing a journal paper and made an error.&nbsp; I don't want to give up anonymity in this post, so I'm going to be vague in places that don't matter.<br />
<br />
I was reviewing a paper, which I thought was overall pretty strong.&nbsp; I thought there was an interesting connection to some paper from Alice Smith (not the author's real name) in the past few years and mentioned this in my review.&nbsp; Not a connection that made the current paper irrelevant, but something the authors should probably talk about.&nbsp; In the revision response, the authors said that they had looked to try to find Smith's paper, but could figure out which one I was talking about, and asked for a pointer.&nbsp; I spend the next five hours looking for the reference and couldn't find it myself.&nbsp; It turns out that actually I was thinking of a paper by Bob Jones, so I provided that citation.&nbsp; But the Jones paper wasn't even as relevant as it seemed at the time I wrote the review, so I apologized and told the authors they didn't really need to cover it that closely.<br />
<br />
Now, you might be thinking to yourself: aha, now I know that Hal was the reviewer of my paper!&nbsp; I remember that happening to me!<br />
<br />
But, sadly, this is not true.&nbsp; <b>I get reviews like this all the time, and I feel it's one of the most irresponsible things reviewers can do.</b>&nbsp; In fact, I don't think a single reviewing cycle has passed where I don't get a review like this.&nbsp; The problem with such reviews is that it enables a reviewer to make whatever claim they want, without any expectation that they have to back it up.&nbsp; And the claims are usually wrong.&nbsp; They're not necessarily being mean (I wasn't trying to be mean), but sometimes they are.<br />
<br />
Here are some of the most ridiculous cases I've seen.&nbsp; I mention these just to show how often this problem occurs.&nbsp; These are all on papers of mine.<br />
<ul><li>One reviewer wrote "This idea is so obvious this must have been done before."&nbsp; This is probably the most humorous example I've seen, but the reviewer was clearly serious.&nbsp; And no, this was <i>not</i> in a review for one of the the "frustratingly easy" papers.<br />
</li>
<li>In a NSF grant review for an educational proposal, we were informed by 4 of 7 reviewers (who each wrote about a paragraph) that our ideas had been done in SIGCSE several times.&nbsp; Before submitting, we had skimmed/read the past 8 years of SIGCSE and could find nothing.&nbsp; (Maybe it's true and we just were looking in the wrong place, but that still isn't helpful.)&nbsp; It turned out to strongly seem that this was basically their way of saying "you are not one of us."<br />
</li>
<li>In a paper on technique X for task A, we were told hands down that it's well known that technique Y works better, with no citations.&nbsp; The paper was rejected, we went and implemented Y, and found that it worked worse on task A.&nbsp; We later found one paper saying that Y works better than X on task B, for B fairly different from A.<br />
</li>
<li>In another paper, we were told that what we were doing had been done before and in this case a citation was provided.&nbsp; The citation was to one of our own papers, and it was quite different by any reasonable metric.&nbsp; At least a citation was provided, but it was clear that the reviewer hadn't bothered reading it.<br />
</li>
<li>We were told that we missed an enormous amount of related work that could be found by a simple web search.&nbsp; I've written such things in reviews, often saying something like "search for 'non-parametric Bayesian'" or something like that.&nbsp; But here, no keywords were provided.&nbsp; It's entirely possible (especially when someone moves into a new domain) that you can miss a large body of related work because you don't know how to find in: that's fine -- just tell me how to find it if you don't want to actually provide citations.</li>
</ul>There are other examples I could cite from my own experience, but I think you get the idea.<br />
<br />
I'm posting this not to gripe (though it's always fun to gripe about reviewing), but to try to draw attention to this problem.&nbsp; It's really just an issue of laziness.&nbsp; If I had bothered trying to look up a reference for Alice Smith's paper, I would have immediately realized I was wrong.&nbsp; But I was lazy.&nbsp; Luckily this didn't really adversely affect the outcome of the acceptance of this paper (journals are useful in that way -- authors can push back -- and yes, I know you can do this in author responses too, but you really need two rounds to make it work in this case).<br />
<br />
I've really really tried ever since my experience above to not ever do this again.&nbsp; And I would encourage future reviewers to try to avoid the temptation to do this: you may find your memory isn't as good as you think.&nbsp; I would also encourage area chairs and co-reviewers to push their colleagues to actually provide citations for otherwise unsubstantiated claims.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/10/my-giant-reviewing-error.html' title='permanent link'>10/05/2010 11:14:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4790628870864598174' onclick=''>9
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/10/my-giant-reviewing-error.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4790628870864598174' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4790628870864598174&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/reviewing' rel='tag'>reviewing</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>24 September 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4584126750147794314'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/09/acl-icml-symposium.html'>ACL / ICML Symposium?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>ACL 2011 ends on June 24, in Portland (that's a Friday).  ICML 2011 begins on June 28, near Seattle (the following Tuesday).  This is pretty much as close to a co-location as we're probably going to get in a long time.  A few folks have been discussing the possibility of having a joint NLP/ML symposium in between.  The current thought is to have it on June 27 at the ICML venue (for various logistical reasons).&nbsp; There are buses and trains that run between the two cities, and we might even be able to charter some buses. <br />
<br />
One worry is that it might <i>only</i> attract ICML folks due to the weekend between the end of ACL and the beginning of said symposium.  As a NLPer/MLer, I believe in data.  So please provide data by filling out the form below and, if you wish, adding comments.<br />
<br />
If you woudn't attend any, you don't need to fill out the poll :).<br />
<br />
The last option is there if you want to tell me "I'm going to go to ACL, and I'd really like to go to the symposium, but the change in venue and the intervening weekend is too problematic to make it possible."<br />
<br />
<form action="http://poll.pollcode.com/2w9s" method="post"><table bgcolor="#eeeeee" border="0" cellpadding="2" cellspacing="0" style="width: 400px;"><tbody>
<tr><td colspan="2"><span style="color: black; font-family: Verdana;"><b>Which would you attend?</b></span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="1" /></td><td><span style="color: black; font-family: Verdana;">Only ACL</span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="2" /></td><td><span style="color: black; font-family: Verdana;">Only ICML</span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="3" /></td><td><span style="color: black; font-family: Verdana;">ACL and the symposium</span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="4" /></td><td><span style="color: black; font-family: Verdana;">ICML and the symposium</span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="5" /></td><td><span style="color: black; font-family: Verdana;">Only ACL and ICML</span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="6" /></td><td><span style="color: black; font-family: Verdana;">All three</span></td></tr>
<tr><td width="5"><input name="answer" type="radio" value="7" /></td><td><span style="color: black; font-family: Verdana;">Only ACL, because of convenience</span></td></tr>
<tr><td colspan="2"><br />
<br />
<br />
<center><input type="submit" value="Vote" />&nbsp;&nbsp;<input name="view" type="submit" value="View" /></center></td></tr>
<tr><td align="right" bgcolor="white" colspan="2"><span style="color: black; font-family: Verdana;">pollcode.com <a href="http://pollcode.com/"><span style="color: navy;">free polls</span></a><span style="color: navy;"> </span></span></td></tr>
</tbody></table></form></p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/09/acl-icml-symposium.html' title='permanent link'>9/24/2010 07:25:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4584126750147794314' onclick=''>1 comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/09/acl-icml-symposium.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4584126750147794314' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4584126750147794314&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/survey' rel='tag'>survey</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>15 September 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='5842730110981448091'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/09/very-sad-news.html'>Very sad news....</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I heard earlier this morning that <a href="http://www.clsp.jhu.edu/%7Ejelinek/">Fred Jelinek</a> passed away last night.&nbsp; Apparently he had been working during the day: a tenacious aspect of Fred that probably has a lot to do with his many successes.<br />
<br />
Fred is probably most infamous for the famous "Every time I fire a linguist the performace of the recognizer improves" quote, which Jurafsky+Martin's textbook says is actually supposed to be the more innocuous "Anytime a linguist leaves the group the recognition rate goes up."&nbsp; And in Fred's 2009 <a href="http://www.mitpressjournals.org/doi/abs/10.1162/coli.2009.35.4.35401">ACL Lifetime Achievement Award speech</a>, he basically said that such a thing never happened.&nbsp; I doubt that will have any effect on how much the story is told.<br />
<br />
Fred has had a remarkable influence on the field.&nbsp; So much so that I won't attempt to list anything here: you can find all about him all of the internet.&nbsp; Let me just say that the first time I met him, I was intimidated.&nbsp; Not only because he was Fred, but because I knew (and still know) next to nothing about speech, and the conversation inevitably turned to speech.&nbsp; Here's roughly how a segment of our conversation went:<br />
<br />
<b>Hal:</b> What new projects are going on these days?<br />
<b>Fred:</b> (Excitedly.)&nbsp; We have a really exciting new speech recognition problem.&nbsp; We're trying to map speech signals directly to fluent text.<br />
<b>Hal:</b> (Really confused.) Isn't that the speech recognition problem?<br />
<b>Fred:</b> (Playing the "teacher role" now.)&nbsp; Normally when you transcribe speech, you end up with a transcrit that includes disfluencies like "uh" and "um" and also false starts <i>[Ed note: like "I went... I went to the um store"]</i>.<br />
<b>Hal:</b> So now you want to produce the actual fluent sentence, not the one that was spoken?<br />
<b>Fred: </b>Right.<br />
<br />
Apparently (who knew) in speech recognition you try to transcribe disfluencies and are penalized for missing them!&nbsp; We then talked for a while about how they were doing this, and other fun topics.<br />
<br />
A few weeks later, I got a voicemail on my home message machine from Fred.&nbsp; That was probably one of the coolest things that have ever happened to me in life.&nbsp; I actually saved it (but subsequently lost it, which saddens me greatly).&nbsp; The content is irrelevant: the point is that Fred -- <i>Fred!</i> -- called me -- <i>me!</i> -- at <i>home!</i>&nbsp; Amazing.<br />
<br />
I'm sure that there are lots of other folks who knew Fred better than me, and they can add their own stories in comments if they'd like.&nbsp; Fred was a great asset to the field, and I will certainly miss his physical presense in the future, though his work will doubtless continue to affect the field for years and decades to come.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/09/very-sad-news.html' title='permanent link'>9/15/2010 08:42:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=5842730110981448091' onclick=''>6
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/09/very-sad-news.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=5842730110981448091' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=5842730110981448091&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>13 September 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='328454048805025605'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/09/aistats-2011-call-for-papers.html'>AIStats 2011 Call for Papers</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>The <a href="http://www.aistats.org/cfp.php">full call</a>, and <a href="http://www.aistats.org/reviewing.php">some changes to the reviewing process</a>.  The submission deadline is Nov 1, and the conference is April 11-13, in Fort Lauderdale, Florida.  Promises to be warm :).<br />
<br />
The changes the the reviewing process are interesting.&nbsp; Basically the main change is that the author response is replaced by a journal-esque "revise and resubmit."&nbsp; That is, you get 2 reviews, edit your paper, submit a new version, and get a 3rd review.&nbsp; The hope is that this will reduce author frustration from the low bandwidth of author response.&nbsp; Like with a journal, you'll also submit a "diff" saying what you've changed.&nbsp; I can see this going really well: the third reviewer will presumably see a (much) better than the first two.&nbsp; The disadvantage, which irked me at ICML last year, is that it often seemed like the third reviewer made the <i>deciding call</i>, and I would want to make sure that the first two reviewers also get updated.&nbsp; I can also see it going poorly: authors invest even more time in "responding" and no one listens.&nbsp; That will be increased frustration :).<br />
<br />
The other change is that there'll be more awards.&nbsp; I'm very much in favor of this, and I spend two years on the NAACL exec trying to get NAACL to do the same thing, but always got voted down :).&nbsp; Oh well.&nbsp; The reason I think it's a good idea is two-fold.&nbsp; First, I think we're bad at selecting single best papers: a committee decision can often lead to selecting least offensive papers rather than ones that really push the boundary.&nbsp; I also think there are lots of ways for papers to be great: they can introduce new awesome algorithms, have new theory, have a great application, introduce a cool new problem, utilize a new linguistic insight, etc., etc., etc... Second, best papers are most useful at promotion time (hiring, and tenure), where you're being compared with people from other fields.&nbsp; Why should <i>our</i> field put <i>our</i> people at a disadvantage by not awarding great work that they can list of their CVs?<br />
<br />
Anyway, it'll be an interesting experiment, and I encourage folks to submit!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/09/aistats-2011-call-for-papers.html' title='permanent link'>9/13/2010 08:09:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=328454048805025605' onclick=''>6
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/09/aistats-2011-call-for-papers.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=328454048805025605' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=328454048805025605&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>07 September 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='8465870987707832413'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/09/manifold-assumption-versus-margin.html'>Manifold Assumption versus Margin Assumption</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p><div style="color: #660000;"><span style="font-size: x-small;">[This post is based on some discussions that came up while talking about manifold learning with <a href="http://www.cs.utah.edu/%7Ewhitaker/">Ross Whitaker</a> and <a href="http://www.cs.utah.edu/%7Esgerber/">Sam Gerber</a>, who had <a href="http://www.cs.utah.edu/%7Esgerber/research/kmm_iccv09.pdf">a great manifold learning paper</a> at ICCV last year.]</span></div><br />
There are two assumptions that are often used in statistical learning (both theory and practice, though probably more of the latter), especially in the semi-supervised setting.&nbsp; Unfortunately, they're incompatible.<br />
<br />
<b>The margin assumption</b> states that your data are well separated.&nbsp; Usually it's in reference to linear, possibly kernelized, classifiers, but that need not be the case.&nbsp; As most of us know, there are lots of other assumptions that boil down to the same thing, such as the low-weight-norm assumption, or the Gaussian prior assumption.&nbsp; At the end of the day, it means your data looks like what you have on the left, below, not what you have on the right.<br />
<div class="separator" style="clear: both; text-align: center;"><a href="//3.bp.blogspot.com/_EZRdsBDhqno/TIbfhQiZWhI/AAAAAAAAABU/h5UilJsHmCw/s1600/margin.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="//3.bp.blogspot.com/_EZRdsBDhqno/TIbfhQiZWhI/AAAAAAAAABU/h5UilJsHmCw/s320/margin.png"></a></div><b>The manifold assumption</b> that is particularly popular in semi-supervised learning, but also shows up in supervised learning, says that your data lie on a low dimensional manifold embedded in a higher dimensional space.&nbsp; One way of thinking about this is saying that your features cannot covary arbitrarily, but the manifold assumption is quite a bit stronger.&nbsp; It usually assumes a Reimannian (i.e., locally Euclidean) structure, with data points "sufficiently" densely sampled.&nbsp; In other words, life looks like the left, not the right, below:<br />
<div class="separator" style="clear: both; text-align: center;"><a href="//2.bp.blogspot.com/_EZRdsBDhqno/TIbgdchU-kI/AAAAAAAAABc/Bgzjkt31efM/s1600/manifold.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="//2.bp.blogspot.com/_EZRdsBDhqno/TIbgdchU-kI/AAAAAAAAABc/Bgzjkt31efM/s320/manifold.png"></a></div>Okay, yes, I know that the "Bad" one is a 2D manifold embedded in 2D, but that's only because I can't draw 3D images :).&nbsp; And anyway, this is a "weird" manifold in the sense that at one point (where the +s and -s meet), it drops down to 1D.&nbsp; This is fine in math-manifold land, but usually not at all accounted for in ML-manifold land.<br />
<br />
The problem, of course, is that once you say "margin" and "manifold" in the same sentence, things just can't possibly work out.&nbsp; You'd end up with a picture like:<br />
<br />
<div class="separator" style="clear: both; text-align: center;"><a href="//4.bp.blogspot.com/_EZRdsBDhqno/TIbhStF5ZkI/AAAAAAAAABk/LVzCOksX8-g/s1600/badifold.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="//4.bp.blogspot.com/_EZRdsBDhqno/TIbhStF5ZkI/AAAAAAAAABk/LVzCOksX8-g/s320/badifold.png"></a></div>This is fine from a margin perspective, but it's definitely not a (densely sampled) manifold any more.<br />
<br />
In fact, almost by definition, once you stick a margin into a manifold (which is okay, since you'll define margin Euclideanly, and manifolds know how to deal with Euclidean geometry locally), you're hosed.<br />
<br />
So I guess the question is: who do you believe?</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/09/manifold-assumption-versus-margin.html' title='permanent link'>9/07/2010 07:10:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=8465870987707832413' onclick=''>9
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/09/manifold-assumption-versus-margin.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=8465870987707832413' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=8465870987707832413&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>31 August 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='455954435539811009'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/08/online-learning-algorithms-that-work.html'>Online Learning Algorithms that Work Harder</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>It seems to be a general goal in practical online learning algorithm development to have the updates be very very simply.&nbsp; Perceptron is probably the simplest, and involves just a few adds.&nbsp; Winnow takes a few multiplies.&nbsp; MIRA takes a bit more, but still nothing hugely complicated.&nbsp; Same with stochastic gradient descent algorithms for, eg., hinge loss.<br />
<br />
I think this maybe used to make sense.&nbsp; I'm not sure that it makes sense any more.&nbsp; In particular, <b>I would be happier with online algorithms that do more work per data point, but require only one pass over the data.</b>&nbsp; There are really only two examples I know of: the <a href="http://hal3.name/docs/daume09onepass.pdf">StreamSVM work</a> that my student <a href="http://www.cs.utah.edu/%7Epiyush">Piyush</a> did with me and <a href="http://www.cs.utah.edu/%7Esuresh/">Suresh</a>, and the <a href="http://www.cs.jhu.edu/%7Emdredze/publications/icml_variance.pdf">confidence-weighted</a> work by Mark Dredze, Koby Crammer and Fernando Pereira (note that they maybe weren't<i> trying</i> to make a one-pass algorithm, but it does seem to work well in that setting).<br />
<br />
Why do I feel this way?<br />
<br />
Well, if you look even at standard classification tasks, you'll find that if you have a highly optimized, dual threaded implementation of stochastic gradient descent, then your bottleneck becomes I/O, not learning.&nbsp; This is what John Langford observed in his <a href="http://hunch.net/%7Evw/">Vowpal Wabbit</a> implementation.&nbsp; He has to do multiple passes.&nbsp; He deals with the I/O bottleneck by creating an I/O friendly, proprietary version of the input file during the first past, and then careening through it on subsequent passes.<br />
<br />
In this case, basically what John is seeing is that I/O is too slow.&nbsp; Or, phrased differently, learning is too fast :).&nbsp; I never thought I'd say that, but I think it's true.&nbsp; Especially when you consider that just having two threads is a pretty low requirement these days, it would be nice to put 8 or 16 threads to good use.<br />
<br />
But I think the problem is actually quite a bit more severe.&nbsp; You can tell this by realizing that the idealized world in which binary classifier algorithms usually get developed is, well, idealized.&nbsp; In particular, <i>someone has already gone through the effort of computing all your features for you.</i>&nbsp; Even running something simple like a tokenizer, stemmer and stop word remover over documents takes a non-negligible amount of time (to convince yourself: run it over Gigaword and see how long it takes!), <i>easily</i> much longer than a silly perceptron update.<br />
<br />
So in the real world, you're probably going to be computing your features and learning on the fly.&nbsp; (Or at least that's what I always do.)&nbsp; In which case, if you have a few threads computing features and one thread learning, your learning thread is <i>always</i> going to be stalling, waiting for features.<br />
<br />
One way to partially circumvent this is to do a variant of what John does: create a big scratch file as you go and write everything to this file on the first pass, so you can just read from it on subsequent passes.&nbsp; In fact, I believe this is what Ryan McDonald does in MSTParser (he can correct me in the comments if I'm wrong :P).&nbsp; I've never tried this myself because I am lazy.&nbsp; Plus, it adds unnecessary complexity to your code, requires you to chew up disk, and of course adds its own delays since you now have to be writing to disk (which gives you tons of seeks to go back to where you were reading from initially).<br />
<br />
A similar problem crops up in structured problems.&nbsp; Since you usually have to run inference to get a gradient, you end up spending way more time on your inference than your gradients.&nbsp; (This is similar to the problems you run into when trying to <a href="http://www.ryanmcd.com/papers/parallel_perceptronNAACL2010.pdf">parallelize the structured perceptron</a>.)<br />
<br />
Anyway, at the end of the day, I would probably be happier with an online algorithm that spent a little more energy per-example and required fewer passes; I hope someone will invent one for me!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/08/online-learning-algorithms-that-work.html' title='permanent link'>8/31/2010 07:09:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=455954435539811009' onclick=''>11
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/08/online-learning-algorithms-that-work.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=455954435539811009' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=455954435539811009&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>,
<a href='https://nlpers.blogspot.com/search/label/online%20learning' rel='tag'>online learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>27 August 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='2085596220220817325'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/08/calibrating-reviews-and-ratings.html'>Calibrating Reviews and Ratings</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>NIPS decision are going out soon, and then we're done with submitting and reviewing for a blessed few months.  Except for journals, of course.<br /><br />If you're not interested in paper reviews, but are interested in sentiment analysis, please skip the first two paragraphs :).<br /><br />One thing that anyone who has ever area chaired, or probably even ever reviewed, has noticed is that different people have different "baseline" ratings.  Conferences try to adjust for this, for instance NIPS defines their 1-10 rating scale as something like "8 = Top 50% of papers accepted to NIPS" or something like that.  Even so, some people are just harsher than others in scoring, and it seems like the area chair's job to calibrate for this.  (For instance, I know I tend to be fairly harsh -- I probably only give one 5 (out of 5) for every ten papers I review, and I probably give two or three 1s in the same size batch.  I have friends who never give a one -- except in the case of something just being <span style="font-style: italic;">wrong</span> -- and often give 5s.  Perhaps I should be nicer; I know CS tends to be harder on itself than other fiends.)  As an aside, this is one reason why I'm generally in favor of fewer reviewers and more reviews per reviewer: it allows easier calibration.<br /><br />There's also the issue of areas.  Some areas simply seem to be harder to get papers into than others (which can lead to some gaming of the system).  For instance, if I have a "new machine learning technique applied to parsing," do I want it reviewed by parsing people or machine learning people?  How do you calibrate across areas, other than by some form of affirmative action for less-represented areas?<br /><br />A similar phenomenon occurs in sentiment analysis, as was pointed out to me at ACL this year by Franz Och.  The example he gives is very nice.  If you go to TripAdvisor and look up <a href="http://www.tripadvisor.com/Restaurant_Review-g33300-d493634-Reviews-The_French_Laundry-Yountville_Napa_Valley_California.html">The French Laundry</a>, which is definitely one of the best restaurants in the U.S. (some people say <span style="font-style: italic;">the best</span>), you'll see that it got 4.0/5.0 stars, and a 79% recommendation.  On the other hand, if you look up <a href="http://www.tripadvisor.com/Restaurant_Review-g32655-d1006054-Reviews-In_N_Out_burger-Los_Angeles_California.html">In'N'Out Burger</a>, a LA-based burger chain (which, having grown up in LA, was admittedly one of my favorite places to eat in high school, back when I ate stuff like that) you see another 4.0/5.0 stars and a 95% recommendation.<br /><br />So now, we train a machine learning system to predict that the rating for The French Laundry is 79% and In'N'Out Burger is 95%.  And we expect this to work?!<br /><br />Probably the main issue here is calibrating for <span style="font-style: italic;">expectations.</span>  As a teacher, I've figured out quickly that managing student expectations is a big part of getting good teaching reviews.  If you go to In'N'Out, and have expectations for a Big Mac, you'll be pleasantly surprised.  If you go to The French Laundry with expectations of having a meal worth selling your soul, your children's souls, etc., for, then you'll probably be disappointed (though I can't really say: I've never been).<br /><br />One way that a similar problem has been dealt with on Hotels.com is that they'll show you ratings for the hotel you're looking at, and statistics of ratings for other hotels within a 10 mile radius (or something).  You could do something similar for restaurants, though distance probably isn't the right categorization: maybe price.  For "$", In'N'Out is probably near the top, and for "$$$$" The French Laundry probably is.<br /><br />(Anticipating comments, I don't think this is just an "aspect" issue.  I don't care how bad your palate is, even just considering the "quality of food" aspect, Laundry has to trump In'N'Out by a large margin.)<br /><br />I think the problem is that in all of these cases -- papers, restaurants, hotels -- and others (movies, books, etc.) there simply isn't a total order on the "quality" of the objects you're looking at.  (For instance, as soon as a book becomes a best seller, or is advocated by Oprah, I am probably <span style="font-style: italic;">less</span> likely to read it.)  There is maybe a situation-depend order, and the distance to hotel, or "$" rating, or area classes are heuristics for describing this "situation."  Bit without knowing the situation, or having a way to approximate it, I worry that we might be entering a garbage-in-garbage-out scenario here.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/08/calibrating-reviews-and-ratings.html' title='permanent link'>8/27/2010 11:14:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=2085596220220817325' onclick=''>10
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/08/calibrating-reviews-and-ratings.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=2085596220220817325' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=2085596220220817325&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/sentiment' rel='tag'>sentiment</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>23 August 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='7219466186164349152'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/08/finite-state-nlp-with-unlabeled-data-on.html'>Finite State NLP with Unlabeled Data on Both Sides</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>(Can you tell, by the recent frequency of posts, that I'm try not to work on getting ready for classes next week?)<br /><br /><span style="color: rgb(153, 0, 0);">[This post is based partially on some conversations with Kevin Duh, though not in the finite state models formalism.]</span><br /><br />The finite state machine approach to NLP is very appealing (I mean both string and tree automata) because you get to build little things in isolation and then chain them together in cool ways.  Kevin Knight has a great slide about how to put these things together that I can't seem to find right now, but trust me that it's awesome, especially when he explains it to you :).<br /><br />The other thing that's cool about them is that because you get to build them in isolation, you can use different data sets, which means data sets with different assumptions about the existence of "labels", to build each part.  For instance, to do speech to speech transliteration from English to Japanese, you might build a component system like:<br /><br />English speech --A--> English phonemes --B--> Japanese phonemes --C--> Japanese speech --D--> Japanese speech LM<br /><br />You'll need a language model (D) for Japanese speech, that can be trained just on acoustic Japanese signals, then parallel Japanese speech/phonemes (for C), parallel English speech/phonemes (for A) and parallel English phonemes/Japanese phonemes (for B).  [Plus, of course, if you're missing any of these, EM comes to your rescue!]<br /><br />Let's take a simpler example, though the point I want to make applies to long chains, too.<br /><br />Suppose I want to just do translation from French to English.  I build an English language model (off of monolingual English text) and then an <span style="font-style: italic;">English-to-French transducer</span> (remember that in the noisy channel, things flip direction).  For the E2F transducer, I'll need parallel English/French text, of course.  The English LM gives me p(e) and the transducer gives me p(f|e), which I can put together via Bayes' rule to get something proportional to p(e|f), which will let me translate new sentences.<br /><br />But, presumably, I also have lots of monolingual French text.  Forgetting math for a moment, which seems to suggest that this can't help me, we can ask: <span style="font-style: italic;">why</span> should this help?<br /><br />Well, it probably won't help with my English language model, but it <span style="font-style: italic;">should</span> be able to help with my transducer.  Why?  Because my transducer is supposed to give me p(f|e).  If I have some French sentence in my GigaFrench corpus to which my transducer assigns zero probability (for instance, max_e p(f|e) = 0), then this is probably a sign that something bad is happening.<br /><br />More generally, I feel like the following two operations should probably give roughly the same probabilities:<br /><ol><li>Drawing an English sentence from the language model p(e).</li><li>Picking a French sentence at random from GigaFrench, and drawing an English sentence from p(e|f), where p(e|f) is the composition of the English LM and the transducer.</li></ol>If you buy this, then perhaps one thing you could do is to try to learn a transducer q(f|e) that has low KL divergence between 1 and 2, above.  If you work through the (short) make, and throw away terms that are independent of the transducer, then you end up wanting to minimize <span style="font-style: italic;">[ </span>sum<span style="font-style: italic;">_e p(e) </span>log sum<span style="font-style: italic;">_f q(f|e) ]</span>.  Here, the sum over f is a <span style="font-style: italic;">finite</span> sum over GigaFrench, and the sum over e is an <span style="font-style: italic;">infinite</span> sum over positive probability English sentences given my the English LM p(e).<br /><br />One could then apply something like <a href="http://www.seas.upenn.edu/%7Etaskar/pubs/pr_jmlr10.pdf">posterior regularization</a> (Kuzman Ganchev, Graça and Taskar) to do the learning.  There's the nasty bit about how to compute these things, but that's why you get to be friends with Jason Eisner so he can tell you how to do anything you could ever want to do with finite state models.<br /><br />Anyway, it seems like an interesting idea.  I'm definitely not aware if anyone has tried it.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/08/finite-state-nlp-with-unlabeled-data-on.html' title='permanent link'>8/23/2010 10:45:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=7219466186164349152' onclick=''>6
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/08/finite-state-nlp-with-unlabeled-data-on.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=7219466186164349152' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=7219466186164349152&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/finite%20state%20methods' rel='tag'>finite state methods</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>21 August 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4819320652193757740'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/08/readers-kill-blogs.html'>Readers kill blogs?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I try to avoid making meta-posts, but the timing here was just too impeccable for me to avoid a short post on something that's been bothering me for a year or so.<br /><ul><li>On the one hand, yesterday, <a href="http://www.stat.columbia.edu/%7Ecook/movabletype/archives/2010/08/why_i_blog.html">Aleks stated that the main reason he blogs is to see comments</a>.  (Similarly, <a href="http://blog.computationalcomplexity.org/2010/08/comments.html">Lance also thinks comments are a very important part of having an "open" blog</a>.)<br /><br /></li><li>On the other hand, <a href="http://earningmyturns.blogspot.com/2010/08/how-do-you-consume-media.html">people are more an more moving to systems like Google Reader, as re-blogged by Fernando</a>, also yesterday.</li></ul>I actually complete agree with both points.  The problem is that I worry that they are actually fairly opposed.  I comment <span style="font-style: italic;">much less</span> on other people's blogs now that I use reader, because the 10 second overhead of clicking on the blog, being redirected, entering a comment, blah blah blah, is just too high.  Plus, I worry that no one (except the blog author) will see my comment, since most readers don't (by default) show comments in with posts.<br /><br />Hopefully the architects behind readers will pick up on this and make these things (adding and viewing comments, within the reader -- yes, I realize that it's then not such a "reader") easier.  That is, unless they want to lose out to tweets!<br /><br />Until then, I'd like to encourage people to continue commenting here.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/08/readers-kill-blogs.html' title='permanent link'>8/21/2010 12:42:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4819320652193757740' onclick=''>14
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/08/readers-kill-blogs.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4819320652193757740' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4819320652193757740&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>19 August 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='726284263469295772'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/08/multi-task-learning-should-our.html'>Multi-task learning: should our hypothesis classes be the same?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>It is almost an unspoken assumption in multitask learning (and domain adaptation) that you use the same type of classifier (or, more formally, the same <span style="font-style: italic;">hypothesis class</span>) for all tasks.  In NLP-land, this usually means that everything is a linear classifier, and the feature sets are the same for all tasks; in ML-land, this usually means that the same kernel is used for every task.  In neural-networks land (ala Rich Caruana), this is enforced by the symmetric structure of the networks used.<br /><br />I probably would have gone on not even considering this unspoken assumption, until a few years ago I saw a couple papers that challenged it, albeit indirectly.  One was <a href="http://acl.ldc.upenn.edu/P/P06/P06-1060.pdf">Factorizing Complex Models: A Case Study in Mention Detection</a> by Radu (Hans) Florian, Hongyan Jing, Nanda Kambhatla and Imed Zitouni, all from IBM.  They're actually considering solving tasks separately rather than jointly, but joint learning and multi-task learning are very closely related.  What they see is that different features are useful for spotting entity spans, and for labeling entity types.<br /><br />That year, or the next, I saw another paper (can't remember who or what -- if someone knows what I'm talking about, please comment!) that basically showed a similar thing, where a linear kernel was doing best for spotting entity spans, and a polynomial kernel was doing best for labeling the entity types (with the same feature sets, if I recall correctly).<br /><br />Now, to some degree this is not surprising.  If I put on my feature engineering hat, then I probably <span style="font-style: italic;">would</span> design slightly different features for these two tasks.  On the other hand, coming from a multitask learning perspective, this is surprising: if I believe that these tasks are related, shouldn't I also believe that I can do well solving them in the same hypothesis space?<br /><br />This raises an important (IMO) question: if I want to allow my hypothesis classes to be different, what can I do?<br /><br />One way is to punt: you can just concatenate your feature vectors and cross your fingers.  Or, more nuanced, you can have some set of shared features and some set of features unique to each task.  This is similar (the nuanced version, not the punting version) to what Jenny Finkel and Chris Manning did in their ACL paper this year, <a href="http://www.stanford.edu/%7Ejrfinkel/papers/hier-joint.pdf">Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a>.<br /><br />An alternative approach is to let the two classifiers "talk" via unlabeled data.  Although motivated differently, this was something of the idea behind my EMNLP 2008 paper on <a href="http://hal3.name/docs/daume08hints.pdf">Cross-Task Knowledge-Constrained Self Training</a>, where we run two models on unlabeled data and look for where they "agree."<br /><br />A final idea that comes to mind, though I don't know if anyone has tried anything like this, would be to try to do some feature extraction over the two data sets.  That is, basically think of it as a combination of multi-view learning (since we have two different hypothesis classes) and multi-task learning.  Under the assumption that we have access to examples labeled for <span style="font-style: italic;">both</span> tasks simultaneously (i.e., not the settings for either Jenny's paper or my paper), then one could do a 4-way kernel CCA, where data points are represented in terms of their task-1 kernel, task-2 kernel, task-1 label and task-2 label.  This would be sort of a blending of CCA-for-multiview-learning and CCA-for-multi-task learning.<br /><br />I'm not sure what the right way to go about this is, but I think it's something important to consider, especially since it's an assumption that usually goes unstated, even though empirical evidence seems to suggest it's not (always) the right assumption.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/08/multi-task-learning-should-our.html' title='permanent link'>8/19/2010 01:45:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=726284263469295772' onclick=''>5
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/08/multi-task-learning-should-our.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=726284263469295772' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=726284263469295772&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/domain%20adaptation' rel='tag'>domain adaptation</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>02 August 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='8966372274793856563'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/08/why-discourse-structure.html'>Why Discourse Structure?</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I come from a strong lineage of discourse folks.  Writing a parser for Rhetorical Structure Theory was one of the first class projects I had when I was a grad student.  Recently, with the release of the <a href="http://nlpers.blogspot.com/2006/05/penn-discourse-treebank.html">Penn Discourse Treebank</a>, there has been a bit of a flurry of interest in this problem (I had <a href="http://nlpers.blogspot.com/2009/09/acl-and-emnlp-retrospective-many-days.html">some snarky comments</a> right after ACL about this).  I've also talked about why this is a <a href="http://nlpers.blogspot.com/2007/04/discourse-is-darn-hard.html">hard problem</a>, but never really about why it is an <i>interesting</i> problem.<br /><br />My thinking about discourse has changed a lot over the years.  My current thinking about it is in an "interpretation as abduction" sense.  (And I sincerely hope all readers know what that means... if not, go back and read some classic papers by Jerry Hobbs.)  This is a view I've been rearing for a while, but I finally started putting it into words (probably mostly Jerry's words) in a conversation at ACL with Hoifung Poon and Joseph Turian (I think it was Joseph... my memory fades quickly these days :P).<br /><br />This view is that discourse is that <span style="font-style: italic;">thing</span> that gives you an interpretation above and beyond whatever interpretations you get from a sentence.  Here's a slightly refined version of the example I came up with on the fly at ACL:<br /><ol><li>I only like traveling to Europe.  So I submitted a paper to ACL.</li><li>I only like traveling to Europe.  Nevertheless, I submitted a paper to ACL.</li></ol>What does the hearer (H) infer from these sentences.  Well, if we look at the sentences <span style="font-style: italic;">on their own</span>, then H infers something like Hal-likes-travel-to-Europe-and-only-Europe, and H infers something like Hal-submitted-a-paper-to-ACL.  But when you throw discourse in, you can derive two additional bits of information.  In example (1), you can infer ACL-is-in-Europe-this-year and in (2) you can infer the negation of that.<br /><br />Pretty amazing stuff, huh?  Replacing a "so" with a "nevertheless" completely changes this interpretation.<br /><br />What does this have to do with interpretation as abduction?  Well, we're going to <span style="font-style: italic;">assume</span> that this discourse is coherent.  Given that assumption, we have to ask ourselves: in (1), what do we have to assume about the world to make this discourse coherent?  The answer is that you have to assume that ACL is in Europe.  And similarly for (2).<br /><br />Of course, there are other things you could assume that would make this discourse coherent.  For (1), you could assume that I have a rich benefactor who likes ACL submissions and will send me to Europe every time I submit something to ACL.  For (2), you could assume that I didn't want my paper to get in, but I wanted a submission to get reviews, and so I submitted a crappy paper.  Or something.  But these fail the Occam's Razor test.  Or, perhaps they are a priori simply less likely (i.e., you have to assume more to get the same result).<br /><br />Interestingly, I can change the interpretation of (2), for instance, by adding a third sentence to the discourse: "I figured that it would be easy to make my way to Europe after going to Israel."  Here, we would abduce that ACL is in Israel, and that I'm willing to travel to Israel on my way to Europe.  For you GOFAI folks, this would be something like non-monotonic reasoning.<br /><br />Whenever I talk about discourse to people who don't know much about it, I always get this nagging sense of "yes, but why do I <span style="font-style: italic;">care</span> that you can recognize that sentence 4 is background to sentence 3, unless I want to do summarization?"  I hope that this view provides some alternative answer to that question.  Namely, that there's some information you can get from sentences, but there is additional information in how those sentences are glued together.<br /><br />Of course, one of the big problems we have is that we have no idea how to represent sentence-level interpretations, or at least some ideas but no way to get there in the general case.  In the sentence-level case, we've seen some progress recently in terms of representing semantics in a sort of substitutability manner (ala paraphrasing), which is nice because the representation is still text.  One could ask if something similar might be possible at a discourse level.  Obviously you could paraphrase discourse connectives, but that's missing the point.  What else could you do?</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/08/why-discourse-structure.html' title='permanent link'>8/02/2010 03:49:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=8966372274793856563' onclick=''>2
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/08/why-discourse-structure.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=8966372274793856563' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=8966372274793856563&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/discourse' rel='tag'>discourse</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>24 July 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4723287240737022045'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/07/acl-2010-retrospective.html'>ACL 2010 Retrospective</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p><a href="http://acl2010.org/general.htm">ACL 2010</a> finished up in Sweden a week ago or so.  Overall, I enjoyed my time there (the local organization was great, though I think we got hit with unexpected heat, so those of us who didn't feel like booking a room at the Best Western -- hah!  why would I have done that?! -- had no A/C and my room was about 28-30 every night).<br /><br />But you don't come here to hear about sweltering nights, you come to hear about papers.  My list is actually pretty short this time.  I'm not quite sure why that happened.  Perhaps NAACL sucked up a lot of the really good stuff, or I went to the wrong sessions, or something.  (Though my experience was echoed by a number of people (n=5) I spoke to after the conference.)  Anyway, here are the things I found interesting.<br /><ul><li>    <div class="ms_author_list">    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1160.pdf">Beyond NomBank: A Study of Implicit  Arguments for Nominal Predicates</a></span>, by Matthew Gerber and Joyce Chai (this was the Best Long Paper award recipient).  This was <span style="font-style: italic;">by far</span> my favorite paper of the conference.  For all you students out there (mine included!), pay attention to this one.  It was great because they looked at a fairly novel problem, in a fairly novel way, put clear effort into doing something (they annotated a bunch of data by hand), developed features that were <span style="font-style: italic;">significantly</span> more interesting than the usual off-the-shelf set, and got impressive results on what is clearly a very hard problem.  Congratulations to Matthew and Joyce -- this was a great paper, and the award is highly deserved.<br /></div><span style="font-style: italic;"><br /></span></div></li><li><div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1010.pdf">Challenge Paper: The Human Language Project: Building a  Universal Corpus of the World&#8217;s Languages</a></span>, by Steven Abney and Steven Bird.  Basically this would be awesome if they can pull it off -- a giant structured database with stuff from tons of languages.  Even just having <span style="font-style: italic;">tokenization</span> in tons of languages would be useful for me.<br /><br /></div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1015.pdf">Extracting Social Networks from  Literary Fiction</a></span>, by David Elson, Nicholas Dames and Kathleen McKeown.  (This was the IBM best student paper.) Basically they construct networks of characters from British fiction and try to analyze some literary theories in terms of those networks, and find that there might be holes in the existing theories.  My biggest question, as someone who's not a literary theorist, is <span style="font-style: italic;">why</span> did those theories exist in the first place?  The analysis was over 80 or so books, surely literary theorists have read and pondered all of them.<br /><br /></div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1047.pdf">Syntax-to-Morphology Mapping in Factored  Phrase-Based Statistical Machine Translation from English to Turkish</a></span>, by Reyyan Yeniterzi and Kemal Oﬂazer.  You probably know that I think translating morphology and translating out of English are both interesting topics, so it's perhaps no big surprise that I liked this paper.  The other thing I liked about this paper is that they presented things that worked, as well as things that might well have worked but didn't.<br /><br /></div></li><li>    <div class="ms_author_list"><a href="http://aclweb.org/anthology-new/P/P10/P10-2034.pdf">Learning Common Grammar from Multilingual  Corpus</a>, by Tomoharu Iwata, Daichi Mochihashi and Hiroshi Sawad.  I wouldn't go so far as to say that I thought this was a great paper, but I would say there is the beginning of something interesting here.  They basically learn a coupled PCFG in Jenny Finkel hierarchical-Bayes style, over multiple languages.  The obvious weakness is that languages don't all have the same structure. If only there were an <a href="http://hal3.name/docs/daume07implication.pdf">area of linguistics that studies how they differ</a>....  (Along similar lines, see<br />   <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1131.pdf">Phylogenetic Grammar Induction</a></span>, by Taylor Berg-Kirkpatrick and Dan Klein, which has a similar approach/goal.)  </div><br /></div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1088.pdf">Bucking the Trend: Large-Scale Cost-Focused  Active Learning for Statistical Machine Translation</a></span>, by Michael Bloodgood and Chris Callison-Burch.  The "trend" referenced in the title is that active learning always asymptotes depressingly early.  They have turkers translate bits of sentences in context (i.e., in a whole sentence, translate the highlighted phrase) and get a large bang-for-the-buck.  Right now they're looking primarily at out-of-vocabulary stuff, but there's a lot more to do here.</div></li></ul>A few papers that I didn't see, but other people told me good things about:<br /><ul><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1018.pdf">&#8220;Was It Good? It Was Provocative.&#8221; Learning  the Meaning of Scalar Adjectives</a></span>, by Marie-Catherine de Marneffe, Christopher D. Manning and Christopher  Pott.  </div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1031.pdf">Unsupervised Ontology Induction from Text</a></span>, by Hoifung Poon and Pedro Domingos.</div></li><li>    <div class="ms_author_list"><a href="http://aclweb.org/anthology-new/P/P10/P10-1046.pdf">Improving the Use of Pseudo-Words for  Evaluating Selectional Preferences</a>, by Nathanael Chambers and Daniel Jurafsky.</div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1083.pdf">Learning to Follow Navigational Directions</a></span>, by Adam Vogel and Daniel Jurafsky.</div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-1093.pdf">Compositional Matrix-Space Models of  Language</a></span>, by Sebastian Rudolph and Eugenie Giesbrecht.  (This was described to me as "thought provoking" though not necessarily more.)</div></li><li>    <div class="ms_author_list"><span style="font-style: italic;"><a href="http://aclweb.org/anthology-new/P/P10/P10-2037.pdf">Top-Down K-Best A* Parsing</a></span>, by Adam Pauls, Dan Klein and Chris Quirk.</div></li></ul>At any rate, I guess that's a reasonably long list.  There were definitely good things, but with a fairly heavy tail.  If you have anything you'd like to add, feel free to comment.  (As an experiment, I've turned comment moderation on as a way to try to stop the spam... I'm not sure I'll do it indefinitely; I hadn't turned it on before because I always thought/hoped that Google would just start doing spam detection and/or putting hard captcha's up or <span style="font-style: italic;">something</span> to try to stop spam, but sadly they don't seem interested.)</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/07/acl-2010-retrospective.html' title='permanent link'>7/24/2010 07:35:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4723287240737022045' onclick=''>6
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/07/acl-2010-retrospective.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4723287240737022045' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4723287240737022045&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>,
<a href='https://nlpers.blogspot.com/search/label/papers' rel='tag'>papers</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>28 June 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4429690407859784904'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/06/icml-2010-retrospective.html'>ICML 2010 Retrospective</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>Just got back from Israel for <a href="http://www.icml2010.org/">ICML</a>, which was a great experience: I'd wanted to go there for a while and this was a perfect opportunity. I'm very glad I spent some time afterwards out of Haifa, though.<br /><br />Overall, I saw a lot of really good stuff.  The usual caveats apply (I didn't see everything it's a biased sample, blah blah blah).  Here are some things that stood out:<br /><br /><a href="http://www.icml2010.org/papers/522.pdf">Structured Output Learning with Indirect Supervision</a> (M.-W. Chang, V. Srikumar, D. Goldwasser, D. Roth).  This was probably one of my favorite papers of the conference, even though I had learned some about the work when I visited UIUC a few months ago.  Let's say you're trying to do word alignment, and you have a few labeled examples of alignments.  But then you also have a bunch of parallel data.  What can you do?  You can turn the parallel data into a <i>classification</i> problem: are these two sentences translations of each other.  You can pair random sentences to get negative examples.  A very clever observation is basically that the weight vector for this binary classifier should point in the same direction as the weight vector for the (latent variable) structured problem!  (Basically the binary classifier should say "yes" only when there exists an alignment that renders these good translations.)  Tom Dietterich asked a question during Q/A: these binary classification problems seem very hard: is that bad?  Ming-Wei reassured him that it wasn't. In thinking about it after the fact, I wonder if it is actually <i>really importantant</i> that they're hard: namely, if they were easy, then you could potentially answer the question without bothering to make up a reasonable alignment.  I suspect this might be the case.<br /><br /><a href="http://www.icml2010.org/papers/384.pdf">A Language-based Approach to Measuring Scholarly Impact</a> (S. Gerrish, D. Blei).  The idea here is that without using citation structure, you can model influence in large document collections.  The basic idea is that when someone has a new idea, they often introduce new terminology to a field that wasn't there before.  The important bit is that they don't change all of science, or even all of ACL: they only change what gets talked about in their particular sub-area (aka topic :P).  It was asked during Q/A what would happen if you did use citations, and my guess based on my own small forays in this area is that the two sources would really reinforce eachother.  That is, you might regularly cite the original EM even if your paper has almost nothing to do with it.  (The example from the talk was then Penn Treebank paper: one that has a bajillion citations, but hasn't <i>lexically</i> affected how people talk about research.)<br /><br /><a href="http://www.icml2010.org/papers/495.pdf">Hilbert Space Embeddings of Hidden Markov Models</a> (L. Song, B. Boots, S. Saddiqi, G. Gordon, A. Smola).  This received one of the best paper awards.  While I definitely liked this paper, actually what I liked more what that it taught me something from COLT last year that I hadn't known (thanks to Percy Liang for giving me more details on this).  That paper was <a href="http://www.cs.mcgill.ca/%7Ecolt2009/papers/011.pdf">A spectral algorithm for learning hidden Markov models</a> (D. Hsu, S. Kakade, T. Zhang) and basically shows that you can use spectral decomposition techniques to "solve" the HMM problem.  You create the matrix of observation pairs (A_ij = how many times did I see observation j follow observation i) and then do some processing and then a spectral decomposition and, voila, you get parameters to an HMM!  In the case that the data was actually generated by an HMM, you get good performance and good guarantees. Unfortunately, if the data was <i>not</i> generated by an HMM, then the theory doesn't work and the practice does worse than EM.  That's a big downer, since nothing is ever generated by the model we use, but it's a cool direction.  At any rate, the current paper basically asks what happens if your observations are drawn from an RKHS, and then does an analysis.  (Meta-comment: as was pointed out in the Q/A session, and then later to me privately, this has fairly strong connections to some stuff that's been done in Gaussian Process land recently.)<br /><br /><a href="http://www.icml2010.org/papers/549.pdf">Forgetting Counts: Constant Memory Inference for a Dependent Hierarchical Pitman-Yor Process</a> (N. Bartlett, D. Pfau, F. Wood).  This paper shows that if you're building a hierarchical Pitman-Yor language model (think Kneser-Ney smoothing if that makes you feel more comfortable) in an online manner, then you should feel free to throw out entire restaurants as you go through the process.  (A restaurant is just the set of counts for a given context.)  You do this to maintain a maximum number of restaurants at any given time (it's a fixed memory algorithm).  You can do this intelligently (via a heuristic) or just stupidly: pick them at random.  Turns out it doesn't matter.  The explanation is roughly that if it were important, and you threw it out, you'd see it again and it would get re-added.  The chance that something that occurs a lot keeps getting picked to be thrown out is low.  There's some connection to using <a href="//www.blogger.com/post-create.g?blogID=19803222">approximate counting for language modeling</a>, but the Bartlett et al. paper is being even stupider than we were being!<br /><br /><a href="http://www.icml2010.org/papers/587.pdf">Learning efficiently with approximate inference via dual losses</a> (O. Meshi, D. Sontag, T. Jaakkola, A. Globerson).  Usually when you train structured models, you alternate between running inference (a maximization to find the most likely output for a given training instance) and running some optimization (a minimization to move your weight vector around to achieve lower loss).  The observation here is that by taking the dual of the inference problem, you turn the maximization into a minimization.  You now have a dual minimization, which you can solve <i>simultaneously</i>, meaning that when your weights are still crappy, you aren't wasting time finding perfect outputs.  Moreover, you can "warm start" your inference for the next round.  It's a very nice idea.  I have to confess I was a bit disappointed by the experimental results, though: the gains weren't quite what I was hoping.  However, most of the graphs they were using weren't very large, so maybe as yo move toward harder problems, the speed-ups will be more obvious.<br /><br /><a href="http://www.icml2010.org/papers/458.pdf">Deep learning via Hessian-free optimization</a> (J. Martens).  Note that I neither saw this presentation nor read the paper (skimmed it!), but I talked with James about this over lunch one day.  The "obvious" take away message is that you should read up on your optimization literature, and start using second order methods instead of your silly gradient methods (and don't store that giant Hessian: use efficient matrix-vector products).  But the less obvious take away message is that some of the prevailing attitudes about optimizing deep belief networks may be wrong.  For those who don't know, the usual deal is to train the networks layer by layer in an auto-encoder fashion, and then at the end apply back-propogation.  The party line that I've already heard is that the layer-wise training is <i>very important</i> to getting the network near a "good" local optimum (whatever that means).  But if James' story holds out, this seems to not be true: he doesn't do any clever initialization and still find good local optima!<br /><br /><a href="http://www.icml2010.org/papers/638.pdf">A theoretical analysis of feature pooling in vision algorithms</a> (Y.-L. Boureau, J. Ponce, Y. LeCun). Yes, that's right: a vision paper.  Why should you read this paper? Here's the question they're asking: after you do some blah blah blah feature extraction stuff (specifically: Sift features), you get something that looks like a multiset of features (hrm.... sounds familiar).  These are often turned into a histogram (basically taking averages) and sometimes just used as a bag: did I see this feature or not.  (Sound familiar yet?)  The analysis is: why should one of these be better and, in particular, why (in practice) do vision people see multiple regimes.  Y-Lan et al. provide a simple, obviously broken, model (that assumes feature independence... okay, this has to sound familiar now) to look at the discriminability of these features (roughly the ration of between-class variances and overall variances) to see how these regimes work out.  And they look basically how they do in practice (modulo one "advanced" model, which doesn't quite work out how they had hoped).<br /><br />Some other papers that I liked, but don't want to write too much about:<br /><ul> <li><a href="http://www.icml2010.org/papers/568.pdf">Learning Programs: A Hierarchical   Bayesian Approach</a> (P. Liang, M. Jordan, D. Klein).  Structured models over programs are <span style="font-style: italic;">very</span> hard; this paper gives one approach to modeling them.<br /><br /></li><li><a href="http://www.icml2010.org/papers/433.pdf">Budgeted Nonparametric Learning from   Data Streams</a> (R. Gomes, A. Krause).  Shows that a clustering problem and a Gaussian process problem are submodular, goes from there.<br /><br /></li><li><a href="http://www.icml2010.org/papers/442.pdf">Internal Rewards Mitigate Agent   Boundedness</a> (J. Sorg, S. Singh, R. Lewis).  Exactly what the title says.<br /><br /></li><li><a href="http://www.icml2010.org/papers/248.pdf">The Translation-invariant Wishart-Dirichlet Process for Clustering Distance Data</a> (J. Vogt, S. Prabhakaran, T. Fuchs, V. Roth).  Been wanting to do something like this for a while, but they did it better than I would have!<br /><br /></li><li><a href="http://www.icml2010.org/papers/636.pdf">Sparse Gaussian Process Regression   via L_1 Penalization</a> (F. Yan, Y. Qi).  Very interesting way to get sparsity in a GP basically by changing your approximating distribution. </li></ul>Some papers that other people said they liked were:<br /><ul> <li><a href="http://www.icml2010.org/papers/569.pdf">Multi-Class Pegasos on a Budget</a> (Z. Wang, K. Crammer, S. Vucetic)<br /><br /></li><li><a href="http://www.icml2010.org/papers/376.pdf">Risk minimization, probability   elicitation, and cost-sensitive SVMs</a> (H. Masnadi-Shirazi,   N. Vasconcelos)<br /><br /></li><li><a href="http://www.icml2010.org/papers/107.pdf">Asymptotic Analysis of Generative Semi-Supervised Learning</a> (J. Dillon, K. Balasubramanian, G. Lebanon) </li></ul>Hope to see you at ACL!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/06/icml-2010-retrospective.html' title='permanent link'>6/28/2010 07:42:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4429690407859784904' onclick=''>12
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/06/icml-2010-retrospective.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4429690407859784904' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4429690407859784904&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>,
<a href='https://nlpers.blogspot.com/search/label/papers' rel='tag'>papers</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>07 June 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='1650900416182826748'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/06/naacl-2010-retrospective.html'>NAACL 2010 Retrospective</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I just returned from <a href="http://www.naacl2010.org/">NAACL 2010</a>, which was simultaneously located in my home town of Los Angeles and located nowhere near my home town of Los Angeles.  (That's me trying to deride downtown LA as being nothing like real LA.)<br /><br />Overall I was pleased with the program.  I saw a few talks that changed (a bit) how I think about some problems.  There were only one or two talks I saw that made me wonder how "that paper" got in, which I think is an acceptable level.  Of course I spend a great deal of time not at talks, but no longer feel bad about doing so.<br /><br />On tutorials day, I saw Hoifung Poon's tutorial on Markov Logic Networks.  I think Hoifung did a great job of targetting the tutorial at just the right audience, which probably wasn't exactly me (though I still quite enjoyed it myself).  I won't try to describe MLNs, but my very brief summary is "language for compactly expressing complex factor graphs (or CRFs, if you prefer)."  That's not exactly right, but I think it's pretty close.  You can check back in a few months and see if there are going to be any upcoming "X, Y and Daume, 2011" papers using MLNs :P.  At any rate, I think it's a topic worth knowing about, especially if you really just want to get a system up and running quickly.  (I'm also interested in trying Andrew McCallum's <a href="http://code.google.com/p/factorie/">Factorie</a> system, which, to some degree, trades easy of use for added functionality.  But honestly, I don't really have time to just try things these days: students have to do that for me.)<br /><br />One of my favorite papers of the conference was one that I hadn't even planned to go see!  It is <a href="//www.blogger.com/aclweb.org/anthology-new/N/N10/N10-1120.pdf">Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables</a> by Tetsuji Nakagawa, Kentaro Inui and Sadao Kurohashi. (I saw it basically because by the end of the conference, I was too lazy to switch rooms after the prvious talk.)  There are two things I really like about this paper.  The first is that the type of sentiment they're going after is really broad.  Example sentences included things that I'd love to look up, but apparently were only in the slides... but definitely more than "I love this movie."  The example in the paper is "Tylenol prevents cancer," which is a nice positive case. <br /><br />The basic idea is that some words give you sentiment.  For instance, by itself, "cancer" is probably negative.  But then some words <i>flip</i> polarity.  Like "prevents."  Or negation.  Or other things.  They set up a model based on sentence level annotations with latent variables for the "polarity" words and for the "flipping" words.  The flipping words are allowed to flip any sentiment below them in the dependency tree.  Cool idea!  Of course, I have to nit-pick the paper a bit.  It probably would be better to allow arguments/adjuncts to flip polarity, too.  Otherwise, negation (which is usually a leaf) will never flip anything.  And adjectives/adverbs can't flip either (eg., going from "happy" to "barely happy").  But overall I liked the paper.<br /><br />A second thing I learned is that XOR problems <i>do</i> exist in real life, which <a href="http://hunch.net/?p=245">I had previously questioned</a>.  The answer came (pretty much unintentionally) from the paper <a href="http://www.aclweb.org/anthology/N/N10/N10-1119.pdf">The viability of web-derived polarity lexicons</a> by Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan and Ryan McDonald.  I won't talk much about this paper other than to say that if you have 4 billion web pages, you can get some pretty good sentimenty words, if you're careful to not blindly apply graph propagation.  But at the end, they throw a meta classifier on the polarity classification task, whose features include things like (1) how many positive terms are in the text, (2) how many negative terms are in the text, (3) how many negations are in the text.  Voila!  XOR!  (Because negation XORs terms.)<br /><br />I truly enjoyed Owen Rambow's poster on <a href="http://www.aclweb.org/anthology/N/N10/N10-1049.pdf">The   Simple Truth about Dependency and Phrase Structure Representations:   An Opinion Piece</a>.  If you're ever taken a class in mathematical logic, it is very easy for me to summarize this paper: parse trees (dependency or phrase structure) are your languge, but unless you have a theory of that language (in the model-theoretic sense) then whatever you do is meaningless.  In more lay terms: you can always push symbols around, but unless you tie a semantics to those symbols, you're really not doing anything.  Take home message: pay attention to the meaning of your symbols!<br /><br />In the category of "things everyone should know about", there was <a href="http://www.aclweb.org/anthology/N/N10/N10-1083.pdf">Painless unsupervised learning with features</a> by Taylor Berg-Kirkpatrick, Alexandre Bouchard Côté, John DeNero and Dan Klein.  The idea is that you can replace your multinomails in an HMM (or other graphical model) with little maxent models.  Do EM in this for unsuperviesd learning and you can throw in a bunch of extra features. I would have liked to have seen a comparison against naive Bayes with the extra features, but <a href="http://hal3.name/docs/daume09unsearn.pdf">my prior belief</a> is sufficiently strong that I'm willing to believe that it's helpful.  The only sucky thing about this training regime is that training maxent models with (tens of) thousands of classes is pretty painful.  Perhaps a reduction like tournaments or SECOC would help bring it down to a log factor.<br /><br />I didn't see the presentation for <a href="http://www.aclweb.org/anthology/N/N10/N10-1116.pdf">From baby steps to leapfrog: How "Less is More" in unsupervised dependency parsing</a> by Valetin Spitkovsky, Hiyan Alshawi and Dan Jurafsky, but I read it. The idea is that you can do better unsupervised dependency parsing by giving your learner progressively harder examples.  I really really really tried to get something like this to work for <a href="http://hal3.name/docs/daume09unsearn.pdf">unsearn</a>, but nothing helped and most things hurn.  (I only tried adding progressively longer sentences: other ideas, based on conversations with other folks, include looking at vocabulary size, part of speech (eg., human babies learn words in a particular order), etc.)  I'm thrilled it actually works.<br /><br />Again, I didn't see <a href="http://www.aclweb.org/anthology/N/N10/N10-1066.pdf">Discriminative Learning over Constrained Latent Representations</a> by Ming-Wei Chang, Dan Goldwasser, Dan Roth and Vivek Srikumar, but I learned about the work when I visited UIUC recently (thanks again for the invitation, Dan R.!).  This paper does exactly what you would guess from the title: learns good discriminative models when you have complex latent structures that you know something about a priori.<br /><br />I usually ask people at the end of conferences what papers they liked. Here are some papers that were spoken highly of by my fellow NAACLers. (This list is almost unadulterated: one person actually nominated one of the papers I thought shouldn't have gotten in, so I've left it off the list.  Other than that, I think I've included everything that was specifically mentioned to me.) <ol> <li><a href="http://www.aclweb.org/anthology/N/N10/N10-1118.pdf">Optimal     Parsing Strategies for Linear Context-Free Rewriting Systems</a>     by Daniel Gildea.<br /><br /></li><li><a href="http://www.aclweb.org/anthology/N/N10/N10-1003.pdf">Products     of Random Latent Variable Grammars</a> by Slav Petrov.<br /><br /></li><li><a href="http://www.aclweb.org/anthology/N/N10/N10-1015.pdf">Joint     Parsing and Alignment with Weakly Synchronized Grammars</a> by     David Burkett, John Blitzer and Dan Klein.<br /><br /></li><li><a href="http://www.aclweb.org/anthology/N/N10/N10-1056.pdf"> For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia</a> by Mark Yatskar, Bo Pang, Cristian   Danescu-Niculescu-Mizil and Lillian Lee.<br /><br /></li><li><a href="http://www.aclweb.org/anthology/N/N10/N10-1082.pdf">Type-Based     MCMC</a> by Percy Liang, Michael I. Jordan and Dan Klein. </li></ol>I think I probably have two high level "complaints" about the program this year.  <b>First</b>, I feel like we're seeing more and more "I downloaded blah blah blah data and trained a model using entirely standard features to predict something and it kind of worked" papers. I apologize if I've just described your paper, but these papers really rub me the wrong way.  I feel like I just don't learn anything from them: we already know that machine learning works surprisingly well and I don't really need more evidence of that.  Now, if my sentence described your paper, but your paper additionally had a really interesting <i>analysis</i> that helps us understand something about language, then you rock!  <b>Second</b>, I saw a <i>lot</i> of presentations were speakers were somewhat embarassingly unaware of very prominent very relevant prior work.  (In none of these cases was the prior work my own: it was work that's much more famous.) Sometimes the papers were cited (and it was more of a "why didn't you compare against that" issue) but very frequently they were not. Obviously not everyone knows about all papers, but I recognized this even for papers that aren't even close to my area.<br /><br />Okay, I just ranted, so let's end on a positive note.  I'm leaving the conference knowing more than when I went, and I had fun at the same time.  Often we complain about the obvious type I errors and not-so-obvious type II errors, but overall I found the program strong.  Many thanks to the entire program committee for putting together an on-average very good set of papers, and many thanks to all of <i>you</i> for writing these papers!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/06/naacl-2010-retrospective.html' title='permanent link'>6/07/2010 08:11:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=1650900416182826748' onclick=''>16
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/06/naacl-2010-retrospective.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=1650900416182826748' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=1650900416182826748&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/conferences' rel='tag'>conferences</a>,
<a href='https://nlpers.blogspot.com/search/label/papers' rel='tag'>papers</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>29 April 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='5939551284766191861'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/04/graduating-want-post-doc-let-nsf-pay.html'>Graduating?  Want a post-doc?  Let NSF pay!</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I get many of emails of the form "I'm looking for a postdoc...."  I'm sure that other, more senior, more famous people get lots of these.  My internal reaction is always "Great: I wish I could afford that!"  NSF has a solution: let them pay for it!  This is the second year of the <a href="http://cifellows.org/">CI fellows program</a>, and I know of two people who did it last year (one in NLP, one in theory).  I think it's a great program, both for faculty and for graduates (especially since the job market is so sucky this year).<br /><br />If you're graduating, you should apply (unless you have other, better, job options already).  But beware, the deadline is <span style="font-weight: bold;">May 23</span>!!! Here's more info directly from NSF's mouth:<br /><blockquote style="font-style: italic;">The CIFellows Project is an opportunity for recent Ph.D. graduates in computer science and closely related fields to obtain one- to two-year postdoctoral positions at universities, industrial research laboratories, and other organizations that advance the field of computing and its positive impact on society. The goals of the CIFellows project are to retain new Ph.D.s in research and teaching and to support intellectual renewal and diversity in the computing fields at U.S. organizations.<br />.....<br />Every CIFellow application must identify 1-3 host mentors. <a href="http://match.cifellows.org/">Click here</a> to visit a website where prospective mentors have posted their information. In addition, openings that have been posted over the past year (and may be a source of viable mentors/host organizations for the CIFellowships) are <a href="http://cifellows.org/opportunities">available here</a>.<br /></blockquote>Good luck!<br /><blockquote></blockquote></p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/04/graduating-want-post-doc-let-nsf-pay.html' title='permanent link'>4/29/2010 09:43:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=5939551284766191861' onclick=''>15
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/04/graduating-want-post-doc-let-nsf-pay.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=5939551284766191861' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=5939551284766191861&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/community' rel='tag'>community</a>,
<a href='https://nlpers.blogspot.com/search/label/hiring' rel='tag'>hiring</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>20 April 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='6521860775208709654'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/04/b-and-not-b-not.html'>((A => B) and (not B)) => (not A)</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I remember back in middle school or high school -- added uncertainty so as not to date myself too much -- I first learned of the existence of file compression software.  We used pkzip, mostly.  I remember one of the first things I did was: compress myfile.exe to myfile.exe.zip.  Then compress that to myfile.exe.zip.zip.  Then myfile.exe.zip.zip.zip.  I cannot tell you how disappointed I was when, at one stage, the file size went up after "compression."<br /><br />We read papers all the time that demonstrate something roughly of the form "if A then B."  The happens most obviously the closer you get to theory (when such statements can be made precise), but happens also in non-theoretical work.  The point of this post is: <b>if you believe A => B, then you have to ask yourself: which do I believe more?  A, or not B?</b><br /><br />The simplest case is the compression case.  Let's say a weak compressor is one that always reduces a (non-empty) file's size by one bit.  A strong compressor is one that cuts the file down to one bit.  I can easily prove to you that if you give me a weak compressor, I can turn it into a strong compressor by running it N-1 times on files of size N.  Trivial, right?  But what do you conclude from this?  You're certainly not happy, I don't think  For what I've proved really is that weak compressors don't exist, not that strong compressors do.  That is, you believe <i>so strongly</i> that a strong compressor is impossible, that you must conclude from (weak) => (strong) that (weak) cannot possibly exist.<br /><br />Let's take the most obvious example from machine learning: boosting.  The basic result of boosting is that I can turn a "weak classifier" into a "strong classifier."  A strong classifier is one that (almost always) does a really good job at classification.  A weak classifier is one that (always always) does a not completely crappy job at classification.  That is, a strong classifier will get you 99.9% accuracy (most of the time) while I weak classifier will only get you 50.1% accuracy (most of the time).  Boosting works by repeatedly applying the weak classifier to modified data sets in order ot get a strong classifier out.<br /><br />In all the boosting papers, the results are presented as positive.  That is: look, obviously we want strong classifiers.  But weak classifiers look much more attainable.  And voila, by boosting magic, we can turn the weak into the strong.  This is an A => B setting: (existence of weak classifiers) => (existence of strong classifiers).<br /><br />Sounds great, right?  But let me ask you: do you believe more strongly that (weak classifiers exist) or more strongly that (strong classifiers do not exist)?  For me, it's the latter.  To some degree, no free lunch theorems apply here.  This yields a <b>totally different read</b> of boosting results, namely: <b>weak classifiers don't even exist!!!</b><br /><br />More on the language side, one of the more classic experimentally results we have is that if you give me a really good semantic representation, I can do an awesome job doing natural language generation from those semantics.  In order to do translation, for instance, I just have to generate the semantics of a French sentence and then I can generate a near-perfect English translation.  (French semantics) => (Perfect translations).  But do I believe mose strongly that we can get perfect French semantics or that we can not get perfect translation?  Right now, almost certainly the latter.<br /><br />My point is really one about critical reading.  When you read a paper, things will be presented in one light.  But that's often not the only light in which they can be interpreted.  Apply your own interpretation.</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/04/b-and-not-b-not.html' title='permanent link'>4/20/2010 11:40:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=6521860775208709654' onclick=''>25
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/04/b-and-not-b-not.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=6521860775208709654' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=6521860775208709654&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>12 April 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='2082227438864109984'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/04/how-i-teach-machine-learning.html'>How I teach machine learning</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>I've had discussions about this with tons of people, and it seems like my approach is fairly odd.  So I thought I'd blog about it because I've put a lot of thought into it over the past four offerings of the machine learning course here at Utah.<br /><br />At a high level, if there is one thing I want them to remember after the semester is over it's the idea of <span style="font-style: italic;">generalization</span> and how it relates to function complexity<span style="font-style: italic;">.</span>  That's it.  Now, more operationally, I'd like them to learn SVMs (and kernels) and EM for generative models.<br /><br />In my opinion, the whole tenor of the class is set by how it starts.  Here's how I start.<br /><ol><li>Decision trees.  No entropy.  No mutual information.  Just decision trees based on classification accuracy.  <span style="font-style: italic;">Why?</span>  Because the point isn't to teach them decision trees.  The point is to get as quickly as possible to the point where we can talk about things like generalization and function complexity.  Why decision trees?  Because EVERYONE gets them.  They're so intuitive.  And analogies to 20 questions abound.  We also talk about the who notion of data being drawn from a distribution and what it means to predict well in the future.<br /><br /></li><li>Nearest neighbor classifiers.  No radial basis functions, no locally weighted methods, etc.  <span style="font-style: italic;">Why</span>?  Because I want to introduce the idea of thinking of data as points in high dimensional space.  This is a <span style="font-style: italic;">big</span> step for a lot of people, and one that takes some getting used to.  We then do k-nearest neighbor and relate it to generalization, overfitting, etc.  The punch line of this section is the idea of a decision boundary and the complexity of decision boundaries.<br /><br /></li><li>Linear algebra and calculus review.  At this point, they're ready to see why these things matter.  We've already hinted at learning as some sort of optimization (via decision trees) and data in high dimensions, hence calculus and linear algebra.  Note: no real probability here.<br /><br /></li><li>Linear classifiers as methods for directly optimizing a decision boundary.  We start with 0-1 loss and then move to perceptron.  Students love perceptron because it's so procedural.<br /></li></ol>The rest follows mostly as almost every other machine learning course out there.  But IMO these first four days are <span style="font-style: italic;">crucial.</span>  I've tried (in the past) starting with linear regression or linear classification and it's just a disaster.  You spend too much time talking about unimportant stuff.  The intro with error-based decision trees moving to kNN is amazingly useful.<br /><br />The sad thing is that there are basically no books that follow any order even remotely like this.  Except...drum roll... it's actually not far from what Mitchell's book does.  Except he does kNN much later.  It's really depressing how bad most machine learning books are from a pedagogical perspective... you'd think that in 12 years someone would have written something that works better.<br /><br />On top of that, the most recent time I taught ML, I structured everything around recommender systems.  You can actually make it all work, and it's a lot of fun.  We actually did recommender systems for classes here at the U (I had about 90-odd students from AI the previous semester fill out ratings on classes they'd taken in the past).  The data was a bit sparse, but I think it was a lot of fun.<br /><br />The other thing I change most recently that I'm very happy with is that I have a full project on feature engineering.  (It ties in to the course recommender system idea.)  Why?  Because most people who take ML, if they ever use it at all, will need to do this.  It's maybe one of the most important things that they'll have to learn.  We should try to teach it.  Again, something that no one ever talks about in books.<br /><br />Anyway, that's my set of tricks.  If you have some that you particularly like, feel free to share!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/04/how-i-teach-machine-learning.html' title='permanent link'>4/12/2010 02:57:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=2082227438864109984' onclick=''>29
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/04/how-i-teach-machine-learning.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=2082227438864109984' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=2082227438864109984&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>,
<a href='https://nlpers.blogspot.com/search/label/teaching' rel='tag'>teaching</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>07 April 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='8857615392509433634'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/04/when-maximum-likelihood-doesnt-make.html'>When Maximum Likelihood Doesn't Make Sense</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>One of the most fun AHA moments in ML or stats is when you see that for multinomial distributions, your naive idea of relative frequency corresponds (through a bunch of calculus) to maximum likelihood estimation.  Ditto means of Gaussians, though that's never quite as amazing because Gaussians seems sort of odd beasts anyway.  It's nice when our intuitions match what stats tells us.<br /><br />I'll often watch a DVD, and then leave it in the DVD player until I want to watch something else.  Usually it will return to the menu screen and the timer display will go through 0:00, 0:01, 0:02, ..., until it hits the length of the title screen loop, and then go back to 0:00.  What I always wonder when I glance up at it is "what is the actual length of the title screen loop?"  That is, what is the highest value it'll count up to.<br /><br />Being a good statistician, I set up a model.  First I play the frequentist game.  Since I glance up and pretty arbitrary times, my observations can be modeled as a uniform random variable from 0 to some upper bound U, which is the quantity I want to infer.<br /><br />Supposing that I only make one observation, say 0:27, I want a maximum likelihood estimate of U.  It's easy to see that U=27 is the maximum likelihood solution.  Anything less than 27 will render my observation impossible.  For any U>=27, my observation has probability 1/(U+1), so the ML solution is precisely U=27.  Note that even if I observe five observations a <= b <= c <= d <= e, the ML solution is still U=e.  Huh.  The math works.  The model is as correct as I could really imagine it.  But this answer doesn't really seem reasonable to me.  Somehow I expect a number <span style="font-style: italic;">greater than</span> my observation to come about.<br /><br />Perhaps I need a prior.  That'll solve it.  I'll add a prior p(U) and then do maximum a posteriori.  Well, it's easy to see that if p(U) is unimodal, and its mode is less than my (highest) observation, then the MAP solution will still be the (highest) observation.  If it's mode is more than my (highest) observation, then the MAP solution will be the mode of p(U).  If I think about this a bit, it's hard for me to justify a multi-modal p(U), and also hard for me to be happy with a system in which my prior essentially completely determines my solution.<br /><br />Or I could be fully Bayesian and look at a posterior distribution p(U | data).  This will just be a left-truncated version of my prior, again, not very satisfying.<br /><br />(Note that the "Maximum Likelihood Sets" idea, which I also like quite a bit, doesn't fix this problem either.)<br /><br />It also really bugs me that only my largest observation really affects the answer.  If I get one hundred observations, most of them centered around 0:10, and then one at 0:30, then I'd guess that 0:30 or 0:31 or 0:32 is probably the right answer.  But if I observe a bunch of stuff spread roughly uniformly between 0:00 and 0:30, then I'd be more willing to believe something larger.<br /><br />I don't really have a solution to this dilemma.  Perhaps you could argue that my difficulties arise from the use of a Uniform distribution -- namely, that were I to use another distribution, these problems wouldn't really arise. I don't think this is quite true, as described below:<br /><br />We actually run in to this problem in NLP fairly often.  I observe a billion documents and see, in this 1b documents, 100k unique words, many of which are singletons.  But I'm not willing to believe that if I see that document 1,000,000,001, that there won't be a new unseen word.  Sure it's less likely that I see a new unique word than in document 1001, where it is almost guaranteed, but there's still a relatively large probability that some new word will show up in that next document.<br /><br />The whole point is that somehow we expect random samples to be representatives of the underlying distribution.  We don't expect them to <span style="font-style: italic;">define</span> the underlying distribution.  I don't actually expect to ever see the corner cases, unless I've seen everything else.<br /><br /><br /><span style="font-weight: bold;">UPDATE: </span>The Bayesian approach actually does do something reasonable.  Here is <a href="http://hal3.name/nlpers/uniform.m">some Matlab code</a> for computing posteriors with a uniform prior, and results with an upper bound of 100 and various observations are plotted below:<br /><br /><img src="https://lh4.googleusercontent.com/proxy/JNZIQmdmjXOvx0Xnzf8DPfQ_rTzGmd6PJpmLpwjiXgg8egsuld51yNGdCkp5WMzDkFUxXmuOGqotRME8Bw=s0-d"><br/><br /><img src="https://lh5.googleusercontent.com/proxy/eZ1NxdO1KcP53qyhC75BobD2DLI6vStj4peBAW-X2VMOMEfTphG8EFxz4M7Hr1uqhNUSkYmbBzRMP7WUZQ=s0-d"><br/><br /><img src="https://lh5.googleusercontent.com/proxy/lrAPbdXee-136amYqIzTDETHi0J1dk-W5V_GspuF5X2ODgaNhRM-2RSHUiDc4gul-aWGrYEaN_yQislohDlBDQ=s0-d"><br/><br /><img src="https://lh6.googleusercontent.com/proxy/b_KeWjmrFiIbkVv1WrkcDRYgA3BkAUayymNobgd73Yvg4VOi60eCBJvQrILZMDzZNfw18P_sOxUPKMDLk5hB=s0-d"><br/></p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/04/when-maximum-likelihood-doesnt-make.html' title='permanent link'>4/07/2010 10:26:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=8857615392509433634' onclick=''>4
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/04/when-maximum-likelihood-doesnt-make.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=8857615392509433634' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=8857615392509433634&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>,
<a href='https://nlpers.blogspot.com/search/label/statistics' rel='tag'>statistics</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>01 April 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='4986175089813985432'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/04/classification-weirdness-regression.html'>Classification weirdness, regression simplicity</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>In the context of some work on multitask learning, we came to realize that classification is kind of weird.  Or at least linear classification.  It's not that it's weird in a way that we didn't already know: it's just sort of a law of unexpected consequences.<br /><br />If we're doing linear (binary) classification, we all know that changing the magnitude of the weight vector doesn't change the predictions.  A standard exercise in a machine learning class might be to show that if your data is linearly separable, then for some models (for instance, unregularized models), the best solution is usually an infinite norm weight vector that's pointing in the right direction.<br /><br />This is definitely not true of (linear) regression.  Taking a good (or even perfect) linear regressor and blowing up the weights by some constant will kill your performance.  By adding a regularizer, what you're basically doing is just saying how big you want that norm to be.<br /><br />Of course, by regression I simply mean minimizing something like squared error and by classification I mean something like 0/1 loss or hinge loss or logistic loss or whatever.<br /><br />I think this is stuff that we all know.<br /><br />Where this can bite you in unexpected ways is the following.  In lots of problems, like domain adaptation and multitask learning, you end up making assumptions roughly of the form "my weight vector for domain A should look like my weight vector for domain B" where "look like" is really the place where you get to be creative and define things how you feel best.<br /><br />This is all well and good in the regression setting.  A magnitude 5 weight in regression means a magnitude 5 weight in regression.  But not so in classification.  Since you can arbitrarily scale your weight vectors and still get the same decision boundaries, a magnitude 5 weight kind of means nothing.  Or at least it means something that has to do more with the difficulty of the problem and how you chose to set your regularization parameter, rather than something to do with the task itself.<br /><br />Perhaps we should be looking for definitions of "look like" that are <span style="font-style: italic;">insensitive</span> to things like magnitude.  Sure you can always normalize all your weight vectors to unit norm before you co-regularize them, but that loses information as well.<br /><br />Perhaps this is a partial explanation of some negative transfer.  One thing that you see, when looking at the literature in DA and MTL, is that all the tasks are typically of about the same difficulty.  My expectation is that if you have two tasks that are highly related, but one is way harder than the other, is going to lead to negative transfer.  Why?  Because the easy task will get low norm weights, and the hard task will get high norm weights.  The high norm weights will pull the low norm weights toward them too much, leading to worse performance on the "easy" task.  In a sense, we actually want the <span style="font-style: italic;">opposite</span> to happen: if you have a really hard task, it shouldn't screw up everyone else that's easy!  (Yes, I know that being Bayesian might help here since you'd get a lot of uncertainty around those high norm weight vectors!)</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/04/classification-weirdness-regression.html' title='permanent link'>4/01/2010 08:18:00 AM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=4986175089813985432' onclick=''>15
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/04/classification-weirdness-regression.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=4986175089813985432' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=4986175089813985432&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/domain%20adaptation' rel='tag'>domain adaptation</a>,
<a href='https://nlpers.blogspot.com/search/label/machine%20learning' rel='tag'>machine learning</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

          </div></div>
        

          <div class="date-outer">
        
<h2 class='date-header'><span>20 February 2010</span></h2>

          <div class="date-posts">
        
<div class='post-outer'>
<div class='post uncustomized-post-template'>
<a name='474582300475348739'></a>
<h3 class='post-title'>
<a href='https://nlpers.blogspot.com/2010/02/google-5gram-corpus-has-unreasonable.html'>Google 5gram corpus has unreasonable 5grams</a>
</h3>
<div class='post-header-line-1'></div>
<div class='post-body'>
<p>In the context of something completely unrelated, I was looking for a fairly general pattern in the Google 1TB corpus.  In particular, I was looking for verbs that are sort of transitive.  I did a quick grep for 5grams of the form "the SOMETHING BLAHed the SOMETHING."  Or, more specifically:<br /><pre>    grep -i '^the [a-z][a-z]* [a-z][a-z]*ed the [a-z]*'<br /></pre>I then took these, lower cased them, and then merged the counts.  Here are the top 25, sorted and with counts:<br /><pre>     1  101500  the surveyor observed the use<br />    2   30619  the rivals shattered the farm<br />    3   27999  the link entitled the names<br />    4   22928  the trolls ambushed the dwarfs<br />    5   22843  the dwarfs ambushed the trolls<br />    6   21427  the poet wicked the woman<br />    7   15644  the software helped the learning<br />    8   13481  the commission released the section<br />    9   12273  the mayor declared the motion<br />   10   11046  the player finished the year<br />   11   10809  the chicken crossed the road<br />   12    8968  the court denied the motion<br />   13    8198  the president declared the bill<br />   14    7890  the board approved the following<br />   15    7848  the bill passed the house<br />   16    7373  the fat feed the muscle<br />   17    7362  the report presented the findings<br />   18    7115  the committee considered the report<br />   19    6956  the respondent registered the domain<br />   20    6923  the chairman declared the motion<br />   21    6767  the court rejected the argument<br />   22    6307  the court instructed the jury<br />   23    5962  the complaint satisfied the formal<br />   24    5688  the lord blessed the sabbath<br />   25    5486  the bill passed the senate<br /></pre>What the heck?!  First of all, the first one is shocking, but maybe you could convince me.  How about numbers 4 and 5?  "The trolls ambushed the dwarfs" (and vice versa)?  These things are the fourth and fifth most common five grams matching my pattern on the web?  "The poet wicked the woman"?  What does "wicked" even mean?  And yet these all beat out "The bill passed the house" and "The court instructed the jury".  But then #23: "The prince compiled the Mishna"???  (#30 is also funny: "the matrix reloaded the matrix" is an amusing segmentation issue.)<br /><br />If we do a vanilla google search for the counts of some of these, we get:<br /><pre>     1     10900  the surveyor observed the use<br />    4      7750  the trolls ambushed the dwarfs<br />    5      7190  the dwarfs ambushed the trolls<br />    6     <span style="font-weight: bold;">ZERO!</span>  the poet wicked the woman<br />   15  20200000  the bill passed the house<br />   22   3600000  the court instructed the jury<br /></pre>This just flabbergasts me.  I'm told that lots of people have expressed worries over the Google 1TB corpus, but have never actually heard anything myself...  And never seen anything myself.<br /><br />Does anyone have an explanation for these effects?  How can I expect to get anything done with such ridiculous data!</p>
<div style='clear: both;'></div>
</div>
<div class='post-footer'>
<p class='post-footer-line post-footer-line-1'>
<span class='post-author'>
Posted by
hal
</span>
<span class='post-timestamp'>
at
<a class='timestamp-link' href='https://nlpers.blogspot.com/2010/02/google-5gram-corpus-has-unreasonable.html' title='permanent link'>2/20/2010 02:22:00 PM</a>
</span>
<span class='post-comment-link'>

            | <a class='comment-link' href='https://www.blogger.com/comment.g?blogID=19803222&postID=474582300475348739' onclick=''>74
comments</a>
</span>
<span class='post-backlinks post-comment-link'>
<a class='comment-link' href='https://nlpers.blogspot.com/2010/02/google-5gram-corpus-has-unreasonable.html#links'>Links to this post</a>
</span>
<span class='post-icons'>
<span class='item-action'>
<a href='https://www.blogger.com/email-post.g?blogID=19803222&postID=474582300475348739' title='Email Post'>
<span class='email-post-icon'>&#160;</span>
</a>
</span>
<span class='item-control blog-admin pid-815832165'>
<a href='https://www.blogger.com/post-edit.g?blogID=19803222&postID=474582300475348739&from=pencil' title='Edit Post'>
<img alt='' class='icon-action' height='18' src='https://resources.blogblog.com/img/icon18_edit_allbkg.gif' width='18'/>
</a>
</span>
</span>
</p>
<p class='post-footer-line post-footer-line-2'>
<span class='post-labels'>
Labels:
<a href='https://nlpers.blogspot.com/search/label/data' rel='tag'>data</a>
</span>
</p>
<p class='post-footer-line post-footer-line-3'></p>
</div>
</div>
</div>

        </div></div>
      
</div>
<div class='blog-pager' id='blog-pager'>
<span id='blog-pager-newer-link'>
<a class='blog-pager-newer-link' href='https://nlpers.blogspot.com/search?updated-max=2012-09-26T08:24:00-06:00&amp;max-results=20&amp;reverse-paginate=true' id='Blog1_blog-pager-newer-link' title='Newer Posts'>Newer Posts</a>
</span>
<span id='blog-pager-older-link'>
<a class='blog-pager-older-link' href='https://nlpers.blogspot.com/search?updated-max=2010-02-20T14:22:00-07:00&amp;max-results=20' id='Blog1_blog-pager-older-link' title='Older Posts'>Older Posts</a>
</span>
<a class='home-link' href='https://nlpers.blogspot.com/'>Home</a>
</div>
<div class='clear'></div>
<div class='blog-feeds'>
<div class='feed-links'>
Subscribe to:
<a class='feed-link' href='https://nlpers.blogspot.com/feeds/posts/default' target='_blank' type='application/atom+xml'>Posts (Atom)</a>
</div>
</div>
</div></div>
</div>
<div id='sidebar-wrapper'>
<div class='sidebar section' id='sidebar'><div class='widget Profile' data-version='1' id='Profile1'>
<h2>About Me</h2>
<div class='widget-content'>
<a href='https://www.blogger.com/profile/02162908373916390369'><img alt='My photo' class='profile-img' height='80' src='//4.bp.blogspot.com/-S8xlo8qPXpU/WL7hxHzH1uI/AAAAAAAAAe8/4fKySmJ6GjgrPj3tFRsoZGtO5ddT4dAigCK4B/s80/menyc-250x300.png' width='65'/></a>
<dl class='profile-datablock'>
<dt class='profile-data'>
<a class='profile-name-link g-profile' href='https://www.blogger.com/profile/02162908373916390369' rel='author' style='background-image: url(//www.blogger.com/img/logo-16.png);'>
hal
</a>
</dt>
</dl>
<a class='profile-link' href='https://www.blogger.com/profile/02162908373916390369' rel='author'>View my complete profile</a>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=Profile&widgetId=Profile1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("Profile1"));' rel='nofollow' target='configProfile1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div><div class='widget Label' data-version='1' id='Label1'>
<h2>Labels</h2>
<div class='widget-content list-label-widget-content'>
<ul>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/acl'>acl</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/ACS'>ACS</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/advising'>advising</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/algorithms'>algorithms</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/bayesian'>bayesian</a>
<span dir='ltr'>(10)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/chunking'>chunking</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/classification'>classification</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/clustering'>clustering</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/community'>community</a>
<span dir='ltr'>(26)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/conferences'>conferences</a>
<span dir='ltr'>(45)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/coreference'>coreference</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/data'>data</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/discourse'>discourse</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/domain%20adaptation'>domain adaptation</a>
<span dir='ltr'>(5)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/evaluation'>evaluation</a>
<span dir='ltr'>(9)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/finite%20state%20methods'>finite state methods</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/graphical%20models'>graphical models</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/hiring'>hiring</a>
<span dir='ltr'>(7)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/information%20retrieval'>information retrieval</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/journals'>journals</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/language%20modeling'>language modeling</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/linguistics'>linguistics</a>
<span dir='ltr'>(7)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/loss%20functions'>loss functions</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/machine%20learning'>machine learning</a>
<span dir='ltr'>(45)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/machine%20translation'>machine translation</a>
<span dir='ltr'>(6)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/mcmc'>mcmc</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/news'>news</a>
<span dir='ltr'>(4)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/online%20learning'>online learning</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/papers'>papers</a>
<span dir='ltr'>(17)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/parsing'>parsing</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/PL'>PL</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/poll'>poll</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/problems'>problems</a>
<span dir='ltr'>(12)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/questions'>questions</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/random'>random</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/research'>research</a>
<span dir='ltr'>(12)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/reviewing'>reviewing</a>
<span dir='ltr'>(2)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/sentiment'>sentiment</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/software'>software</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/speech'>speech</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/statistics'>statistics</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/structured%20prediction'>structured prediction</a>
<span dir='ltr'>(5)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/summarization'>summarization</a>
<span dir='ltr'>(4)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/survey'>survey</a>
<span dir='ltr'>(6)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/teaching'>teaching</a>
<span dir='ltr'>(3)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/theory'>theory</a>
<span dir='ltr'>(1)</span>
</li>
<li>
<a dir='ltr' href='https://nlpers.blogspot.com/search/label/topic%20models'>topic models</a>
<span dir='ltr'>(1)</span>
</li>
</ul>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=Label&widgetId=Label1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("Label1"));' rel='nofollow' target='configLabel1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div><div class='widget BlogList' data-version='1' id='BlogList1'>
<h2 class='title'>My Blog List</h2>
<div class='widget-content'>
<div class='blog-list-container' id='BlogList1_container'>
<ul id='BlogList1_blogs'>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/gRmMbXYxsKaakAnDOlu22pn73PfLHqJy0TyxmGvXE-ML8dhnWFBNJqSvxj-F3dL2PwdVpIFMEuhSg9TVTHET5mLLbaXNHw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://statmodeling.stat.columbia.edu' target='_blank'>
Statistical Modeling, Causal Inference, and Social Science</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~3/FfbqmTx5vIo/' target='_blank'>
What&#8217;s the upshot?
</a>
</span>
<div class='item-time'>
10 hours ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/KFVTjwSPBHxu5yTo_QpLSb-6NWXuxf2iDLG36Od0kKaCT5noGO4IyTITQC-PI7Ndf_ixNQFYuDRikb_FAI1KdA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://nuit-blanche.blogspot.com/' target='_blank'>
Nuit Blanche</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/blogspot/wCeDd/~3/ii2Vklni8BM/neumann-networks-for-inverse-problems.html' target='_blank'>
Neumann Networks for Inverse Problems in Imaging
</a>
</span>
<div class='item-time'>
22 hours ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/BnQftQM3jOX5aZOFpbgfSbqiTDnjOdA3RAaJu6RpPDh5yYUC1nVfGB4wgFODh2WwDNjgUrOJSaS0e2Rc1yttzDRC=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://lucatrevisan.wordpress.com' target='_blank'>
in   theory</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/' target='_blank'>
Online Optimization Post 2: Constructing Pseudorandom Sets
</a>
</span>
<div class='item-time'>
1 day ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/0p3jSguSoH_b3zcUal959JlweUeJsUYgG7_ZDjsq9rPDH7hWb2xQIslqAloeEhhnSWRi-3tNicza8RuACF2qNFJNWjjbXZjZ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://blog.computationalcomplexity.org/' target='_blank'>
Computational Complexity</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html' target='_blank'>
Geo-Centric Complexity
</a>
</span>
<div class='item-time'>
1 day ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/AXKkogToc1zzaQzQ7gQUlMlDOT6yNpXYfoH-u-EN96orKMNShipc4B2-3w16-oxNSA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://lemire.me/blog' target='_blank'>
Daniel Lemire's blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/daniel-lemire/atom/~3/S2s4xkcZJVA/' target='_blank'>
The shopper&#8217;s dilemma: wait for new technology or buy now?
</a>
</span>
<div class='item-time'>
2 days ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/TArGCb8h_qeNfbn8xnZy2RHwgKsytt8SWh5ZNtfgwhAZt4bkHuFLHrIAGwOtIsSzmmXmCNQ0AShaBFwuggP4GZTC=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://mysliceofpizza.blogspot.com/' target='_blank'>
my slice of pizza</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://mysliceofpizza.blogspot.com/2019/04/conference-acm-siam-algorithmic.html' target='_blank'>
Conference: ACM-SIAM Algorithmic Principles of Computer Systems (APoCS20)
</a>
</span>
<div class='item-time'>
1 week ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/ZKegNZzwsozNHjZpHzqSB0RB3LH1XsP1IQCNkIRdzmSfXTs02gCPddS2GpBW-5uN9-xevClYJ7t5i_aie0o=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://terrytao.wordpress.com' target='_blank'>
What's new</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://terrytao.wordpress.com/2019/04/12/nominations-for-2020-doob-prize-now-open/' target='_blank'>
Nominations for 2020 Doob Prize now open
</a>
</span>
<div class='item-time'>
2 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/i986p1ASLFBHizPu7L-Izuc0qmUrwf8P1BUod8VTD041x9HGwTveSdoIAuT4Qayitd-54yMh8ag=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://blog.geomblog.org/' target='_blank'>
The Geomblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/TheGeomblog/~3/f-Go67FP6JE/new-conference-announcement.html' target='_blank'>
New conference announcement
</a>
</span>
<div class='item-time'>
2 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/sby2Wfl6cEqBMMS7feUM8QArNbm7_7tFmV5YGMt_MTCTQFGBFCuWGxbjuGlaTRrPRgqZ5y8oetOMrw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://wadler.blogspot.com/' target='_blank'>
Wadler's Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://wadler.blogspot.com/2019/03/pinkers-thirteen-rules-for-better.html' target='_blank'>
Pinker's Thirteen Rules for Better Writing
</a>
</span>
<div class='item-time'>
4 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/yxui1sZaOfhURaKfvaicNoXIzX5vD4DblwTuFJuQkMNTTYUt70-jS6QCK4L14MlSplmF6WSYDW6Lpk_L=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.talkingbrains.org/' target='_blank'>
Talking Brains</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/TalkingBrains/~3/spYQGDVYZ_k/postdoc-positions-in-cognitive.html' target='_blank'>
Postdoc Positions in Cognitive Neuroscience of Communication at the University of Connecticut
</a>
</span>
<div class='item-time'>
4 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/3mp7ahsogluZg1OlYi96t2lnp14FEJ4ZzPCbXZj-R7eVlc31hW1QQH6aeaJGEDIERIvcAoIXxOLd=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://www.jstatsoft.org/index.php/jss' target='_blank'>
Journal of Statistical Software</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://www.jstatsoft.org/index.php/jss/article/view/v088i07' target='_blank'>
CoClust: A Python Package for Co-Clustering
</a>
</span>
<div class='item-time'>
4 weeks ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/FjV3tq-0b3CpIwoo6e7Q0SOyQzgPvWNlD6xsQ0JVOK0FRoaifpI9GxPRZR2FA66r=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://hunch.net' target='_blank'>
Machine Learning (Theory)</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://hunch.net/?p=11377237' target='_blank'>
Code submission should be encouraged but not compulsory
</a>
</span>
<div class='item-time'>
2 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/KfOhaf0BkgQuxaz6OmtKkdMPZRS5wqyGaEaqCg0bmjAt7O2rCfYaYEcCp-BLMQ9wJlXznI9vUNoLY4FElPaEEQ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://mybiasedcoin.blogspot.com/' target='_blank'>
My Biased Coin</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://mybiasedcoin.blogspot.com/2019/01/analco-sosa-soda-post.html' target='_blank'>
ANALCO, SOSA, SODA post
</a>
</span>
<div class='item-time'>
3 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/QGeVyt5AkoZIpxWjBk_NL4HujYlryY5uNlkOWV3esZCj_zhLTAnX5EPH7F6g2omSs2kvidfzA25gO366=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://gowers.wordpress.com' target='_blank'>
Gowers's Weblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://gowers.wordpress.com/2018/12/22/how-craig-barton-wishes-hed-taught-maths/' target='_blank'>
How Craig Barton wishes he&#8217;d taught maths
</a>
</span>
<div class='item-time'>
4 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/vB_dUUOTiWKpwmQA3wQs0CbQvYmScWDWUELl0QmKNEBHiHgfmKIPefMnSE1S9DImirNZWWXy9VnQ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.scala-lang.org/news/' target='_blank'>
The Scala Programming Language</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://www.scala-lang.org/news/2018/12/20/programming-reactive-systems-course.html' target='_blank'>
New Course: &#8220;Programming Reactive Systems&#8221;
</a>
</span>
<div class='item-time'>
4 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/etR8ZpeKnVIJT2AedMQO5lqZAahaieyYw_JtjgTr--mRepSFK4iTaWba9AGRoqiNqdx7jW2-E2jCZ3ULwKw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://www.earningmyturns.org/' target='_blank'>
Earning My Turns</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://www.earningmyturns.org/2018/09/august-september-music.html' target='_blank'>
August-September music
</a>
</span>
<div class='item-time'>
7 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/FbIQW-FqR87X7HuQMennc2Ij_4FGVD44paEILlJTi6fQrteu6yFSXQ41CCjGHREZAMLQLzqE=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://math.andrej.com/' target='_blank'>
Mathematics and Computation</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://math.andrej.com/2018/08/25/how-to-implement-type-theory-in-an-hour/' target='_blank'>
How to implement type theory in an hour
</a>
</span>
<div class='item-time'>
8 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/SoLaIWMkhytJYtynE-8jWUTJKicd7Hih3dq7bSgDvrRsZmfOfTzqSVtUwICMMcpjmqpI-Y4N4uQNmU748YQLZ0Q=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.computervisionblog.com/' target='_blank'>
tombone's blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html' target='_blank'>
DeepFakes: AI-powered deception machines
</a>
</span>
<div class='item-time'>
11 months ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/yt8h1ZYHFMDQW_9PR6rNo1Dw2K3FUSn3fbGA6Fy5WOCLPRuxX055MHLXHwxADHzWnig0XH9lDwPdKaKkrA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://tcsmath.wordpress.com' target='_blank'>
tcs math - some mathematics of theoretical computer science</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://tcsmath.wordpress.com/2018/04/12/two-pages-of-the-book-stuck-together/' target='_blank'>
Two pages of the book, stuck together
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/noV6FJXy-GpUHYDaX4Jz08874ToASMix2ktY42-gnXHyTk7MAdICtwgaDvt-Z7zwTPOfnFwHaasPZGY=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://glinden.blogspot.com/' target='_blank'>
Geeking with Greg</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/GeekingWithGreg/~3/8hdwr6FYRNs/two-decades-of-amazoncom-recommendations.html' target='_blank'>
Two decades of Amazon.com recommendations
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/GpKKryW83zrC7eMxsACQryipwCxw37p_XUy-gHToXE8vtDcIeAYgSYOW6J6cF62aXSJsWursKdGRgjWZ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://logicomp.blogspot.com/' target='_blank'>
Logicomp</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://logicomp.blogspot.com/2017/06/correctness-by-design-vs-formal.html' target='_blank'>
Correctness by design vs. formal verification
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/-SCE5lhNjiVahN050Lb1_OtBsRZNaZAPXOXMvHwhVUauJzbLVJ_TwcE4lhH_aO0djhZDj6-f=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://xorshammer.com' target='_blank'>
XOR's Hammer</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='https://xorshammer.com/2017/05/29/how-is-it-even-possible-for-a-sailboat-to-sail-into-the-wind/' target='_blank'>
How is it even possible for a sailboat to sail into the wind?
</a>
</span>
<div class='item-time'>
1 year ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/GDBy96Tf59cU2vfXiwchvcA9KjhB6tN0w3DqzTp7ddHhBLCXWRwAEj0sW565UPv800n-pAF7TWti=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://michaelnielsen.org/blog' target='_blank'>
Michael Nielsen</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://feedproxy.google.com/~r/michaelnielsen/wmna/~3/lKQMYJNDCbs/' target='_blank'>
Is there a tension between creativity and accuracy?
</a>
</span>
<div class='item-time'>
2 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/78EWePFP1AJYPtSm9a_hjfzYgAE8s2PjjF6scLE0c_AEI0epeXqlAWuI1HHxko8OG7E7YNfeqA=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://blog.oddhead.com/' target='_blank'>
Oddhead Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://blog.oddhead.com/2015/10/25/algorithmic-economics-postdoc-msr-nyc/' target='_blank'>
Algorithmic economics postdoc position at Microsoft Research, NYC
</a>
</span>
<div class='item-time'>
3 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/Bldaa0ZH6pLuY6nJFpI0g2JBZuuqvE4iPnXgesmCMKKZH43KMjWJIuk6WsEruDwYjSHl8o8quz9A4Xnar6cLwLg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://andysresearch.blogspot.com/' target='_blank'>
Andy's Math/CS page</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://andysresearch.blogspot.com/2014/10/making-academic-contacts-some-thoughts.html' target='_blank'>
Making academic contacts (some thoughts for new researchers)
</a>
</span>
<div class='item-time'>
4 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/LZsZyd-3XVF0IO2rrd5Jpd_n1Z2W6neGxuoRv5w35le1VaGs9Lz_qyhVH4VTayrtpJsY3FAy3FGDmw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://statmt.blogspot.com/' target='_blank'>
The StatMT Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://statmt.blogspot.com/2014/09/easy-parallel-corpora-from-wikipedia.html' target='_blank'>
Easy parallel corpora from Wikipedia
</a>
</span>
<div class='item-time'>
4 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/qhUJAYiAXvnFUlSdEzeOuYoBnsake1NYn9ZwXSxs_7YdZw8JKc_9T1D6IsHMvnlVrR5Qj8RZmVteWxg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://vimsu99.blogspot.com/' target='_blank'>
Learning in Vision</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://vimsu99.blogspot.com/2014/03/dual-submissions-busted.html' target='_blank'>
Dual submissions -- busted!
</a>
</span>
<div class='item-time'>
5 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/GGKgv2BmcHGmvUAIfX7sUhK57Dc9d6KSQ8DVJMI71doXga1P7UuFrsFBK8DFUGaw4Uc8qWDIV3rRCvUhz_I=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://infoweekly.blogspot.com/' target='_blank'>
WebDiarios de Motocicleta</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://infoweekly.blogspot.com/2012/05/presburger-award.html' target='_blank'>
Presburger Award
</a>
</span>
<div class='item-time'>
6 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/NbftHuWwXwRm0DaBzWWdl-v1Mc9g0HRSI_iWCKDuPINT9Kk659MvF15X3YIXAuvyCqA7FbY3pFXT=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://lingpipe-blog.com' target='_blank'>
LingPipe Blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://lingpipe-blog.com/2010/05/04/upgrading-java-classes-backward-compatible-serialization/' target='_blank'>
Upgrading Java Classes with Backward-Compatible Serialization
</a>
</span>
<div class='item-time'>
8 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/f6P41xXANB9H0q36WCoBVSpW9pOS_vy-kZuJBs9330tEelQCHLfWLAN9opWP6cnffVGwsf_Vacckj_rCbH0iTg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://qualgorithms.blogspot.com/' target='_blank'>
Quantum Algorithms</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://qualgorithms.blogspot.com/2008/12/polynomial-time-quantum-algorithm-for.html' target='_blank'>
Polynomial-time quantum algorithm for the simulation of chemical dynamics
</a>
</span>
<div class='item-time'>
10 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/JizXqqROCXx8pzJDKXwmbRKZYGxbxWSENCQR8s8xlGXYHeNXOBV6qBTtTRMQmwtpGf4wfrHfT67b=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.sixthform.info/maths' target='_blank'>
Mathematics Weblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://www.sixthform.info/maths/?p=166' target='_blank'>
A Levels
</a>
</span>
<div class='item-time'>
11 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/pOtzqKmyiuTgPPj0x7w-GnTQwM6XUNQ6ETzm22wWCKOJw2qJiO9GT_bhN2-WEgGr0CSovWmQZbzZu4Dou14s=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://structlearn.blogspot.com/' target='_blank'>
Structured Learning</a>
</div>
<div class='item-content'>
<span class='item-title'>
<a href='http://structlearn.blogspot.com/2007/07/corrections-to-acl-anthology-urls.html' target='_blank'>
Corrections to ACL Anthology URLs
</a>
</span>
<div class='item-time'>
11 years ago
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/HrcFRkeXWL8DHGyWNKVC4n3rrz7VkVt2_fOnOcd6z7mNlFKx1kdF19QChXdXiOLDHFQ6uib2K8FUqp7rg0VFP1A=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://apperceptual.wordpress.com/feed/' target='_blank'>
Apperceptual</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/2XvQqtbpDV0xsPTDKoGYXAMTMSEsE97PvyMzhsNBBL47_FcjCOx0NBAve0qjidMoCmfhJAg=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://sixthform.info/maths/b2rss2.php' target='_blank'>
Mathematics Weblog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/ZPe0wqG0xJWmj6H4oD1pOfw7IPjgERounhx4FY8Wdn_QgFGkkAUwOWujgdqug_lS=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://mehve.org/ywml/atom.xml' target='_blank'>
yw's machine learning blog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/cWKQwFJ3NTJZtcZQin2RkroItRhwJX0wR2-uEshtM7URWaTrgTwxk6bLQYp3YFoSqV4xmg0NoleX5lB2iBwEk2Tl=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://undirectedgrad.blogspot.com/feeds/posts/default' target='_blank'>
Undirected Grad</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/vJRjUacZ51faqmiZWjpVOmGzEJkwnGm0RzdSQvPRd3ZOWkraGAzzGClKvQ7htgW9PoQ0G6yPdY_5zXz6=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://www.datawrangling.com/feed/' target='_blank'>
Data Wrangling</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh3.googleusercontent.com/proxy/c2zXSkDdDJAXffRUYfEemwqH78a-oGktig4Kiz_mxkTXvApTM_ZABOe-y9LjggcQlOgx2ZNhTq-Z6Od9-QQP=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://ergodicity.iamganesh.com/feed/atom/' target='_blank'>
Ganesh Swami</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/w4aA3hr7EXbhjjUyVhJtI2EIVGifPwOB9_fTxO8SsAR_z8raYW9iq4V_Uco_bmBhtqcRJGVPXz2QCfQroP8uKZO4K1PPnMkjqy0=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='https://www.lists.utah.edu/wws/rss/latest_d_read/mstatbiostat?count=20&for=10' target='_blank'>
mstatbiostat :</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/qXnoxci4mhuckJyGBIyTYsRQDNoiuL2KdUaWYog8x0kKXNl9wjzFgl9AArHka6tgC7-qOuBvkw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://groundtruth.info/AstroStat/slog/feed/' target='_blank'>
The AstroStat Slog</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/UY6Ny1sQrfL6ayfx-wsyDsh4sekpUEWG8gO158YZrXDDlZ-hTux5J1byz3Plv_yQAGJwWZd97FrhnHQ=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://feeds.feedburner.com/MainlyData' target='_blank'>
Mainly Data</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/m_j2k_iy9FaVdLNKRfRNdBs8OrO-qn6gqG571o0DnIoJyjG6egaEuApJ_3yhpPu2UWNe=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://conflate.net/inductio/feed/' target='_blank'>
Inductio Ex Machina</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh5.googleusercontent.com/proxy/fifP-pSmqTcZADNbIY9nO3WHTJEyBTE6t9AsMgnbh5ZWHukAJzpJ7xLQtjLfljH-wh_ldKiFb28AzbBjwvNv=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://magic.aladdin.cs.cmu.edu/feed/atom/' target='_blank'>
[Lowerbounds, Upperbounds]</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/RML3Rwk2SwWmmZjNQVAk8eA5DWqj0rIh44aXVluOXhziCiULvrgajKJjTVbOWNXoNoGKv_S97chvpKttSqgaL5Z8ZIIlNAk_=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://clair.si.umich.edu:8080/wordpress/?feed=atom' target='_blank'>
Information Engineering</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/qXnoxci4mhuckJyGBIyTYsRQDNoiuL2KdUaWYog8x0kKXNl9wjzFgl9AArHka6tgC7-qOuBvkw=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://groundtruth.info/AstroStat/slog/' target='_blank'>
http://groundtruth.info/AstroStat/slog/</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh6.googleusercontent.com/proxy/7qHgz80_XsH_8TUY9dNHcdWkry8tQ26AsKIrGtDxAYNcS6Uxd8VEvKxh80KRhPIfryVq-qnvnFs0ZqAIUq9FgOCO=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://ciir.cs.umass.edu/~fdiaz/irblog/?feed=atom' target='_blank'>
Information Retrieval</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
<li style='display: block;'>
<div class='blog-icon'>
<img data-lateloadsrc='https://lh4.googleusercontent.com/proxy/voorwkyCRbH1oCYI0tgn66M0QcGgc1XopARmKFsoCAynNrJmoBEdBTQ-EWNnrgOr1P7vsESI=s0-d' height='16' width='16'/>
</div>
<div class='blog-content'>
<div class='blog-title'>
<a href='http://ba.stat.cmu.edu/forthcoming.xml' target='_blank'>
Bayesian Analysis Journal :: Forthcoming Articles</a>
</div>
<div class='item-content'>
<span class='item-title'>
<!--Can't find substitution for tag [item.itemTitle]-->
</span>
<div class='item-time'>
<!--Can't find substitution for tag [item.timePeriodSinceLastUpdate]-->
</div>
</div>
</div>
<div style='clear: both;'></div>
</li>
</ul>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=BlogList&widgetId=BlogList1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("BlogList1"));' rel='nofollow' target='configBlogList1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div>
</div><div class='widget BlogArchive' data-version='1' id='BlogArchive1'>
<h2>Blog Archive</h2>
<div class='widget-content'>
<div id='ArchiveList'>
<div id='BlogArchive1_ArchiveList'>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2018/'>
2018
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2018/07/'>
July
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2018/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/'>
2017
</a>
<span class='post-count' dir='ltr'>(10)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/08/'>
August
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/04/'>
April
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2017/03/'>
March
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/'>
2016
</a>
<span class='post-count' dir='ltr'>(17)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/12/'>
December
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/11/'>
November
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/08/'>
August
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/07/'>
July
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/06/'>
June
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/05/'>
May
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2016/03/'>
March
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/'>
2015
</a>
<span class='post-count' dir='ltr'>(7)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/12/'>
December
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/10/'>
October
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2015/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/'>
2014
</a>
<span class='post-count' dir='ltr'>(14)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/11/'>
November
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/10/'>
October
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/09/'>
September
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/07/'>
July
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/06/'>
June
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/05/'>
May
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2014/04/'>
April
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/'>
2013
</a>
<span class='post-count' dir='ltr'>(4)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/09/'>
September
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/07/'>
July
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2013/04/'>
April
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/'>
2012
</a>
<span class='post-count' dir='ltr'>(7)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/12/'>
December
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/06/'>
June
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2012/02/'>
February
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/'>
2011
</a>
<span class='post-count' dir='ltr'>(16)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/12/'>
December
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/10/'>
October
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/07/'>
July
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/05/'>
May
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/04/'>
April
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/03/'>
March
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/02/'>
February
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2011/01/'>
January
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate expanded'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy toggle-open'>

        &#9660;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/'>
2010
</a>
<span class='post-count' dir='ltr'>(29)</span>
<ul class='hierarchy'>
<li class='archivedate expanded'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy toggle-open'>

        &#9660;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/11/'>
November
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/11/crowdsourcing-workshop-tutorial.html'>Crowdsourcing workshop (/tutorial) decisions</a></li>
<li><a href='https://nlpers.blogspot.com/2010/11/managing-group-papers.html'>Managing group papers</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/10/'>
October
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/10/comparing-bounds.html'>Comparing Bounds</a></li>
<li><a href='https://nlpers.blogspot.com/2010/10/my-giant-reviewing-error.html'>My Giant Reviewing Error</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/09/'>
September
</a>
<span class='post-count' dir='ltr'>(4)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/09/acl-icml-symposium.html'>ACL / ICML Symposium?</a></li>
<li><a href='https://nlpers.blogspot.com/2010/09/very-sad-news.html'>Very sad news....</a></li>
<li><a href='https://nlpers.blogspot.com/2010/09/aistats-2011-call-for-papers.html'>AIStats 2011 Call for Papers</a></li>
<li><a href='https://nlpers.blogspot.com/2010/09/manifold-assumption-versus-margin.html'>Manifold Assumption versus Margin Assumption</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/08/'>
August
</a>
<span class='post-count' dir='ltr'>(6)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/08/online-learning-algorithms-that-work.html'>Online Learning Algorithms that Work Harder</a></li>
<li><a href='https://nlpers.blogspot.com/2010/08/calibrating-reviews-and-ratings.html'>Calibrating Reviews and Ratings</a></li>
<li><a href='https://nlpers.blogspot.com/2010/08/finite-state-nlp-with-unlabeled-data-on.html'>Finite State NLP with Unlabeled Data on Both Sides...</a></li>
<li><a href='https://nlpers.blogspot.com/2010/08/readers-kill-blogs.html'>Readers kill blogs?</a></li>
<li><a href='https://nlpers.blogspot.com/2010/08/multi-task-learning-should-our.html'>Multi-task learning: should our hypothesis classes...</a></li>
<li><a href='https://nlpers.blogspot.com/2010/08/why-discourse-structure.html'>Why Discourse Structure?</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/07/'>
July
</a>
<span class='post-count' dir='ltr'>(1)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/07/acl-2010-retrospective.html'>ACL 2010 Retrospective</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/06/'>
June
</a>
<span class='post-count' dir='ltr'>(2)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/06/icml-2010-retrospective.html'>ICML 2010 Retrospective</a></li>
<li><a href='https://nlpers.blogspot.com/2010/06/naacl-2010-retrospective.html'>NAACL 2010 Retrospective</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/04/'>
April
</a>
<span class='post-count' dir='ltr'>(5)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/04/graduating-want-post-doc-let-nsf-pay.html'>Graduating?  Want a post-doc?  Let NSF pay!</a></li>
<li><a href='https://nlpers.blogspot.com/2010/04/b-and-not-b-not.html'>((A =&gt; B) and (not B)) =&gt; (not A)</a></li>
<li><a href='https://nlpers.blogspot.com/2010/04/how-i-teach-machine-learning.html'>How I teach machine learning</a></li>
<li><a href='https://nlpers.blogspot.com/2010/04/when-maximum-likelihood-doesnt-make.html'>When Maximum Likelihood Doesn&#39;t Make Sense</a></li>
<li><a href='https://nlpers.blogspot.com/2010/04/classification-weirdness-regression.html'>Classification weirdness, regression simplicity</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/02/'>
February
</a>
<span class='post-count' dir='ltr'>(3)</span>
<ul class='posts'>
<li><a href='https://nlpers.blogspot.com/2010/02/google-5gram-corpus-has-unreasonable.html'>Google 5gram corpus has unreasonable 5grams</a></li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2010/01/'>
January
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/'>
2009
</a>
<span class='post-count' dir='ltr'>(34)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/12/'>
December
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/11/'>
November
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/10/'>
October
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/09/'>
September
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/08/'>
August
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/07/'>
July
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/06/'>
June
</a>
<span class='post-count' dir='ltr'>(8)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/05/'>
May
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/04/'>
April
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/03/'>
March
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/02/'>
February
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2009/01/'>
January
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/'>
2008
</a>
<span class='post-count' dir='ltr'>(37)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/12/'>
December
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/11/'>
November
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/09/'>
September
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/08/'>
August
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/07/'>
July
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/06/'>
June
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/05/'>
May
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/04/'>
April
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/03/'>
March
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/02/'>
February
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2008/01/'>
January
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/'>
2007
</a>
<span class='post-count' dir='ltr'>(58)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/12/'>
December
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/11/'>
November
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/10/'>
October
</a>
<span class='post-count' dir='ltr'>(3)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/09/'>
September
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/08/'>
August
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/07/'>
July
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/06/'>
June
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/05/'>
May
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/04/'>
April
</a>
<span class='post-count' dir='ltr'>(8)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/03/'>
March
</a>
<span class='post-count' dir='ltr'>(2)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/02/'>
February
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2007/01/'>
January
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/'>
2006
</a>
<span class='post-count' dir='ltr'>(78)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/12/'>
December
</a>
<span class='post-count' dir='ltr'>(1)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/11/'>
November
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/10/'>
October
</a>
<span class='post-count' dir='ltr'>(10)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/09/'>
September
</a>
<span class='post-count' dir='ltr'>(4)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/08/'>
August
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/07/'>
July
</a>
<span class='post-count' dir='ltr'>(8)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/06/'>
June
</a>
<span class='post-count' dir='ltr'>(5)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/05/'>
May
</a>
<span class='post-count' dir='ltr'>(11)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/04/'>
April
</a>
<span class='post-count' dir='ltr'>(7)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/03/'>
March
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/02/'>
February
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2006/01/'>
January
</a>
<span class='post-count' dir='ltr'>(9)</span>
</li>
</ul>
</li>
</ul>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2005/'>
2005
</a>
<span class='post-count' dir='ltr'>(6)</span>
<ul class='hierarchy'>
<li class='archivedate collapsed'>
<a class='toggle' href='javascript:void(0)'>
<span class='zippy'>

        &#9658;&#160;
      
</span>
</a>
<a class='post-count-link' href='https://nlpers.blogspot.com/2005/12/'>
December
</a>
<span class='post-count' dir='ltr'>(6)</span>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=BlogArchive&widgetId=BlogArchive1&action=editWidget&sectionId=sidebar' onclick='return _WidgetManager._PopupConfig(document.getElementById("BlogArchive1"));' rel='nofollow' target='configBlogArchive1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div>
</div></div>
</div>
<!-- spacer for skins that want sidebar and main to be the same height-->
<div class='clear'>&#160;</div>
</div>
<!-- end content-wrapper -->
<div id='footer-wrapper'>
<div class='footer section' id='footer'><div class='widget HTML' data-version='1' id='HTML1'>
<div class='widget-content'>
<script src="//s17.sitemeter.com/js/counter.js?site=s17nlers" type="text/javascript">
</script>
</div>
<div class='clear'></div>
<span class='widget-item-control'>
<span class='item-control blog-admin'>
<a class='quickedit' href='//www.blogger.com/rearrange?blogID=19803222&widgetType=HTML&widgetId=HTML1&action=editWidget&sectionId=footer' onclick='return _WidgetManager._PopupConfig(document.getElementById("HTML1"));' rel='nofollow' target='configHTML1' title='Edit'>
<img alt='' height='18' src='https://resources.blogblog.com/img/icon18_wrench_allbkg.png' width='18'/>
</a>
</span>
</span>
<div class='clear'></div>
</div></div>
</div>
</div></div>
<!-- end outer-wrapper -->
<script src='https://apis.google.com/js/plusone.js' type='text/javascript'></script>

<script type="text/javascript" src="https://www.blogger.com/static/v1/widgets/640298382-widgets.js"></script>
<script type='text/javascript'>
window['__wavt'] = 'AOuZoY7Xmr3dRMM9riQZn265dX49q8bpYg:1556334931355';_WidgetManager._Init('//www.blogger.com/rearrange?blogID\x3d19803222','//nlpers.blogspot.com/2010/','19803222');
_WidgetManager._SetDataContext([{'name': 'blog', 'data': {'blogId': '19803222', 'title': 'natural language processing blog', 'url': 'https://nlpers.blogspot.com/2010/', 'canonicalUrl': 'https://nlpers.blogspot.com/2010/', 'homepageUrl': 'https://nlpers.blogspot.com/', 'searchUrl': 'https://nlpers.blogspot.com/search', 'canonicalHomepageUrl': 'https://nlpers.blogspot.com/', 'blogspotFaviconUrl': 'https://nlpers.blogspot.com/favicon.ico', 'bloggerUrl': 'https://www.blogger.com', 'hasCustomDomain': false, 'httpsEnabled': true, 'enabledCommentProfileImages': true, 'gPlusViewType': 'FILTERED_POSTMOD', 'adultContent': false, 'analyticsAccountNumber': '', 'encoding': 'UTF-8', 'locale': 'en', 'localeUnderscoreDelimited': 'en', 'languageDirection': 'ltr', 'isPrivate': false, 'isMobile': false, 'isMobileRequest': false, 'mobileClass': '', 'isPrivateBlog': false, 'feedLinks': '\x3clink rel\x3d\x22alternate\x22 type\x3d\x22application/atom+xml\x22 title\x3d\x22natural language processing blog - Atom\x22 href\x3d\x22https://nlpers.blogspot.com/feeds/posts/default\x22 /\x3e\n\x3clink rel\x3d\x22alternate\x22 type\x3d\x22application/rss+xml\x22 title\x3d\x22natural language processing blog - RSS\x22 href\x3d\x22https://nlpers.blogspot.com/feeds/posts/default?alt\x3drss\x22 /\x3e\n\x3clink rel\x3d\x22service.post\x22 type\x3d\x22application/atom+xml\x22 title\x3d\x22natural language processing blog - Atom\x22 href\x3d\x22https://www.blogger.com/feeds/19803222/posts/default\x22 /\x3e\n', 'meTag': '', 'adsenseHostId': 'ca-host-pub-1556223355139109', 'adsenseHasAds': false, 'view': '', 'dynamicViewsCommentsSrc': '//www.blogblog.com/dynamicviews/4224c15c4e7c9321/js/comments.js', 'dynamicViewsScriptSrc': '//www.blogblog.com/dynamicviews/b1e9f650b2e5f9f3', 'plusOneApiSrc': 'https://apis.google.com/js/plusone.js', 'disableGComments': true, 'sharing': {'platforms': [{'name': 'Get link', 'key': 'link', 'shareMessage': 'Get link', 'target': ''}, {'name': 'Facebook', 'key': 'facebook', 'shareMessage': 'Share to Facebook', 'target': 'facebook'}, {'name': 'BlogThis!', 'key': 'blogThis', 'shareMessage': 'BlogThis!', 'target': 'blog'}, {'name': 'Twitter', 'key': 'twitter', 'shareMessage': 'Share to Twitter', 'target': 'twitter'}, {'name': 'Pinterest', 'key': 'pinterest', 'shareMessage': 'Share to Pinterest', 'target': 'pinterest'}, {'name': 'Email', 'key': 'email', 'shareMessage': 'Email', 'target': 'email'}], 'disableGooglePlus': true, 'googlePlusShareButtonWidth': 300, 'googlePlusBootstrap': '\x3cscript type\x3d\x22text/javascript\x22\x3ewindow.___gcfg \x3d {\x27lang\x27: \x27en\x27};\x3c/script\x3e'}, 'hasCustomJumpLinkMessage': false, 'jumpLinkMessage': 'Read more', 'pageType': 'archive', 'pageName': '2010', 'pageTitle': 'natural language processing blog: 2010'}}, {'name': 'features', 'data': {'sharing_get_link_dialog': 'true', 'sharing_native': 'false'}}, {'name': 'messages', 'data': {'edit': 'Edit', 'linkCopiedToClipboard': 'Link copied to clipboard!', 'ok': 'Ok', 'postLink': 'Post Link'}}, {'name': 'template', 'data': {'name': 'custom', 'localizedName': 'Custom', 'isResponsive': false, 'isAlternateRendering': false, 'isCustom': true}}, {'name': 'view', 'data': {'classic': {'name': 'classic', 'url': '?view\x3dclassic'}, 'flipcard': {'name': 'flipcard', 'url': '?view\x3dflipcard'}, 'magazine': {'name': 'magazine', 'url': '?view\x3dmagazine'}, 'mosaic': {'name': 'mosaic', 'url': '?view\x3dmosaic'}, 'sidebar': {'name': 'sidebar', 'url': '?view\x3dsidebar'}, 'snapshot': {'name': 'snapshot', 'url': '?view\x3dsnapshot'}, 'timeslide': {'name': 'timeslide', 'url': '?view\x3dtimeslide'}, 'isMobile': false, 'title': 'natural language processing blog', 'description': 'my biased thoughts on the fields of natural language processing (NLP), computational linguistics (CL) and related topics (machine learning, math, funding, etc.)', 'url': 'https://nlpers.blogspot.com/2010/', 'type': 'feed', 'isSingleItem': false, 'isMultipleItems': true, 'isError': false, 'isPage': false, 'isPost': false, 'isHomepage': false, 'isArchive': true, 'isLabelSearch': false, 'archive': {'year': 2010, 'rangeMessage': 'Showing posts from 2010'}}}]);
_WidgetManager._RegisterWidget('_NavbarView', new _WidgetInfo('Navbar1', 'navbar', document.getElementById('Navbar1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HeaderView', new _WidgetInfo('Header1', 'header', document.getElementById('Header1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogView', new _WidgetInfo('Blog1', 'main', document.getElementById('Blog1'), {'cmtInteractionsEnabled': false, 'lightboxEnabled': true, 'lightboxModuleUrl': 'https://www.blogger.com/static/v1/jsbin/1977841774-lbx.js', 'lightboxCssUrl': 'https://www.blogger.com/static/v1/v-css/368954415-lightbox_bundle.css'}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_ProfileView', new _WidgetInfo('Profile1', 'sidebar', document.getElementById('Profile1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_LabelView', new _WidgetInfo('Label1', 'sidebar', document.getElementById('Label1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogListView', new _WidgetInfo('BlogList1', 'sidebar', document.getElementById('BlogList1'), {'numItemsToShow': 0, 'totalItems': 47}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogArchiveView', new _WidgetInfo('BlogArchive1', 'sidebar', document.getElementById('BlogArchive1'), {'languageDirection': 'ltr', 'loadingMessage': 'Loading\x26hellip;'}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HTMLView', new _WidgetInfo('HTML1', 'footer', document.getElementById('HTML1'), {}, 'displayModeFull'));
</script>
</body>
</html>